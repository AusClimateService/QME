Loading singularity
Loading conda/analysis3-25.06

Loading gadi_jupyterlab/23.02
  Module ERROR: invalid command name "::tclPkgUnknown"
    In '/g/data/dk92/apps/Modules/modulefiles/gadi_jupyterlab/23.02'
    Please contact <root@localhost>
2025-12-04 15:56:39,703 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:38717'
2025-12-04 15:56:39,709 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:45103'
2025-12-04 15:56:39,717 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:40563'
2025-12-04 15:56:39,720 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:45191'
2025-12-04 15:56:39,724 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:34845'
2025-12-04 15:56:39,729 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:41985'
2025-12-04 15:56:39,733 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:35391'
2025-12-04 15:56:39,739 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:33313'
2025-12-04 15:56:39,743 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:35257'
2025-12-04 15:56:39,747 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:34505'
2025-12-04 15:56:39,751 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:33449'
2025-12-04 15:56:39,755 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:36111'
2025-12-04 15:56:39,758 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:43037'
2025-12-04 15:56:39,762 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:43279'
2025-12-04 15:56:39,766 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:33205'
2025-12-04 15:56:39,770 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:33481'
2025-12-04 15:56:39,775 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:37561'
2025-12-04 15:56:39,779 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:34935'
2025-12-04 15:56:39,783 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:45355'
2025-12-04 15:56:39,786 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:35551'
2025-12-04 15:56:39,791 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:39291'
2025-12-04 15:56:39,872 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:36239'
2025-12-04 15:56:39,876 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:42579'
2025-12-04 15:56:39,881 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:36565'
2025-12-04 15:56:39,886 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:32829'
2025-12-04 15:56:39,891 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:38185'
2025-12-04 15:56:39,896 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:33417'
2025-12-04 15:56:39,900 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:44741'
2025-12-04 15:56:39,905 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:45203'
2025-12-04 15:56:39,909 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:35861'
2025-12-04 15:56:39,912 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:44465'
2025-12-04 15:56:39,917 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:42623'
2025-12-04 15:56:39,923 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:43785'
2025-12-04 15:56:39,927 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:35453'
2025-12-04 15:56:39,931 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:43863'
2025-12-04 15:56:39,936 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:43925'
2025-12-04 15:56:39,940 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:33671'
2025-12-04 15:56:39,944 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:41241'
2025-12-04 15:56:39,950 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:34999'
2025-12-04 15:56:39,954 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:46559'
2025-12-04 15:56:39,957 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:45595'
2025-12-04 15:56:39,960 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:33081'
2025-12-04 15:56:39,965 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:33221'
2025-12-04 15:56:39,969 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:37941'
2025-12-04 15:56:39,972 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:43173'
2025-12-04 15:56:39,975 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:34053'
2025-12-04 15:56:39,979 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:36051'
2025-12-04 15:56:39,981 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:41367'
2025-12-04 15:56:41,270 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:35149
2025-12-04 15:56:41,270 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:35149
2025-12-04 15:56:41,270 - distributed.worker - INFO -          dashboard at:            10.6.5.29:40607
2025-12-04 15:56:41,270 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,270 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,270 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,270 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,270 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-z4fy94b_
2025-12-04 15:56:41,270 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,288 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,289 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,289 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,308 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,308 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:33231
2025-12-04 15:56:41,308 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:33231
2025-12-04 15:56:41,308 - distributed.worker - INFO -          dashboard at:            10.6.5.29:39999
2025-12-04 15:56:41,308 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,308 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,308 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,308 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,308 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-3iv5yqkd
2025-12-04 15:56:41,309 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,315 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:40917
2025-12-04 15:56:41,316 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:40917
2025-12-04 15:56:41,316 - distributed.worker - INFO -          dashboard at:            10.6.5.29:38665
2025-12-04 15:56:41,316 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,316 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,316 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,316 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,316 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-tuhjcwlj
2025-12-04 15:56:41,316 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,322 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:38891
2025-12-04 15:56:41,322 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:38891
2025-12-04 15:56:41,322 - distributed.worker - INFO -          dashboard at:            10.6.5.29:34703
2025-12-04 15:56:41,322 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,322 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,322 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,322 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,322 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-alzlwood
2025-12-04 15:56:41,322 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,327 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:41837
2025-12-04 15:56:41,327 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:41837
2025-12-04 15:56:41,327 - distributed.worker - INFO -          dashboard at:            10.6.5.29:41507
2025-12-04 15:56:41,327 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,327 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,327 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,327 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,327 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-duea7tji
2025-12-04 15:56:41,327 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,327 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,328 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,328 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,329 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,334 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,334 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,334 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,335 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,338 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,339 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,339 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,339 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,347 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:42499
2025-12-04 15:56:41,347 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:42499
2025-12-04 15:56:41,347 - distributed.worker - INFO -          dashboard at:            10.6.5.29:35415
2025-12-04 15:56:41,347 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,347 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,347 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,347 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,347 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,347 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:45425
2025-12-04 15:56:41,347 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-b4qbas2x
2025-12-04 15:56:41,347 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:45425
2025-12-04 15:56:41,347 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,347 - distributed.worker - INFO -          dashboard at:            10.6.5.29:42045
2025-12-04 15:56:41,347 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,347 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,347 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,348 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,348 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-m7lqtcxa
2025-12-04 15:56:41,348 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,348 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,348 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,349 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,349 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:34727
2025-12-04 15:56:41,349 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:34727
2025-12-04 15:56:41,349 - distributed.worker - INFO -          dashboard at:            10.6.5.29:36293
2025-12-04 15:56:41,349 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,349 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,349 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,350 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,349 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:35007
2025-12-04 15:56:41,349 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:46653
2025-12-04 15:56:41,350 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-jmby78qg
2025-12-04 15:56:41,350 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:35007
2025-12-04 15:56:41,350 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:46653
2025-12-04 15:56:41,350 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,350 - distributed.worker - INFO -          dashboard at:            10.6.5.29:35013
2025-12-04 15:56:41,350 - distributed.worker - INFO -          dashboard at:            10.6.5.29:36245
2025-12-04 15:56:41,350 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,350 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,350 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,350 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:41929
2025-12-04 15:56:41,350 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,350 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,350 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,350 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:41929
2025-12-04 15:56:41,350 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,350 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,350 - distributed.worker - INFO -          dashboard at:            10.6.5.29:40595
2025-12-04 15:56:41,350 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-fsbuqq96
2025-12-04 15:56:41,350 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-a8u9g66j
2025-12-04 15:56:41,350 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,350 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,350 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,350 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,350 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,350 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,350 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-zpgjww1d
2025-12-04 15:56:41,350 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:36641
2025-12-04 15:56:41,350 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,350 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:36641
2025-12-04 15:56:41,350 - distributed.worker - INFO -          dashboard at:            10.6.5.29:39881
2025-12-04 15:56:41,350 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,350 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,350 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,351 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,351 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-p6f3cfx2
2025-12-04 15:56:41,351 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,359 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:41729
2025-12-04 15:56:41,360 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:41729
2025-12-04 15:56:41,360 - distributed.worker - INFO -          dashboard at:            10.6.5.29:35727
2025-12-04 15:56:41,360 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,360 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,360 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,360 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,360 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-luzejpw6
2025-12-04 15:56:41,360 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,360 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,361 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,361 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,362 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,362 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:41483
2025-12-04 15:56:41,362 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:41483
2025-12-04 15:56:41,362 - distributed.worker - INFO -          dashboard at:            10.6.5.29:37983
2025-12-04 15:56:41,362 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,362 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,362 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,362 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,362 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,362 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-s6s8tvx5
2025-12-04 15:56:41,362 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,362 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,362 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,363 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,364 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,364 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,364 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,365 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,366 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:46595
2025-12-04 15:56:41,366 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:46595
2025-12-04 15:56:41,366 - distributed.worker - INFO -          dashboard at:            10.6.5.29:38531
2025-12-04 15:56:41,366 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,366 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,366 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,366 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,366 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-bsy4xw7z
2025-12-04 15:56:41,366 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,367 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:33053
2025-12-04 15:56:41,367 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:33053
2025-12-04 15:56:41,367 - distributed.worker - INFO -          dashboard at:            10.6.5.29:40473
2025-12-04 15:56:41,367 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,367 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,367 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,367 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,367 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-ebymun5r
2025-12-04 15:56:41,367 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,368 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,369 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,369 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,370 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,370 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,371 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,371 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,371 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,372 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,372 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,372 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,372 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,372 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,373 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,373 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,374 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,376 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:35439
2025-12-04 15:56:41,376 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:35439
2025-12-04 15:56:41,377 - distributed.worker - INFO -          dashboard at:            10.6.5.29:40209
2025-12-04 15:56:41,377 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,377 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,377 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,377 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,377 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-jzm4m6jx
2025-12-04 15:56:41,377 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,378 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,379 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,379 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,379 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,379 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,379 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,380 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,380 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,383 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:43993
2025-12-04 15:56:41,383 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:43993
2025-12-04 15:56:41,383 - distributed.worker - INFO -          dashboard at:            10.6.5.29:46555
2025-12-04 15:56:41,383 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,383 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,383 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,383 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,384 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-7zkhbvih
2025-12-04 15:56:41,384 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,385 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,386 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,386 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,387 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,387 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,387 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,388 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,389 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,394 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:38343
2025-12-04 15:56:41,394 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:38343
2025-12-04 15:56:41,394 - distributed.worker - INFO -          dashboard at:            10.6.5.29:38631
2025-12-04 15:56:41,395 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,395 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,395 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,395 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,395 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,395 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-udsjs79k
2025-12-04 15:56:41,395 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,395 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,395 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,396 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,396 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,397 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,397 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,399 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,410 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,411 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,411 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,412 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,445 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:44367
2025-12-04 15:56:41,445 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:44367
2025-12-04 15:56:41,445 - distributed.worker - INFO -          dashboard at:            10.6.5.29:45271
2025-12-04 15:56:41,445 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,445 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,445 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,446 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,446 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-djklqkul
2025-12-04 15:56:41,446 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,451 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:43411
2025-12-04 15:56:41,452 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:43411
2025-12-04 15:56:41,452 - distributed.worker - INFO -          dashboard at:            10.6.5.29:43063
2025-12-04 15:56:41,452 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,452 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,452 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,452 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,452 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-l6nwxwp4
2025-12-04 15:56:41,452 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,453 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:39781
2025-12-04 15:56:41,453 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:39781
2025-12-04 15:56:41,453 - distributed.worker - INFO -          dashboard at:            10.6.5.29:39911
2025-12-04 15:56:41,453 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,453 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,453 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,453 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,453 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-94y1wtwe
2025-12-04 15:56:41,453 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,467 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,468 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,468 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,468 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:44975
2025-12-04 15:56:41,468 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:44975
2025-12-04 15:56:41,468 - distributed.worker - INFO -          dashboard at:            10.6.5.29:46087
2025-12-04 15:56:41,468 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,468 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,468 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,468 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,468 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-tcrl0ml9
2025-12-04 15:56:41,468 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,468 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:43231
2025-12-04 15:56:41,468 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:43231
2025-12-04 15:56:41,468 - distributed.worker - INFO -          dashboard at:            10.6.5.29:33963
2025-12-04 15:56:41,468 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,468 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,468 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,469 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,469 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-m9p1a8zo
2025-12-04 15:56:41,469 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,469 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,472 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,473 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,473 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,475 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,475 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,475 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,476 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,477 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,479 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:46195
2025-12-04 15:56:41,479 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:46195
2025-12-04 15:56:41,479 - distributed.worker - INFO -          dashboard at:            10.6.5.29:41869
2025-12-04 15:56:41,479 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,479 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,479 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,479 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,479 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-vh6o1zkj
2025-12-04 15:56:41,479 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,479 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,480 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,480 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,480 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,481 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,481 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,481 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,482 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,499 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:35687
2025-12-04 15:56:41,499 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:35687
2025-12-04 15:56:41,499 - distributed.worker - INFO -          dashboard at:            10.6.5.29:43707
2025-12-04 15:56:41,499 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,499 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,499 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,499 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,499 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-2i76o4je
2025-12-04 15:56:41,499 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,501 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,502 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,502 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,502 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:35483
2025-12-04 15:56:41,502 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:35483
2025-12-04 15:56:41,502 - distributed.worker - INFO -          dashboard at:            10.6.5.29:34461
2025-12-04 15:56:41,502 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,502 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,502 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,502 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,502 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-mxaajnkt
2025-12-04 15:56:41,502 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,503 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,503 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:38715
2025-12-04 15:56:41,503 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:38715
2025-12-04 15:56:41,503 - distributed.worker - INFO -          dashboard at:            10.6.5.29:35465
2025-12-04 15:56:41,503 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,503 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,503 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,503 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,503 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-j5sskpxu
2025-12-04 15:56:41,503 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,506 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:34993
2025-12-04 15:56:41,506 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:34993
2025-12-04 15:56:41,506 - distributed.worker - INFO -          dashboard at:            10.6.5.29:33217
2025-12-04 15:56:41,506 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,506 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,506 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,506 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,506 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-3sgbcbez
2025-12-04 15:56:41,506 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,515 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,516 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,516 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,517 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,519 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,519 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,519 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,519 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:35863
2025-12-04 15:56:41,519 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:35863
2025-12-04 15:56:41,520 - distributed.worker - INFO -          dashboard at:            10.6.5.29:42769
2025-12-04 15:56:41,520 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,520 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,520 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,520 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,520 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-wksabzz9
2025-12-04 15:56:41,520 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,520 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,522 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,523 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,523 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,524 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,525 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,526 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,526 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,527 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,535 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:35005
2025-12-04 15:56:41,535 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:35005
2025-12-04 15:56:41,535 - distributed.worker - INFO -          dashboard at:            10.6.5.29:40629
2025-12-04 15:56:41,535 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,535 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,535 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,536 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,536 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-g2__8mju
2025-12-04 15:56:41,536 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,543 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,543 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,544 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,545 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,555 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,556 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,556 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,557 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,573 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:42701
2025-12-04 15:56:41,573 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:42701
2025-12-04 15:56:41,573 - distributed.worker - INFO -          dashboard at:            10.6.5.29:40065
2025-12-04 15:56:41,574 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,574 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,574 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,574 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,574 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-0gu_m23f
2025-12-04 15:56:41,574 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,575 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:42281
2025-12-04 15:56:41,576 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:42281
2025-12-04 15:56:41,576 - distributed.worker - INFO -          dashboard at:            10.6.5.29:42797
2025-12-04 15:56:41,576 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,576 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,576 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,576 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,576 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-i3lkt3yu
2025-12-04 15:56:41,576 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,577 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:33891
2025-12-04 15:56:41,578 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:33891
2025-12-04 15:56:41,578 - distributed.worker - INFO -          dashboard at:            10.6.5.29:40503
2025-12-04 15:56:41,578 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,578 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,578 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,578 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,578 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-c6m88rwm
2025-12-04 15:56:41,578 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,579 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:36899
2025-12-04 15:56:41,579 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:36899
2025-12-04 15:56:41,579 - distributed.worker - INFO -          dashboard at:            10.6.5.29:45303
2025-12-04 15:56:41,579 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,579 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,579 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,580 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,580 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-4d_lkvhc
2025-12-04 15:56:41,580 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,585 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:35725
2025-12-04 15:56:41,585 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:35725
2025-12-04 15:56:41,585 - distributed.worker - INFO -          dashboard at:            10.6.5.29:33849
2025-12-04 15:56:41,585 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,585 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,585 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,585 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,585 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-uogb_mo5
2025-12-04 15:56:41,585 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,590 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:36755
2025-12-04 15:56:41,590 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:36755
2025-12-04 15:56:41,590 - distributed.worker - INFO -          dashboard at:            10.6.5.29:43857
2025-12-04 15:56:41,590 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,590 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,590 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,590 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,590 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,590 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-__uon570
2025-12-04 15:56:41,590 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,591 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,591 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,591 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,595 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,595 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:39657
2025-12-04 15:56:41,595 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:39657
2025-12-04 15:56:41,595 - distributed.worker - INFO -          dashboard at:            10.6.5.29:41213
2025-12-04 15:56:41,595 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,595 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,595 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,595 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,595 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-4818er9x
2025-12-04 15:56:41,595 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,596 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,596 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,597 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,597 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,598 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,598 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,599 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,599 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,600 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,600 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,601 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,602 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,602 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,602 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,603 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,604 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,605 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,605 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,606 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,609 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:45291
2025-12-04 15:56:41,609 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:45291
2025-12-04 15:56:41,609 - distributed.worker - INFO -          dashboard at:            10.6.5.29:34947
2025-12-04 15:56:41,609 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,609 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,609 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,609 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,609 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-at5i6anp
2025-12-04 15:56:41,609 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,611 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:40927
2025-12-04 15:56:41,611 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:40927
2025-12-04 15:56:41,612 - distributed.worker - INFO -          dashboard at:            10.6.5.29:35083
2025-12-04 15:56:41,612 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,612 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,612 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,612 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,612 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:36487
2025-12-04 15:56:41,612 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-t0ia1kwj
2025-12-04 15:56:41,612 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:36487
2025-12-04 15:56:41,612 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,612 - distributed.worker - INFO -          dashboard at:            10.6.5.29:42245
2025-12-04 15:56:41,612 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,612 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,612 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,612 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,612 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-dyrwolq8
2025-12-04 15:56:41,612 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,615 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:40107
2025-12-04 15:56:41,615 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:40107
2025-12-04 15:56:41,615 - distributed.worker - INFO -          dashboard at:            10.6.5.29:34119
2025-12-04 15:56:41,615 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,615 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,615 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,615 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,615 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-vt0w1ncg
2025-12-04 15:56:41,615 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,617 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,617 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,617 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,618 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,619 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:44811
2025-12-04 15:56:41,619 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:44811
2025-12-04 15:56:41,619 - distributed.worker - INFO -          dashboard at:            10.6.5.29:41227
2025-12-04 15:56:41,619 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,619 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,619 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,620 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,620 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-h_0on_kh
2025-12-04 15:56:41,620 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,624 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,625 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,625 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,626 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,626 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,626 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,627 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,627 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,631 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,631 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:45817
2025-12-04 15:56:41,631 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:45817
2025-12-04 15:56:41,631 - distributed.worker - INFO -          dashboard at:            10.6.5.29:41561
2025-12-04 15:56:41,631 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,631 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,631 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,631 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,631 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-zuxzi17l
2025-12-04 15:56:41,631 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,631 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,631 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,633 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,633 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,634 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,634 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,635 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,638 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,639 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,639 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,641 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,643 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:38275
2025-12-04 15:56:41,643 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:38275
2025-12-04 15:56:41,643 - distributed.worker - INFO -          dashboard at:            10.6.5.29:32921
2025-12-04 15:56:41,643 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,643 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,643 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,643 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,643 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-ntu2blh8
2025-12-04 15:56:41,643 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,647 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:42559
2025-12-04 15:56:41,647 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:42559
2025-12-04 15:56:41,647 - distributed.worker - INFO -          dashboard at:            10.6.5.29:38605
2025-12-04 15:56:41,647 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,647 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,647 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,647 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,647 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-fryoj8r6
2025-12-04 15:56:41,647 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,651 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,652 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,652 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,653 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,658 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:38473
2025-12-04 15:56:41,658 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:38473
2025-12-04 15:56:41,658 - distributed.worker - INFO -          dashboard at:            10.6.5.29:44965
2025-12-04 15:56:41,658 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,658 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,659 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,659 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,659 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-46ewlyaw
2025-12-04 15:56:41,659 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,663 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,663 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,664 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,665 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,667 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,668 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,668 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,670 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,679 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,680 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,680 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,682 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:56:41,707 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:36159
2025-12-04 15:56:41,707 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:36159
2025-12-04 15:56:41,707 - distributed.worker - INFO -          dashboard at:            10.6.5.29:46707
2025-12-04 15:56:41,707 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,707 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,707 - distributed.worker - INFO -               Threads:                          1
2025-12-04 15:56:41,707 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-04 15:56:41,707 - distributed.worker - INFO -       Local Directory: /jobfs/155954975.gadi-pbs/dask-scratch-space/worker-dnzofatt
2025-12-04 15:56:41,707 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,718 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-04 15:56:41,718 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8722
2025-12-04 15:56:41,718 - distributed.worker - INFO - -------------------------------------------------
2025-12-04 15:56:41,719 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8722
2025-12-04 15:57:01,068 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,068 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,068 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,069 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,069 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,069 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,069 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,069 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,070 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,070 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,070 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,070 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,070 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,070 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,070 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,070 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,070 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,070 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,070 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,071 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,071 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,071 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,071 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,071 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,071 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,071 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,072 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,072 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,072 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,072 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,072 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,072 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,072 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,072 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,073 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,073 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,073 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,073 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,073 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,073 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,073 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,073 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,074 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,074 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,074 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,074 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,074 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,075 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,075 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,075 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,075 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,075 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,075 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,075 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,073 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,075 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,075 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,076 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,076 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,074 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,076 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,076 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,076 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,076 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,076 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,076 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,076 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,076 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,076 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,077 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,077 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,077 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,077 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,077 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,077 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,077 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,077 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,077 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,077 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,078 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,078 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,078 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,077 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,079 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,079 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,079 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,077 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,080 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,082 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,082 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,083 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,083 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,085 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,086 - distributed.worker - INFO - Starting Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-04 15:57:01,093 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:01,095 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-04 15:57:03,450 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,450 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,449 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,450 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,450 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,450 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,451 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,451 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,451 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,452 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,452 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,452 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,452 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,452 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,452 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,452 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,452 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,452 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,451 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,453 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,452 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,453 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,453 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,453 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,451 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,453 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,453 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,453 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,453 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,454 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,454 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,454 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,454 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,453 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,454 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,454 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,454 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,454 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,454 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,455 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,455 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,454 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,455 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,454 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,453 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,455 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,455 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,455 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,455 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,456 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,455 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,456 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,455 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,456 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,455 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,456 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,456 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,456 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,456 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,456 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,457 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,456 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,457 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,457 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,457 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,457 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,457 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,458 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,457 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,457 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,456 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,458 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,458 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,459 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,458 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,458 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,457 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,459 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,458 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,459 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,459 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,459 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,457 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,460 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,458 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,460 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,460 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,460 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,460 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,461 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,459 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,461 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,458 - distributed.worker - INFO - Starting Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-04 15:57:03,463 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,463 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,465 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-04 15:57:03,503 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,504 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,504 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,505 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,505 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,506 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,506 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,506 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,506 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,506 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,506 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,507 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,507 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,508 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,508 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,508 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,508 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,508 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,508 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,509 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,509 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,509 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,509 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,510 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,510 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,510 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,510 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,511 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,511 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,511 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,511 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,511 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,512 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,512 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,512 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,513 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,513 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,513 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,513 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,513 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,514 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,514 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,513 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,514 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,514 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,514 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,515 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,515 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,515 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,515 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,515 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,516 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,516 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,516 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,516 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,516 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,517 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,517 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,517 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,517 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,517 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,517 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,517 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,517 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,518 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,518 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,518 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,518 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,518 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,518 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,518 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,519 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,519 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,519 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,519 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,519 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,519 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,519 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,520 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,520 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,520 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,520 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,520 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,520 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,520 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,521 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,521 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,522 - distributed.worker - INFO - Starting Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-04 15:57:03,522 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,522 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,522 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,522 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,522 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,523 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,523 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,524 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-04 15:57:03,596 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,597 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,597 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,597 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,597 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,598 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,598 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,598 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,598 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,598 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,598 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,598 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,598 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,598 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,598 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,599 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,599 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,599 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,599 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,599 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,599 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,599 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,599 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,599 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,599 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,599 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,599 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,600 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,600 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,600 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,600 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,600 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,600 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,600 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,600 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,601 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,601 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,601 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,601 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,601 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,601 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,601 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,601 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,601 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,601 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,601 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,602 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,602 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,602 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,602 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,602 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,602 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,602 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,602 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,602 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,602 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,602 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,603 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,603 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,603 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,603 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,603 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,603 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,603 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,603 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,604 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,604 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,604 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,604 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,604 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,604 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,604 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,604 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,604 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,604 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,604 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,604 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,605 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,605 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,605 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,605 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,605 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,605 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,605 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,605 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,605 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,605 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,606 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,606 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,606 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-04 15:57:03,606 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,606 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,606 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,606 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,607 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:03,607 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-04 15:57:56,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 15:57:56,313 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 15:57:56,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 15:57:56,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 15:57:56,444 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 15:57:56,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 15:57:56,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 15:57:56,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 15:57:56,601 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 15:57:56,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 15:57:56,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 15:57:56,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 15:57:56,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 15:57:56,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 15:57:56,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 15:57:56,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 15:57:56,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 15:57:56,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 15:57:56,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 15:57:56,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 15:57:57,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 15:57:57,068 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,165 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,165 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,178 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:30,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,282 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,282 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,282 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,287 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:39,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:55,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:58,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:58,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:58,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:58,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:58,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:59,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:59,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:59,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:59,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:59,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:59,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:59,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:59,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:59,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:00:59,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,044 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,044 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,044 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,044 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:38,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,228 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,228 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,239 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:01:43,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:13,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:13,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:13,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:13,929 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:13,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:13,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:13,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:13,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:13,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:13,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:13,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:13,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:13,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:13,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,738 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,738 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,741 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,741 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:16,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:19,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:19,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-04 16:03:19,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-c6m88rwm/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-2i76o4je/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-dnzofatt/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-bsy4xw7z/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-b4qbas2x/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-0gu_m23f/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-zpgjww1d/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-djklqkul/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-uogb_mo5/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-p6f3cfx2/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-ebymun5r/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-fryoj8r6/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-j5sskpxu/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-4d_lkvhc/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-luzejpw6/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-duea7tji/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-tcrl0ml9/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-at5i6anp/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-tuhjcwlj/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-7zkhbvih/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-a8u9g66j/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-fsbuqq96/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-mxaajnkt/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-alzlwood/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-t0ia1kwj/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-46ewlyaw/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-udsjs79k/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-94y1wtwe/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-wksabzz9/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-i3lkt3yu/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-dyrwolq8/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-ntu2blh8/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-jmby78qg/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-g2__8mju/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-h_0on_kh/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-4818er9x/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
2025-12-04 16:09:36,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-s6s8tvx5/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-vt0w1ncg/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-m7lqtcxa/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-3iv5yqkd/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-jzm4m6jx/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-3sgbcbez/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-__uon570/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-m9p1a8zo/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-z4fy94b_/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-vh6o1zkj/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-zuxzi17l/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
2025-12-04 16:14:49,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
/jobfs/155954975.gadi-pbs/dask-scratch-space/worker-l6nwxwp4/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
2025-12-04 22:11:09,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-05 02:25:53,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-05 06:40:50,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-05 07:06:07,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-05 08:39:25,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-05 10:13:08,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-05 10:40:09,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-05 10:40:36,932 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38320 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,936 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,939 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:34727. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,932 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38274 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,940 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,940 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:35005. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,933 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38230 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,932 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38414 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,932 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38306 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,932 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38280 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,941 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,933 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38186 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,933 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38234 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,942 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:36641. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,933 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38150 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,932 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38428 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,942 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,944 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:45425. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,942 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,932 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38466 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,944 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:35483. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,933 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:60406 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,932 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38496 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,943 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,943 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,946 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:44975. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,935 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:60380 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,933 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38368 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,943 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,946 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:38715. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,945 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,947 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:33053. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,944 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,947 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:46653. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,944 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,947 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:41837. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,933 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38358 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,946 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,945 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,947 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:35725. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,933 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:60402 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,948 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:40107. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,932 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38348 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,931 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38468 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,933 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38178 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,948 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:41729. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,947 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,932 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38396 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,939 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45620 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,933 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38386 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,933 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38164 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,944 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,933 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38490 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,950 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:46595. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,950 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:35007. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,947 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,949 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,949 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,951 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:45291. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,951 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:42499. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,951 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:41483. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,933 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38262 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,949 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,933 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:60394 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,949 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,952 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:40927. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,935 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38134 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,952 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:39657. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,949 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,950 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,950 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,953 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:35863. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,948 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,953 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:41929. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,951 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45938 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,953 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:38275. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,954 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:43993. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,949 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45672 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,953 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,954 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:36159. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,950 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,955 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:38473. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,954 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,956 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:38891. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,951 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,957 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:36755. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,956 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,950 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,958 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:36487. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,954 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45844 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,958 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:33231. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,955 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,935 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45688 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,955 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45918 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,954 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45584 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,959 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:43411. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,952 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45738 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,940 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45560 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,959 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,958 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,934 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:38122 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,960 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:38343. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,960 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:42281. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,952 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45618 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,959 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,936 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45700 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,961 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:42559. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,961 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,935 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45762 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,962 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:35439. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,960 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,957 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45766 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,962 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,963 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:43231. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,963 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:44367. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,962 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,964 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:46195. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,963 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,965 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:35687. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,963 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,967 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:35149. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,967 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:42579'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,937 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45726 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,968 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:45103'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,969 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:34845'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,969 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:43785'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,969 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:41367'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,970 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:33481'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,970 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 8, 19))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,970 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 18, 11))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,970 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,970 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:41985'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,956 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45914 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,970 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 20, 14))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,968 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,970 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,970 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,970 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,970 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:35257'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,970 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,970 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,970 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 9, 20))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,970 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,970 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,970 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,970 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 19, 15))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,971 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:37941'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,971 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:39781. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,971 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,971 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,971 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,971 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,971 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,971 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,971 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,971 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,971 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 8, 13))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,971 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,971 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:43173'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,971 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,971 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,971 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,971 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,971 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,971 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,971 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,971 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,971 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 11, 2))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,971 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,971 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,970 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,972 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,972 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,972 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,972 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:45817. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,972 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,972 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,972 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,972 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,973 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 13, 12))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,974 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 13, 4))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,974 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,974 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,974 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,974 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,974 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,974 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,974 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,971 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45828 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,974 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:36111'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,974 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,974 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,975 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,975 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 4, 8))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,975 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,936 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45856 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,975 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,976 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,974 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,976 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,976 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,976 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:33891. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,976 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,977 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:36899. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,961 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,979 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:40917. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,955 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45912 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,979 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45838 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,983 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,984 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:42701. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,983 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 8, 17, 12))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,985 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,985 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,985 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,985 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,986 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,980 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,989 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:44811. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,946 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45772 remote=tcp://10.6.5.28:8722>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-05 10:40:36,991 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:36051'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,991 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:35861'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,989 - distributed.core - INFO - Connection to tcp://10.6.5.28:8722 has been closed.
2025-12-05 10:40:36,991 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:33221'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,992 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:34993. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,992 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:45191'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,992 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 8, 18))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,992 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 2, 5))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,992 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:36239'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,992 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,992 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:43037'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,992 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,992 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,992 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,993 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,993 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 20, 17))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,993 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 8, 15))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,993 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 20, 22))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,993 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,993 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:33449'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,993 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,993 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,993 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,993 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,993 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,993 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:43863'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,993 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,993 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,993 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,993 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 8, 21))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,993 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,994 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,994 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,994 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,994 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:46559'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,994 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,994 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,994 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:34935'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,994 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,994 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 8, 14))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,994 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:42623'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,994 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,994 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,994 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,994 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:45595'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,995 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,995 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,995 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,995 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,995 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:34505'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,995 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,995 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 4, 1))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,995 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,995 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:35453'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,995 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 2, 8))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,995 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,995 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 9, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,995 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:44741'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,995 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,995 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 18, 19))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,995 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,995 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,995 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:35391'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,996 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,996 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,996 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,996 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,996 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 8, 15, 21))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,996 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,996 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:35551'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,996 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,996 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,996 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 13, 16))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,996 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,996 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,996 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,996 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,996 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,996 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,996 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,996 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,996 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 4, 13))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,996 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,996 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,996 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 19, 1))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,996 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,996 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,996 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,997 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,997 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,997 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,997 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,997 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,997 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,997 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 7, 1))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,997 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,997 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,997 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,997 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,997 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:34053'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:36,997 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,997 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,997 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,997 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,997 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,997 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,998 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 8, 10))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:36,998 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,998 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,998 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:36,999 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:36,999 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:36,999 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:36,999 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 8, 2, 15))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:37,000 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:37,000 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:37,000 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:37,000 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:37,000 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:37,000 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:37,000 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:37,000 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:32829'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:37,000 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:37,000 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:37,000 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:37,000 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:43925'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:37,001 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:33081'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:37,001 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:45355'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:37,001 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 14, 23))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:37,001 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 9, 6))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:37,001 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:37,001 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:37,002 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:37,002 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:37,002 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:37,002 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:37,002 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:37,002 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:37,002 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:37,002 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 8, 6))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:37,002 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:37,003 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:36,998 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:37,003 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:37,003 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:45203'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:37,003 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:37,003 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:37,003 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:37,003 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:37,003 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:37,003 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:37,004 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:33417'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:37,004 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 8, 8, 13))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:37,004 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:37,005 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:37,004 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 4, 17))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:37,005 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:37,005 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:37,005 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:37,005 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:37,005 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:37,005 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:37,005 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:37,006 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:37,008 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 4, 3))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:37,008 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:37,008 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:37,009 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:37,009 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:37,009 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:37,009 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:43279'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:37,009 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:37561'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:37,010 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:33671'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:37,010 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 2, 10))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:37,010 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:37,010 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:37,010 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:37,010 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:37,010 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:37,011 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 19, 22))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:37,011 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:37,012 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:37,012 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:37,012 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:37,012 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:37,013 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 8, 2, 1))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:37,014 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:37,014 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:34999'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:37,015 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:37,015 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:37,015 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:37,015 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:37,015 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 9, 1))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:37,016 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:37,016 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:37,016 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:37,016 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:37,016 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:37,018 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:33313'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:37,020 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:40563'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:37,020 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:36565'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:37,020 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 13, 13))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:37,021 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:37,021 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:44465'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:37,021 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:37,021 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:37,021 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:37,021 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:37,021 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:38717'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:37,021 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 18, 2))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:37,021 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:37,022 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:37,022 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 15, 14))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:37,022 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:37,022 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:37,022 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:37,022 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:37,022 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:37,022 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:39291'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:37,022 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:37,022 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:37,022 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:37,022 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:38185'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:37,025 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 5, 10))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:37,026 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:37,026 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:37,026 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:37,026 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:37,026 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:37,029 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 8, 9))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:37,029 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:37,030 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:37,030 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:37,030 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:37,030 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:37,031 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 4, 23))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:37,031 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:37,031 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:37,032 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:37,032 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:37,032 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:37,039 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:33205'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:37,039 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 9, 22))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:37,039 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:37,039 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:37,039 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:37,039 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:37,039 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:37,040 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 20, 11))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:37,040 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:37,040 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:37,040 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:37,040 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:37,040 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:37,041 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:41241'. Reason: worker-handle-scheduler-connection-broken
2025-12-05 10:40:37,043 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-4d5a2bf92cca46082e8f8f8883029ff2', 9, 9, 8))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-05 10:40:37,043 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-05 10:40:37,044 - distributed.worker - INFO - Removing Worker plugin qme_utils.py766c9118-c65a-423d-af80-2fa87fb295f4
2025-12-05 10:40:37,044 - distributed.worker - INFO - Removing Worker plugin qme_vars.py51f408bf-d4f6-4a5a-b859-39d4d86368bf
2025-12-05 10:40:37,044 - distributed.worker - INFO - Removing Worker plugin qme_train.py20db7f30-a2c1-4fc8-9162-af3514f025a3
2025-12-05 10:40:37,044 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya45a2914-6fac-4fe4-ae60-50d1c1eb7f23
2025-12-05 10:40:38,683 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:38,686 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:38,698 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:38,698 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:38,706 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:38,713 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:38,741 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:39,827 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:39,827 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:39,849 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:39,860 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:39,863 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:39,866 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:39,873 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:39,884 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:39,884 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:39,889 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:39,893 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:39,895 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:39,896 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:39,902 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:39,917 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:39,923 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:39,947 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:39,951 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:39,973 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:39,986 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:40,004 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:40,090 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:40,397 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:40,437 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:40,475 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:40,597 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:40,615 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:40,617 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:40,621 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:40,625 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:40,625 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:40,628 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:40,632 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:40,632 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:40,635 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:40,635 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:40,636 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:40,638 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:40,641 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:40,655 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:40,666 - distributed.nanny - INFO - Worker closed
2025-12-05 10:40:40,688 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-12-05 10:40:40,701 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-12-05 10:40:41,830 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-12-05 10:40:41,869 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-12-05 10:40:41,872 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-12-05 10:40:41,897 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-12-05 10:40:41,899 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-12-05 10:40:41,902 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-12-05 10:40:41,909 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-12-05 10:40:41,932 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-12-05 10:40:42,070 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:35551'. Reason: nanny-close-gracefully
2025-12-05 10:40:42,079 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:35551' closed.
2025-12-05 10:40:42,096 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-12-05 10:40:42,150 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:33671'. Reason: nanny-close-gracefully
2025-12-05 10:40:42,151 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:33671' closed.
2025-12-05 10:40:42,191 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:39291'. Reason: nanny-close-gracefully
2025-12-05 10:40:42,197 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:39291' closed.
2025-12-05 10:40:42,243 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:43279'. Reason: nanny-close-gracefully
2025-12-05 10:40:42,244 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:43279' closed.
2025-12-05 10:40:42,309 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:42579'. Reason: nanny-close-gracefully
2025-12-05 10:40:42,311 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:42579' closed.
2025-12-05 10:40:42,362 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:45191'. Reason: nanny-close-gracefully
2025-12-05 10:40:42,371 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:45191' closed.
2025-12-05 10:40:42,402 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-12-05 10:40:42,479 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:34053'. Reason: nanny-close-gracefully
2025-12-05 10:40:42,480 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:34053' closed.
2025-12-05 10:40:42,631 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-12-05 10:40:42,643 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-12-05 10:40:42,646 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-12-05 10:40:42,673 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-12-05 10:40:43,413 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:36111'. Reason: nanny-close-gracefully
2025-12-05 10:40:43,433 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:36111' closed.
2025-12-05 10:40:43,470 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:43863'. Reason: nanny-close-gracefully
2025-12-05 10:40:43,472 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:43863' closed.
2025-12-05 10:40:43,531 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:36565'. Reason: nanny-close-gracefully
2025-12-05 10:40:43,539 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:36565' closed.
2025-12-05 10:40:43,603 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:35257'. Reason: nanny-close-gracefully
2025-12-05 10:40:43,604 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:35257' closed.
