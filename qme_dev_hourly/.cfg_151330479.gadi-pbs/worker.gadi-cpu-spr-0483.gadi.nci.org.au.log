Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-10-01 11:43:28,415 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:42173'
2025-10-01 11:43:28,425 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:43101'
2025-10-01 11:43:28,428 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:35641'
2025-10-01 11:43:28,432 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:43375'
2025-10-01 11:43:28,437 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:37505'
2025-10-01 11:43:28,440 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:36781'
2025-10-01 11:43:28,444 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:41119'
2025-10-01 11:43:28,448 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:40297'
2025-10-01 11:43:28,452 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:37115'
2025-10-01 11:43:28,457 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:42835'
2025-10-01 11:43:28,462 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:45241'
2025-10-01 11:43:28,466 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:45875'
2025-10-01 11:43:28,470 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:44987'
2025-10-01 11:43:28,475 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:41649'
2025-10-01 11:43:28,479 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:39161'
2025-10-01 11:43:28,483 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:36287'
2025-10-01 11:43:28,488 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:34939'
2025-10-01 11:43:28,492 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:42725'
2025-10-01 11:43:28,497 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:32823'
2025-10-01 11:43:28,501 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:40279'
2025-10-01 11:43:28,619 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:39595'
2025-10-01 11:43:28,623 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:36623'
2025-10-01 11:43:28,628 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:46009'
2025-10-01 11:43:28,638 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:34975'
2025-10-01 11:43:28,643 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:35431'
2025-10-01 11:43:28,647 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:38665'
2025-10-01 11:43:28,653 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:43239'
2025-10-01 11:43:28,657 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:39303'
2025-10-01 11:43:28,662 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:38653'
2025-10-01 11:43:28,666 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:46679'
2025-10-01 11:43:28,669 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:44809'
2025-10-01 11:43:28,674 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:41887'
2025-10-01 11:43:28,679 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:35813'
2025-10-01 11:43:28,684 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:43201'
2025-10-01 11:43:28,689 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:46287'
2025-10-01 11:43:28,693 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:35295'
2025-10-01 11:43:28,697 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:33819'
2025-10-01 11:43:28,703 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:46839'
2025-10-01 11:43:28,707 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:43635'
2025-10-01 11:43:28,713 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:39131'
2025-10-01 11:43:28,717 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:46523'
2025-10-01 11:43:28,720 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:42157'
2025-10-01 11:43:28,725 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:39399'
2025-10-01 11:43:28,730 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:35167'
2025-10-01 11:43:28,734 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:37791'
2025-10-01 11:43:28,738 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:33865'
2025-10-01 11:43:28,743 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:36633'
2025-10-01 11:43:28,748 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:34191'
2025-10-01 11:43:28,752 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:40939'
2025-10-01 11:43:28,756 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:37575'
2025-10-01 11:43:28,760 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:42697'
2025-10-01 11:43:28,765 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.51:35179'
2025-10-01 11:43:29,770 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:46659
2025-10-01 11:43:29,770 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:42911
2025-10-01 11:43:29,770 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:46119
2025-10-01 11:43:29,770 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:35969
2025-10-01 11:43:29,770 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:46659
2025-10-01 11:43:29,770 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:42911
2025-10-01 11:43:29,770 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:43571
2025-10-01 11:43:29,770 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:42387
2025-10-01 11:43:29,770 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:41361
2025-10-01 11:43:29,770 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:46119
2025-10-01 11:43:29,770 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:32811
2025-10-01 11:43:29,770 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:43907
2025-10-01 11:43:29,770 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:35969
2025-10-01 11:43:29,770 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:37511
2025-10-01 11:43:29,770 - distributed.worker - INFO -          dashboard at:          10.6.102.51:34897
2025-10-01 11:43:29,770 - distributed.worker - INFO -          dashboard at:          10.6.102.51:34115
2025-10-01 11:43:29,770 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:36973
2025-10-01 11:43:29,770 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:43173
2025-10-01 11:43:29,770 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:33821
2025-10-01 11:43:29,770 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:43571
2025-10-01 11:43:29,770 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:42387
2025-10-01 11:43:29,770 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:41361
2025-10-01 11:43:29,770 - distributed.worker - INFO -          dashboard at:          10.6.102.51:34825
2025-10-01 11:43:29,771 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:32811
2025-10-01 11:43:29,771 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:43907
2025-10-01 11:43:29,771 - distributed.worker - INFO -          dashboard at:          10.6.102.51:42597
2025-10-01 11:43:29,771 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,771 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:37511
2025-10-01 11:43:29,771 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,771 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:36973
2025-10-01 11:43:29,771 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:43173
2025-10-01 11:43:29,771 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:33821
2025-10-01 11:43:29,771 - distributed.worker - INFO -          dashboard at:          10.6.102.51:45461
2025-10-01 11:43:29,771 - distributed.worker - INFO -          dashboard at:          10.6.102.51:35605
2025-10-01 11:43:29,771 - distributed.worker - INFO -          dashboard at:          10.6.102.51:39663
2025-10-01 11:43:29,771 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,771 - distributed.worker - INFO -          dashboard at:          10.6.102.51:32787
2025-10-01 11:43:29,771 - distributed.worker - INFO -          dashboard at:          10.6.102.51:38203
2025-10-01 11:43:29,771 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO -          dashboard at:          10.6.102.51:42331
2025-10-01 11:43:29,771 - distributed.worker - INFO -          dashboard at:          10.6.102.51:38967
2025-10-01 11:43:29,771 - distributed.worker - INFO -          dashboard at:          10.6.102.51:35723
2025-10-01 11:43:29,771 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,771 - distributed.worker - INFO -          dashboard at:          10.6.102.51:39793
2025-10-01 11:43:29,771 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,771 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,771 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,771 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,771 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,771 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,771 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,771 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,771 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,771 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,771 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,771 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,771 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,771 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-09icr40o
2025-10-01 11:43:29,771 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,771 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-k3f6u7q4
2025-10-01 11:43:29,771 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,771 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,771 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,771 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,771 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,771 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,771 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-e9v_7bed
2025-10-01 11:43:29,771 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,771 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,771 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-g7toldui
2025-10-01 11:43:29,771 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-vnv39jpt
2025-10-01 11:43:29,771 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,771 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-4l8ckcjz
2025-10-01 11:43:29,771 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-tr34hh9i
2025-10-01 11:43:29,771 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-byqixr1i
2025-10-01 11:43:29,771 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:38767
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-x1fbb4e5
2025-10-01 11:43:29,771 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-j_fwmddd
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-7x3apbpw
2025-10-01 11:43:29,771 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-cszo3_so
2025-10-01 11:43:29,771 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-oaujuqei
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:38767
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO -          dashboard at:          10.6.102.51:39643
2025-10-01 11:43:29,771 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,771 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,771 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,772 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,772 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-ky02hhae
2025-10-01 11:43:29,772 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,776 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:46623
2025-10-01 11:43:29,776 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:46623
2025-10-01 11:43:29,776 - distributed.worker - INFO -          dashboard at:          10.6.102.51:44217
2025-10-01 11:43:29,776 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,776 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,776 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,776 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,776 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-4g_8zq7m
2025-10-01 11:43:29,776 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,777 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:45051
2025-10-01 11:43:29,778 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:45051
2025-10-01 11:43:29,778 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:41327
2025-10-01 11:43:29,778 - distributed.worker - INFO -          dashboard at:          10.6.102.51:36921
2025-10-01 11:43:29,778 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:41327
2025-10-01 11:43:29,778 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,778 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,778 - distributed.worker - INFO -          dashboard at:          10.6.102.51:44559
2025-10-01 11:43:29,778 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,778 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,778 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,778 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,778 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,778 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,778 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-vtubc8gl
2025-10-01 11:43:29,778 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-_0o07ozf
2025-10-01 11:43:29,778 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,778 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,781 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:45917
2025-10-01 11:43:29,781 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:45917
2025-10-01 11:43:29,781 - distributed.worker - INFO -          dashboard at:          10.6.102.51:46783
2025-10-01 11:43:29,781 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,781 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,781 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,781 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,781 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-h4voi1es
2025-10-01 11:43:29,781 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,797 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,797 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,797 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,798 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,803 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,803 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,803 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,804 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,809 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,810 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,810 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,812 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,814 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,815 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,815 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,817 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,818 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,819 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,819 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,820 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,820 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,821 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,822 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,823 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,823 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,824 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,824 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,826 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,826 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,827 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,827 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,829 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,830 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,831 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,832 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,832 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,833 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,834 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,834 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,835 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,835 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,836 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,836 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,837 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,838 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,838 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,838 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,839 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,839 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,839 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,840 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,840 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,841 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,842 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,842 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,843 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,844 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,845 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,845 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,845 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,846 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,847 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,847 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,847 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,848 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,849 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,849 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,850 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,850 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,851 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,851 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,852 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:41283
2025-10-01 11:43:29,852 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:41283
2025-10-01 11:43:29,852 - distributed.worker - INFO -          dashboard at:          10.6.102.51:35031
2025-10-01 11:43:29,852 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,852 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,852 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,852 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,852 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-51_asvhd
2025-10-01 11:43:29,852 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,853 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,893 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,894 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,894 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,896 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,900 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:44091
2025-10-01 11:43:29,900 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:44091
2025-10-01 11:43:29,900 - distributed.worker - INFO -          dashboard at:          10.6.102.51:45067
2025-10-01 11:43:29,900 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,900 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,900 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,900 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,900 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-ly8vcmxt
2025-10-01 11:43:29,900 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,912 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:34289
2025-10-01 11:43:29,912 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:34289
2025-10-01 11:43:29,912 - distributed.worker - INFO -          dashboard at:          10.6.102.51:42147
2025-10-01 11:43:29,912 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,912 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,912 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,912 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,912 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-d3f1z54o
2025-10-01 11:43:29,912 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,914 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:39969
2025-10-01 11:43:29,914 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:39969
2025-10-01 11:43:29,914 - distributed.worker - INFO -          dashboard at:          10.6.102.51:36081
2025-10-01 11:43:29,914 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,914 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,914 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,914 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,914 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-c_vxzgeh
2025-10-01 11:43:29,914 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,915 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:34585
2025-10-01 11:43:29,915 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:34585
2025-10-01 11:43:29,915 - distributed.worker - INFO -          dashboard at:          10.6.102.51:33483
2025-10-01 11:43:29,915 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,915 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,915 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,915 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,915 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-qkpq0tm_
2025-10-01 11:43:29,915 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,923 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:33947
2025-10-01 11:43:29,924 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:33947
2025-10-01 11:43:29,924 - distributed.worker - INFO -          dashboard at:          10.6.102.51:40839
2025-10-01 11:43:29,924 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,924 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,924 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,924 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,924 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-k8gjaroy
2025-10-01 11:43:29,924 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,924 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,925 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,925 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,926 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,927 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,927 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,927 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,928 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,931 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:39641
2025-10-01 11:43:29,931 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:39641
2025-10-01 11:43:29,931 - distributed.worker - INFO -          dashboard at:          10.6.102.51:42187
2025-10-01 11:43:29,931 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,931 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,931 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,931 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,931 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-8hax8qpc
2025-10-01 11:43:29,931 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,936 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,936 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,936 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,938 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,939 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,940 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,940 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,941 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,943 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:38119
2025-10-01 11:43:29,943 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:38119
2025-10-01 11:43:29,943 - distributed.worker - INFO -          dashboard at:          10.6.102.51:33815
2025-10-01 11:43:29,943 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,943 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,943 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,943 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,943 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-97o6ysn3
2025-10-01 11:43:29,943 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,949 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,950 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,950 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,952 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,955 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:35715
2025-10-01 11:43:29,955 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:35715
2025-10-01 11:43:29,955 - distributed.worker - INFO -          dashboard at:          10.6.102.51:37399
2025-10-01 11:43:29,955 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,955 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,955 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,955 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,955 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-0c9v7kg4
2025-10-01 11:43:29,955 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,958 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,959 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,959 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,960 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,960 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:37401
2025-10-01 11:43:29,960 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:37401
2025-10-01 11:43:29,960 - distributed.worker - INFO -          dashboard at:          10.6.102.51:44449
2025-10-01 11:43:29,960 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,961 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,961 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,961 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,961 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-j6g4ygsz
2025-10-01 11:43:29,961 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,964 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,965 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,965 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,967 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,971 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,972 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,972 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,973 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,973 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:33683
2025-10-01 11:43:29,973 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:33683
2025-10-01 11:43:29,973 - distributed.worker - INFO -          dashboard at:          10.6.102.51:43485
2025-10-01 11:43:29,973 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,973 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,973 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:29,973 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:29,973 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-w_ey8jhw
2025-10-01 11:43:29,973 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,975 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,975 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,975 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,977 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:29,997 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:29,998 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:29,998 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:29,999 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,000 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:46801
2025-10-01 11:43:30,000 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:46801
2025-10-01 11:43:30,000 - distributed.worker - INFO -          dashboard at:          10.6.102.51:43673
2025-10-01 11:43:30,000 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,000 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,000 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,000 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,000 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-iajel9lh
2025-10-01 11:43:30,000 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,005 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:36051
2025-10-01 11:43:30,005 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:36051
2025-10-01 11:43:30,005 - distributed.worker - INFO -          dashboard at:          10.6.102.51:44169
2025-10-01 11:43:30,005 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,005 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,005 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,005 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,005 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-fvky55sc
2025-10-01 11:43:30,005 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,020 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:46303
2025-10-01 11:43:30,020 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:46303
2025-10-01 11:43:30,020 - distributed.worker - INFO -          dashboard at:          10.6.102.51:33937
2025-10-01 11:43:30,020 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,020 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,020 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,021 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,021 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-qdyo28g2
2025-10-01 11:43:30,021 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,025 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,026 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,027 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,028 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,033 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,033 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,034 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,035 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,047 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,048 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,048 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,050 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,059 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:33591
2025-10-01 11:43:30,059 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:33591
2025-10-01 11:43:30,059 - distributed.worker - INFO -          dashboard at:          10.6.102.51:40201
2025-10-01 11:43:30,059 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,059 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,059 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,059 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,059 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-cni72i00
2025-10-01 11:43:30,059 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,085 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,086 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,086 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,088 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,111 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:44965
2025-10-01 11:43:30,111 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:44965
2025-10-01 11:43:30,111 - distributed.worker - INFO -          dashboard at:          10.6.102.51:40417
2025-10-01 11:43:30,112 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,112 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,112 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,112 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,112 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-2k785n9n
2025-10-01 11:43:30,112 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,136 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,137 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,137 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,138 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,199 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:42653
2025-10-01 11:43:30,199 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:42653
2025-10-01 11:43:30,199 - distributed.worker - INFO -          dashboard at:          10.6.102.51:39101
2025-10-01 11:43:30,199 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,199 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,199 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,199 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,199 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-zg31pi5r
2025-10-01 11:43:30,199 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,200 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:46349
2025-10-01 11:43:30,200 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:46349
2025-10-01 11:43:30,200 - distributed.worker - INFO -          dashboard at:          10.6.102.51:38811
2025-10-01 11:43:30,200 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,200 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,200 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,200 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,200 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-b8fnatbp
2025-10-01 11:43:30,200 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,201 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:46499
2025-10-01 11:43:30,202 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:46499
2025-10-01 11:43:30,202 - distributed.worker - INFO -          dashboard at:          10.6.102.51:46621
2025-10-01 11:43:30,202 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,202 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,202 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,202 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,202 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-ke9v5ib_
2025-10-01 11:43:30,202 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,204 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:34301
2025-10-01 11:43:30,204 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:34301
2025-10-01 11:43:30,204 - distributed.worker - INFO -          dashboard at:          10.6.102.51:46365
2025-10-01 11:43:30,204 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,204 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,204 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,204 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,204 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-rrv7vzl5
2025-10-01 11:43:30,204 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,205 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:38143
2025-10-01 11:43:30,205 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:38143
2025-10-01 11:43:30,205 - distributed.worker - INFO -          dashboard at:          10.6.102.51:44829
2025-10-01 11:43:30,205 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,205 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,205 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,205 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,205 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-9qx9utdt
2025-10-01 11:43:30,205 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,205 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:33837
2025-10-01 11:43:30,205 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:33837
2025-10-01 11:43:30,205 - distributed.worker - INFO -          dashboard at:          10.6.102.51:45015
2025-10-01 11:43:30,206 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,206 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,206 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,206 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,206 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-7f7gsgv0
2025-10-01 11:43:30,206 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,212 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:44805
2025-10-01 11:43:30,212 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:44805
2025-10-01 11:43:30,212 - distributed.worker - INFO -          dashboard at:          10.6.102.51:46167
2025-10-01 11:43:30,212 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,212 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,212 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,212 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,212 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-gmicgsd3
2025-10-01 11:43:30,213 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,213 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:40665
2025-10-01 11:43:30,213 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:40665
2025-10-01 11:43:30,213 - distributed.worker - INFO -          dashboard at:          10.6.102.51:36167
2025-10-01 11:43:30,213 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,213 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,213 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,213 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,213 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-os8s334w
2025-10-01 11:43:30,213 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,213 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:35249
2025-10-01 11:43:30,213 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:35249
2025-10-01 11:43:30,213 - distributed.worker - INFO -          dashboard at:          10.6.102.51:33881
2025-10-01 11:43:30,213 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,213 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,214 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,214 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,214 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-dn54d9de
2025-10-01 11:43:30,214 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,215 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:46759
2025-10-01 11:43:30,215 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:46759
2025-10-01 11:43:30,215 - distributed.worker - INFO -          dashboard at:          10.6.102.51:44477
2025-10-01 11:43:30,215 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,215 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,215 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,215 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,215 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-xeue0j89
2025-10-01 11:43:30,215 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,217 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:45447
2025-10-01 11:43:30,217 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:45447
2025-10-01 11:43:30,217 - distributed.worker - INFO -          dashboard at:          10.6.102.51:42951
2025-10-01 11:43:30,217 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,217 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,217 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:39293
2025-10-01 11:43:30,217 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,217 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,217 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:39293
2025-10-01 11:43:30,217 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-_41qrieo
2025-10-01 11:43:30,217 - distributed.worker - INFO -          dashboard at:          10.6.102.51:40689
2025-10-01 11:43:30,217 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,217 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,217 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,217 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,217 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,217 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-1d9vi2cv
2025-10-01 11:43:30,217 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,218 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,219 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,219 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,219 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,220 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:45483
2025-10-01 11:43:30,220 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:45483
2025-10-01 11:43:30,220 - distributed.worker - INFO -          dashboard at:          10.6.102.51:46267
2025-10-01 11:43:30,220 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,220 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,221 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,221 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,221 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-_lcnjb8w
2025-10-01 11:43:30,221 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,221 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:43181
2025-10-01 11:43:30,221 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:43181
2025-10-01 11:43:30,221 - distributed.worker - INFO -          dashboard at:          10.6.102.51:46325
2025-10-01 11:43:30,221 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,221 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,221 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,221 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,221 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-iy25etzu
2025-10-01 11:43:30,221 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,224 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:34303
2025-10-01 11:43:30,224 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:33577
2025-10-01 11:43:30,224 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:34303
2025-10-01 11:43:30,224 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:33577
2025-10-01 11:43:30,224 - distributed.worker - INFO -          dashboard at:          10.6.102.51:42379
2025-10-01 11:43:30,224 - distributed.worker - INFO -          dashboard at:          10.6.102.51:41769
2025-10-01 11:43:30,224 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,224 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,224 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,224 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,224 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,224 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,224 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,224 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,224 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-cd5iwbmn
2025-10-01 11:43:30,224 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-ync525gj
2025-10-01 11:43:30,224 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,224 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,228 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,229 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,230 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:42131
2025-10-01 11:43:30,230 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,230 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:42131
2025-10-01 11:43:30,230 - distributed.worker - INFO -          dashboard at:          10.6.102.51:33397
2025-10-01 11:43:30,230 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,230 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,230 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,230 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,230 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-0kyf62ln
2025-10-01 11:43:30,230 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,231 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,232 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,234 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,234 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,235 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,236 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,236 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,236 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,237 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,241 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,242 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,243 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,244 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,244 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,246 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,246 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,247 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,248 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,249 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,249 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,250 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,251 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,252 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.51:46875
2025-10-01 11:43:30,252 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.51:46875
2025-10-01 11:43:30,252 - distributed.worker - INFO -          dashboard at:          10.6.102.51:46831
2025-10-01 11:43:30,252 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,252 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,252 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:30,252 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:30,252 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-mhh8h26d
2025-10-01 11:43:30,252 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,252 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,252 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,253 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,254 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,254 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,254 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,255 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,256 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,256 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,256 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,257 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,258 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,258 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,258 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,259 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,260 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,260 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,260 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,260 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,260 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,260 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,261 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,261 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,263 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,264 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,265 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,265 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,266 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,266 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,266 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,268 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,268 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,269 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,269 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,269 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,270 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,270 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,270 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,272 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:30,275 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:30,276 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:30,276 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:30,277 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 17:43:18,716 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,716 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,716 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,717 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:33577. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,717 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,717 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:46875. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,717 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:43181. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,718 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:44805. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,718 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:38143. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,717 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,719 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:46801. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,719 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:33683. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,719 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:39641. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,719 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:34289. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,717 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,719 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:34585. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,717 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,717 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,720 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:42131. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,717 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,720 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,718 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,719 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,719 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,721 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:45483. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,719 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,719 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,721 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:41361. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,719 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,719 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,721 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:37511. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,719 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,720 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,721 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:42911. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,722 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:39293. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,720 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,720 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,722 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:34301. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,722 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,722 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:45051. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,720 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,722 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:33837. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,722 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:43173. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,722 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:40665. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,722 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:43571. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,722 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:33821. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,723 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:46659. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,723 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:43907. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,723 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:35249. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,723 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:32811. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,723 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:46623. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,723 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:46119. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,723 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:45917. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,724 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:33591. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,724 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:36973. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,722 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,724 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:38767. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,724 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:34303. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,724 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:42387. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,724 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:35969. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,725 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:46759. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,725 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:46349. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,725 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:44965. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,725 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:41283. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,725 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:37401. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,725 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:42653. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,726 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:46499. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,726 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:45447. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,726 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:44091. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,726 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:36051. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,725 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:41327. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,726 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:46303. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,727 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:33947. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,728 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:39969. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,729 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:35715. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,729 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.51:38119. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,731 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.51:47954 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,738 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:40939'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,740 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,735 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.51:59532 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,745 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,747 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:35179'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,748 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:44809'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,748 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,748 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:46009'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,748 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:35813'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,748 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,749 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:39303'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,749 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,749 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,749 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:43201'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,749 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:32823'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,749 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,750 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:41887'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,749 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,750 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:33819'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,750 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,750 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,751 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,751 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,753 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,754 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,754 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,754 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,755 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,755 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,755 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,756 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,760 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:40279'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,761 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:46679'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,761 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:43635'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,761 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:37115'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,761 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:41119'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,762 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:42835'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,762 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,762 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:42697'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,762 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:45875'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,762 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,762 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,762 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:39399'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,762 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,762 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:36633'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,762 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,763 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,763 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:46523'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,763 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,763 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:46839'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,763 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:35431'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,763 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,763 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:44987'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,763 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,764 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:42173'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,764 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,764 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:39161'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,764 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,764 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:36623'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,764 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:38665'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,764 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,764 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,764 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:42725'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,764 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,764 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,765 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:45241'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,764 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,765 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:35641'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,765 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:37505'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,765 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:43101'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,765 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:40297'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,765 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,765 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:33865'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,765 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,765 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,766 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:41649'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,766 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:34975'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,766 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,766 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:37575'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,766 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:43375'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,766 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,766 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:36287'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,766 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,766 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:34191'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,766 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,766 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,766 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,767 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:34939'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,766 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,766 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,767 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,767 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:37791'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,767 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,767 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:42157'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,767 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,767 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:38653'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,767 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,768 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:39131'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,768 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:35167'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,768 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:43239'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,768 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,768 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:39595'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,768 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:36781'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,768 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,768 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,768 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,769 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,769 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,769 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,769 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,769 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,770 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,771 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,771 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:35295'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,771 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.51:46287'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,772 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,772 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,773 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,773 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,773 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,773 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,774 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,774 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,775 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,776 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,776 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,776 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,765 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1535a4869410>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-01 17:43:18,776 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,776 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,776 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,777 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,767 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-01 17:43:18,778 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,779 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,779 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,779 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,779 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,779 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,779 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,779 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,780 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,780 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,781 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,781 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,781 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,781 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,782 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,782 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,782 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,771 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x146d19661310>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-01 17:43:18,770 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14e44ddc6c10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-01 17:43:18,782 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,783 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,773 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14d1c1152bd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-01 17:43:18,785 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,773 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1515d441ae10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-01 17:43:18,785 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,787 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,788 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,788 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,790 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,790 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:20,758 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,759 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,760 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,785 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,785 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,786 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,787 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,787 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,788 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,788 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,789 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,790 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,795 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,796 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,973 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:40939'. Reason: nanny-close-gracefully
2025-10-01 17:43:20,977 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:44809'. Reason: nanny-close-gracefully
2025-10-01 17:43:20,978 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:40939' closed.
2025-10-01 17:43:20,979 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:44809' closed.
2025-10-01 17:43:20,983 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:35179'. Reason: nanny-close-gracefully
2025-10-01 17:43:20,983 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:32823'. Reason: nanny-close-gracefully
2025-10-01 17:43:20,984 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:35179' closed.
2025-10-01 17:43:20,984 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:32823' closed.
2025-10-01 17:43:20,995 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:41887'. Reason: nanny-close-gracefully
2025-10-01 17:43:20,996 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:41887' closed.
2025-10-01 17:43:21,002 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:43201'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,004 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:43201' closed.
2025-10-01 17:43:21,011 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:35813'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,012 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:46009'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,019 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:35813' closed.
2025-10-01 17:43:21,020 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:46009' closed.
2025-10-01 17:43:21,044 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:40279'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,046 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:40279' closed.
2025-10-01 17:43:21,067 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:33819'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,069 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:33819' closed.
2025-10-01 17:43:21,071 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:39131'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,071 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:39131' closed.
2025-10-01 17:43:21,078 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:39303'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,078 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:39303' closed.
2025-10-01 17:43:21,097 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:46287'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,098 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:46287' closed.
2025-10-01 17:43:21,103 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:35431'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,104 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:35431' closed.
2025-10-01 17:43:21,120 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:46523'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,121 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:46523' closed.
2025-10-01 17:43:21,124 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:37115'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,126 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:37115' closed.
2025-10-01 17:43:21,144 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:37505'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,146 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:37505' closed.
2025-10-01 17:43:21,151 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:40297'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,152 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:44987'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,153 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:40297' closed.
2025-10-01 17:43:21,153 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:44987' closed.
2025-10-01 17:43:21,155 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:45875'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,156 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:45875' closed.
2025-10-01 17:43:21,169 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:36633'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,169 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:42835'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,170 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:36623'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,171 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:36633' closed.
2025-10-01 17:43:21,171 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:42835' closed.
2025-10-01 17:43:21,171 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:36623' closed.
2025-10-01 17:43:21,174 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:38665'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,175 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:42697'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,176 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:38665' closed.
2025-10-01 17:43:21,176 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:42697' closed.
2025-10-01 17:43:21,186 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:43635'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,187 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:43635' closed.
2025-10-01 17:43:21,190 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:43101'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,191 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:43101' closed.
2025-10-01 17:43:21,204 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:39399'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,205 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:39399' closed.
2025-10-01 17:43:21,208 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:38653'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,209 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:38653' closed.
2025-10-01 17:43:21,212 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:45241'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,216 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:35167'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,217 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:36287'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,217 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:39595'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,218 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:45241' closed.
2025-10-01 17:43:21,220 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:33865'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,221 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:35167' closed.
2025-10-01 17:43:21,221 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:34975'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,222 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:36287' closed.
2025-10-01 17:43:21,222 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:39595' closed.
2025-10-01 17:43:21,223 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:33865' closed.
2025-10-01 17:43:21,223 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:34975' closed.
2025-10-01 17:43:21,235 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:41649'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,239 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:43239'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,239 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:42173'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,240 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:41119'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,241 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:43375'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,241 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:42725'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,242 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:41649' closed.
2025-10-01 17:43:21,243 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:46839'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,243 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:43239' closed.
2025-10-01 17:43:21,245 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:42173' closed.
2025-10-01 17:43:21,245 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:41119' closed.
2025-10-01 17:43:21,246 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:43375' closed.
2025-10-01 17:43:21,246 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:42725' closed.
2025-10-01 17:43:21,246 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:34191'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,247 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:46839' closed.
2025-10-01 17:43:21,247 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:34191' closed.
2025-10-01 17:43:21,271 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:46679'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,274 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:37791'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,275 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:46679' closed.
2025-10-01 17:43:21,276 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:42157'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,279 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:36781'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,282 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:37791' closed.
2025-10-01 17:43:21,282 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:35641'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,283 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:34939'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,283 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:39161'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,284 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:35295'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,284 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:42157' closed.
2025-10-01 17:43:21,285 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:36781' closed.
2025-10-01 17:43:21,286 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:35641' closed.
2025-10-01 17:43:21,286 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:34939' closed.
2025-10-01 17:43:21,286 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.51:37575'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,287 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:39161' closed.
2025-10-01 17:43:21,287 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:35295' closed.
2025-10-01 17:43:21,287 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.51:37575' closed.
2025-10-01 17:43:21,290 - distributed.dask_worker - INFO - End worker
