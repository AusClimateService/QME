Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 10:52:49,665 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:34181'
2025-09-03 10:52:49,675 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:42027'
2025-09-03 10:52:49,681 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:44021'
2025-09-03 10:52:49,687 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:43851'
2025-09-03 10:52:49,692 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:46701'
2025-09-03 10:52:49,725 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:41201'
2025-09-03 10:52:49,732 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:34953'
2025-09-03 10:52:49,736 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:39133'
2025-09-03 10:52:49,741 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:44505'
2025-09-03 10:52:49,747 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:38165'
2025-09-03 10:52:49,751 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:33709'
2025-09-03 10:52:49,755 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:34751'
2025-09-03 10:52:49,759 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:42633'
2025-09-03 10:52:49,764 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:44087'
2025-09-03 10:52:49,768 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:35745'
2025-09-03 10:52:49,774 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:39701'
2025-09-03 10:52:49,779 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:35611'
2025-09-03 10:52:49,783 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:40851'
2025-09-03 10:52:49,789 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:45737'
2025-09-03 10:52:49,796 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:42011'
2025-09-03 10:52:49,800 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:45411'
2025-09-03 10:52:49,805 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:40389'
2025-09-03 10:52:49,810 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:45713'
2025-09-03 10:52:49,814 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:41653'
2025-09-03 10:52:49,820 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:43095'
2025-09-03 10:52:49,825 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:44827'
2025-09-03 10:52:49,829 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:34645'
2025-09-03 10:52:49,833 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:42547'
2025-09-03 10:52:49,837 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:33833'
2025-09-03 10:52:49,842 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:46787'
2025-09-03 10:52:49,846 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:42309'
2025-09-03 10:52:49,851 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:44757'
2025-09-03 10:52:49,855 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:44795'
2025-09-03 10:52:49,859 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:37231'
2025-09-03 10:52:49,862 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:37243'
2025-09-03 10:52:49,865 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:43995'
2025-09-03 10:52:49,869 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:40581'
2025-09-03 10:52:49,872 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:34259'
2025-09-03 10:52:49,877 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:44535'
2025-09-03 10:52:49,881 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:35871'
2025-09-03 10:52:49,886 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:43561'
2025-09-03 10:52:49,964 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:37183'
2025-09-03 10:52:49,969 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:46861'
2025-09-03 10:52:49,973 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:35713'
2025-09-03 10:52:49,977 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:37499'
2025-09-03 10:52:49,982 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:43597'
2025-09-03 10:52:49,989 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:45573'
2025-09-03 10:52:49,993 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:37001'
2025-09-03 10:52:49,998 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:39375'
2025-09-03 10:52:50,002 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:43117'
2025-09-03 10:52:50,749 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:44775'
2025-09-03 10:52:50,752 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:46293'
2025-09-03 10:52:50,755 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:38095'
2025-09-03 10:52:50,760 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:46295'
2025-09-03 10:52:50,764 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:37803'
2025-09-03 10:52:50,770 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:45747'
2025-09-03 10:52:50,775 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:44055'
2025-09-03 10:52:50,780 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:40479'
2025-09-03 10:52:50,786 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:33943'
2025-09-03 10:52:50,789 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:38129'
2025-09-03 10:52:50,795 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:32845'
2025-09-03 10:52:50,801 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:34551'
2025-09-03 10:52:50,805 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:35241'
2025-09-03 10:52:50,811 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:34419'
2025-09-03 10:52:50,817 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:36333'
2025-09-03 10:52:50,820 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:34413'
2025-09-03 10:52:50,822 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:37209'
2025-09-03 10:52:50,824 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:38395'
2025-09-03 10:52:50,830 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:38199'
2025-09-03 10:52:50,833 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:41001'
2025-09-03 10:52:50,835 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:43427'
2025-09-03 10:52:50,838 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:35489'
2025-09-03 10:52:50,842 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:33873'
2025-09-03 10:52:50,843 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:43707'
2025-09-03 10:52:50,846 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:37545'
2025-09-03 10:52:50,849 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:44043'
2025-09-03 10:52:50,852 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:46349'
2025-09-03 10:52:50,856 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:42187'
2025-09-03 10:52:50,863 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:46667'
2025-09-03 10:52:50,868 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:36289'
2025-09-03 10:52:50,871 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:44843'
2025-09-03 10:52:50,873 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:36693'
2025-09-03 10:52:50,876 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:38759'
2025-09-03 10:52:50,879 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:33109'
2025-09-03 10:52:50,882 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:37873'
2025-09-03 10:52:50,887 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:40337'
2025-09-03 10:52:50,905 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:44697'
2025-09-03 10:52:50,909 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:34057'
2025-09-03 10:52:50,912 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:34087'
2025-09-03 10:52:50,915 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:43863'
2025-09-03 10:52:50,919 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:39301'
2025-09-03 10:52:50,922 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:35645'
2025-09-03 10:52:50,925 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:46465'
2025-09-03 10:52:50,931 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:34767'
2025-09-03 10:52:51,387 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:45267
2025-09-03 10:52:51,387 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:43991
2025-09-03 10:52:51,387 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:44543
2025-09-03 10:52:51,387 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:45267
2025-09-03 10:52:51,387 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:45253
2025-09-03 10:52:51,387 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:39287
2025-09-03 10:52:51,387 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:33113
2025-09-03 10:52:51,387 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:43991
2025-09-03 10:52:51,387 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:44543
2025-09-03 10:52:51,387 - distributed.worker - INFO -          dashboard at:          10.6.105.15:44271
2025-09-03 10:52:51,387 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:40867
2025-09-03 10:52:51,387 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:34341
2025-09-03 10:52:51,387 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:34649
2025-09-03 10:52:51,387 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:44887
2025-09-03 10:52:51,387 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:45253
2025-09-03 10:52:51,387 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:35967
2025-09-03 10:52:51,387 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:36623
2025-09-03 10:52:51,387 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:33111
2025-09-03 10:52:51,387 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:39287
2025-09-03 10:52:51,387 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:33113
2025-09-03 10:52:51,387 - distributed.worker - INFO -          dashboard at:          10.6.105.15:40515
2025-09-03 10:52:51,387 - distributed.worker - INFO -          dashboard at:          10.6.105.15:43625
2025-09-03 10:52:51,387 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,387 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:40867
2025-09-03 10:52:51,387 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:45157
2025-09-03 10:52:51,387 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:34341
2025-09-03 10:52:51,387 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:34649
2025-09-03 10:52:51,387 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:32885
2025-09-03 10:52:51,387 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:38161
2025-09-03 10:52:51,387 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:46611
2025-09-03 10:52:51,387 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:39291
2025-09-03 10:52:51,387 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:44887
2025-09-03 10:52:51,387 - distributed.worker - INFO -          dashboard at:          10.6.105.15:46197
2025-09-03 10:52:51,387 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:35967
2025-09-03 10:52:51,387 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:36623
2025-09-03 10:52:51,387 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:40083
2025-09-03 10:52:51,387 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:33111
2025-09-03 10:52:51,387 - distributed.worker - INFO -          dashboard at:          10.6.105.15:40395
2025-09-03 10:52:51,387 - distributed.worker - INFO -          dashboard at:          10.6.105.15:43563
2025-09-03 10:52:51,387 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,387 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:40083
2025-09-03 10:52:51,387 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,387 - distributed.worker - INFO -          dashboard at:          10.6.105.15:43047
2025-09-03 10:52:51,387 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:45157
2025-09-03 10:52:51,387 - distributed.worker - INFO -          dashboard at:          10.6.105.15:36797
2025-09-03 10:52:51,387 - distributed.worker - INFO -          dashboard at:          10.6.105.15:41677
2025-09-03 10:52:51,387 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:32885
2025-09-03 10:52:51,388 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,387 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:38161
2025-09-03 10:52:51,387 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:46611
2025-09-03 10:52:51,387 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:39291
2025-09-03 10:52:51,387 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,387 - distributed.worker - INFO -          dashboard at:          10.6.105.15:38361
2025-09-03 10:52:51,387 - distributed.worker - INFO -          dashboard at:          10.6.105.15:40937
2025-09-03 10:52:51,388 - distributed.worker - INFO -          dashboard at:          10.6.105.15:44889
2025-09-03 10:52:51,388 - distributed.worker - INFO -          dashboard at:          10.6.105.15:34703
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,388 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,388 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO -          dashboard at:          10.6.105.15:46387
2025-09-03 10:52:51,388 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,388 - distributed.worker - INFO -          dashboard at:          10.6.105.15:34769
2025-09-03 10:52:51,388 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO -          dashboard at:          10.6.105.15:35795
2025-09-03 10:52:51,388 - distributed.worker - INFO -          dashboard at:          10.6.105.15:33227
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO -          dashboard at:          10.6.105.15:38627
2025-09-03 10:52:51,388 - distributed.worker - INFO -          dashboard at:          10.6.105.15:34229
2025-09-03 10:52:51,388 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,388 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,388 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,388 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,388 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,388 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,388 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,388 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,388 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,388 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,388 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,388 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,388 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,388 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-v95gpos0
2025-09-03 10:52:51,388 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,388 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,388 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,388 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,388 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-gc71ys5z
2025-09-03 10:52:51,388 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,388 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,388 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,388 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ukgrr1vm
2025-09-03 10:52:51,388 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,388 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,388 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,388 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ydojw2qa
2025-09-03 10:52:51,388 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,388 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-o4fm8syy
2025-09-03 10:52:51,388 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,388 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,388 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,388 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,388 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,388 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,388 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:35935
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-57oal4oh
2025-09-03 10:52:51,388 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,388 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-4ph1zvw3
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-nlm5c70w
2025-09-03 10:52:51,388 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,388 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-fl8bky7x
2025-09-03 10:52:51,388 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,388 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,388 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-q37rjqxa
2025-09-03 10:52:51,388 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,388 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-n9c0vm1e
2025-09-03 10:52:51,388 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-lhx94n5e
2025-09-03 10:52:51,388 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-01msztzk
2025-09-03 10:52:51,388 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:35935
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-jcg26f4h
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-dwzu25wc
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-qun67i6f
2025-09-03 10:52:51,388 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-xilbz4th
2025-09-03 10:52:51,388 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-7ezfn_pe
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-wltvhfvm
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO -          dashboard at:          10.6.105.15:34451
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,389 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,389 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,389 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,389 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,389 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,389 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,389 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,389 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,389 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-6q2p0wdv
2025-09-03 10:52:51,389 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,390 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:45035
2025-09-03 10:52:51,390 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:45035
2025-09-03 10:52:51,390 - distributed.worker - INFO -          dashboard at:          10.6.105.15:39591
2025-09-03 10:52:51,390 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,390 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,390 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,390 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,390 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-uzfb6x_9
2025-09-03 10:52:51,390 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,413 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:34049
2025-09-03 10:52:51,413 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:34049
2025-09-03 10:52:51,413 - distributed.worker - INFO -          dashboard at:          10.6.105.15:36641
2025-09-03 10:52:51,413 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,413 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,413 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,413 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,413 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-yr0i7p5r
2025-09-03 10:52:51,413 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,447 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:41675
2025-09-03 10:52:51,447 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:41675
2025-09-03 10:52:51,447 - distributed.worker - INFO -          dashboard at:          10.6.105.15:38575
2025-09-03 10:52:51,447 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,447 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,447 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,447 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,447 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-gyun_g0f
2025-09-03 10:52:51,448 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,464 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:36615
2025-09-03 10:52:51,464 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:36615
2025-09-03 10:52:51,464 - distributed.worker - INFO -          dashboard at:          10.6.105.15:34733
2025-09-03 10:52:51,464 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,464 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,464 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,464 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-a6385007
2025-09-03 10:52:51,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,492 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:41985
2025-09-03 10:52:51,492 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:41985
2025-09-03 10:52:51,492 - distributed.worker - INFO -          dashboard at:          10.6.105.15:44383
2025-09-03 10:52:51,492 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,492 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,492 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,492 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,492 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-6fd1or3c
2025-09-03 10:52:51,492 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,512 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:37471
2025-09-03 10:52:51,512 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:37471
2025-09-03 10:52:51,512 - distributed.worker - INFO -          dashboard at:          10.6.105.15:45179
2025-09-03 10:52:51,512 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,512 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,512 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,512 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,512 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-8pwqk622
2025-09-03 10:52:51,512 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,609 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:36335
2025-09-03 10:52:51,609 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:36335
2025-09-03 10:52:51,609 - distributed.worker - INFO -          dashboard at:          10.6.105.15:34147
2025-09-03 10:52:51,609 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,609 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,609 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,609 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,609 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-uhc6yga1
2025-09-03 10:52:51,610 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,636 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:43877
2025-09-03 10:52:51,636 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:43877
2025-09-03 10:52:51,636 - distributed.worker - INFO -          dashboard at:          10.6.105.15:33175
2025-09-03 10:52:51,636 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,636 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,636 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,636 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,636 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-kg1q5yrs
2025-09-03 10:52:51,636 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,641 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:35033
2025-09-03 10:52:51,641 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:35033
2025-09-03 10:52:51,641 - distributed.worker - INFO -          dashboard at:          10.6.105.15:46091
2025-09-03 10:52:51,641 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,641 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,641 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,641 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,641 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-k6d763if
2025-09-03 10:52:51,641 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,645 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:42717
2025-09-03 10:52:51,645 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:42717
2025-09-03 10:52:51,645 - distributed.worker - INFO -          dashboard at:          10.6.105.15:41895
2025-09-03 10:52:51,645 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,645 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,645 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,645 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,645 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-g6axp5z8
2025-09-03 10:52:51,646 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,649 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:43475
2025-09-03 10:52:51,649 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:43475
2025-09-03 10:52:51,649 - distributed.worker - INFO -          dashboard at:          10.6.105.15:40655
2025-09-03 10:52:51,649 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,649 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,649 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,649 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,649 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-7ig0r5co
2025-09-03 10:52:51,649 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,655 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:45749
2025-09-03 10:52:51,655 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:45749
2025-09-03 10:52:51,655 - distributed.worker - INFO -          dashboard at:          10.6.105.15:35793
2025-09-03 10:52:51,656 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,656 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,656 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,656 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,656 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-r875tac1
2025-09-03 10:52:51,656 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,662 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:33681
2025-09-03 10:52:51,662 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:33681
2025-09-03 10:52:51,662 - distributed.worker - INFO -          dashboard at:          10.6.105.15:37999
2025-09-03 10:52:51,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,662 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,662 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,662 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,662 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_fkvv6f1
2025-09-03 10:52:51,662 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,669 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:41891
2025-09-03 10:52:51,669 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:41891
2025-09-03 10:52:51,669 - distributed.worker - INFO -          dashboard at:          10.6.105.15:36249
2025-09-03 10:52:51,669 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,669 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,669 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,669 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,669 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-974hx0uw
2025-09-03 10:52:51,669 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,670 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:37777
2025-09-03 10:52:51,670 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:37777
2025-09-03 10:52:51,670 - distributed.worker - INFO -          dashboard at:          10.6.105.15:33429
2025-09-03 10:52:51,670 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,670 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,670 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,670 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,671 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-3zc0unho
2025-09-03 10:52:51,671 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,675 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:42289
2025-09-03 10:52:51,675 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:42289
2025-09-03 10:52:51,675 - distributed.worker - INFO -          dashboard at:          10.6.105.15:37021
2025-09-03 10:52:51,675 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,675 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,675 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,676 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,676 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-a8y6ofvc
2025-09-03 10:52:51,676 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,680 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:45587
2025-09-03 10:52:51,680 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:45587
2025-09-03 10:52:51,680 - distributed.worker - INFO -          dashboard at:          10.6.105.15:40099
2025-09-03 10:52:51,680 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,680 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,680 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,681 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,681 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-tcnpfyuv
2025-09-03 10:52:51,681 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,688 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:35065
2025-09-03 10:52:51,688 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:35065
2025-09-03 10:52:51,688 - distributed.worker - INFO -          dashboard at:          10.6.105.15:43187
2025-09-03 10:52:51,688 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,688 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,688 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,688 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,688 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-rah0m29o
2025-09-03 10:52:51,688 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,696 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:35491
2025-09-03 10:52:51,696 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:35491
2025-09-03 10:52:51,696 - distributed.worker - INFO -          dashboard at:          10.6.105.15:43111
2025-09-03 10:52:51,696 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,696 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,696 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,696 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,696 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-jlsktnsz
2025-09-03 10:52:51,696 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,703 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:41621
2025-09-03 10:52:51,703 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:41621
2025-09-03 10:52:51,703 - distributed.worker - INFO -          dashboard at:          10.6.105.15:40289
2025-09-03 10:52:51,703 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,703 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,703 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,703 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,703 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-tkx2c79u
2025-09-03 10:52:51,703 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,704 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:45037
2025-09-03 10:52:51,704 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:45037
2025-09-03 10:52:51,704 - distributed.worker - INFO -          dashboard at:          10.6.105.15:34497
2025-09-03 10:52:51,704 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,704 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,704 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,704 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,704 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-k0d4oct6
2025-09-03 10:52:51,704 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,709 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:40299
2025-09-03 10:52:51,709 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:40299
2025-09-03 10:52:51,709 - distributed.worker - INFO -          dashboard at:          10.6.105.15:39329
2025-09-03 10:52:51,709 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,709 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,709 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,709 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,709 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-flwz062_
2025-09-03 10:52:51,709 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,716 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:37095
2025-09-03 10:52:51,716 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:37095
2025-09-03 10:52:51,716 - distributed.worker - INFO -          dashboard at:          10.6.105.15:44597
2025-09-03 10:52:51,717 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,717 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,717 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,717 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,717 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_6tc58xt
2025-09-03 10:52:51,717 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,727 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:45283
2025-09-03 10:52:51,727 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:45283
2025-09-03 10:52:51,727 - distributed.worker - INFO -          dashboard at:          10.6.105.15:42431
2025-09-03 10:52:51,727 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,727 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,727 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,727 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,727 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-zx9gwplh
2025-09-03 10:52:51,727 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,727 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:46559
2025-09-03 10:52:51,727 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:46559
2025-09-03 10:52:51,727 - distributed.worker - INFO -          dashboard at:          10.6.105.15:46703
2025-09-03 10:52:51,727 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,727 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,727 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,728 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,728 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-0be5rgdx
2025-09-03 10:52:51,728 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,728 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:42967
2025-09-03 10:52:51,728 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:42967
2025-09-03 10:52:51,729 - distributed.worker - INFO -          dashboard at:          10.6.105.15:43959
2025-09-03 10:52:51,729 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,729 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,729 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,729 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,729 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-k2nf7wob
2025-09-03 10:52:51,729 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,731 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:35827
2025-09-03 10:52:51,731 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:35827
2025-09-03 10:52:51,732 - distributed.worker - INFO -          dashboard at:          10.6.105.15:32829
2025-09-03 10:52:51,732 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,732 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,732 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,732 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,732 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:35911
2025-09-03 10:52:51,732 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-3ntc9spc
2025-09-03 10:52:51,732 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:35911
2025-09-03 10:52:51,732 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,732 - distributed.worker - INFO -          dashboard at:          10.6.105.15:43525
2025-09-03 10:52:51,732 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,732 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,732 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,732 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,732 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-as11bdm8
2025-09-03 10:52:51,732 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,733 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:40829
2025-09-03 10:52:51,733 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:40829
2025-09-03 10:52:51,733 - distributed.worker - INFO -          dashboard at:          10.6.105.15:34189
2025-09-03 10:52:51,733 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,733 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,733 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,733 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,734 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-emwi4tte
2025-09-03 10:52:51,734 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,734 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:40547
2025-09-03 10:52:51,734 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:40547
2025-09-03 10:52:51,734 - distributed.worker - INFO -          dashboard at:          10.6.105.15:34547
2025-09-03 10:52:51,734 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,734 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,734 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:51,734 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:51,734 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-zui33rh5
2025-09-03 10:52:51,734 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,014 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:37241'
2025-09-03 10:52:52,018 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:35671'
2025-09-03 10:52:52,021 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:39393'
2025-09-03 10:52:52,025 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:36567'
2025-09-03 10:52:52,028 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:45481'
2025-09-03 10:52:52,032 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:40057'
2025-09-03 10:52:52,036 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:44635'
2025-09-03 10:52:52,039 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:46487'
2025-09-03 10:52:52,044 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:39999'
2025-09-03 10:52:52,049 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.15:41387'
2025-09-03 10:52:52,297 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:38875
2025-09-03 10:52:52,297 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:38875
2025-09-03 10:52:52,297 - distributed.worker - INFO -          dashboard at:          10.6.105.15:40111
2025-09-03 10:52:52,297 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,297 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,298 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,298 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,298 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-pru2pb66
2025-09-03 10:52:52,298 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,300 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:35881
2025-09-03 10:52:52,301 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:35881
2025-09-03 10:52:52,301 - distributed.worker - INFO -          dashboard at:          10.6.105.15:42879
2025-09-03 10:52:52,301 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,301 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,301 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,301 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,301 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-wk4dqs0a
2025-09-03 10:52:52,301 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,315 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:41933
2025-09-03 10:52:52,316 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:41933
2025-09-03 10:52:52,316 - distributed.worker - INFO -          dashboard at:          10.6.105.15:40341
2025-09-03 10:52:52,316 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,316 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,316 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,316 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,316 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-kl3padoe
2025-09-03 10:52:52,316 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,344 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:44689
2025-09-03 10:52:52,344 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:44689
2025-09-03 10:52:52,344 - distributed.worker - INFO -          dashboard at:          10.6.105.15:39377
2025-09-03 10:52:52,344 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,344 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,344 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,344 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,344 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-cexptazj
2025-09-03 10:52:52,344 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,367 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:43901
2025-09-03 10:52:52,367 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:43901
2025-09-03 10:52:52,367 - distributed.worker - INFO -          dashboard at:          10.6.105.15:40073
2025-09-03 10:52:52,367 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,367 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,367 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,367 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,367 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-vuulsiuu
2025-09-03 10:52:52,367 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,376 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:39303
2025-09-03 10:52:52,377 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:39303
2025-09-03 10:52:52,377 - distributed.worker - INFO -          dashboard at:          10.6.105.15:39267
2025-09-03 10:52:52,377 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,377 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,377 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,377 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,377 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-754ovpvl
2025-09-03 10:52:52,377 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,450 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:44565
2025-09-03 10:52:52,450 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:44565
2025-09-03 10:52:52,450 - distributed.worker - INFO -          dashboard at:          10.6.105.15:34837
2025-09-03 10:52:52,450 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,450 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,450 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,450 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,450 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9f43tfo2
2025-09-03 10:52:52,450 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,517 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:38879
2025-09-03 10:52:52,518 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:38879
2025-09-03 10:52:52,518 - distributed.worker - INFO -          dashboard at:          10.6.105.15:44025
2025-09-03 10:52:52,518 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,518 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,518 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,518 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,518 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-5mfjg18j
2025-09-03 10:52:52,518 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,566 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:35499
2025-09-03 10:52:52,566 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:35499
2025-09-03 10:52:52,566 - distributed.worker - INFO -          dashboard at:          10.6.105.15:35493
2025-09-03 10:52:52,566 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,566 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,566 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,566 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,566 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-bzjx8wtp
2025-09-03 10:52:52,566 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,577 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:40875
2025-09-03 10:52:52,577 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:40875
2025-09-03 10:52:52,577 - distributed.worker - INFO -          dashboard at:          10.6.105.15:43801
2025-09-03 10:52:52,577 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,577 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,577 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,577 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,577 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-pnhs9sul
2025-09-03 10:52:52,577 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,589 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:36497
2025-09-03 10:52:52,589 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:36497
2025-09-03 10:52:52,589 - distributed.worker - INFO -          dashboard at:          10.6.105.15:45039
2025-09-03 10:52:52,589 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,589 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,589 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,589 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,589 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-vq12lzp6
2025-09-03 10:52:52,589 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,596 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:40471
2025-09-03 10:52:52,597 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:40471
2025-09-03 10:52:52,597 - distributed.worker - INFO -          dashboard at:          10.6.105.15:43837
2025-09-03 10:52:52,597 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,597 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,597 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,597 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,597 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-41y_75wx
2025-09-03 10:52:52,597 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,602 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:41611
2025-09-03 10:52:52,602 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:41611
2025-09-03 10:52:52,602 - distributed.worker - INFO -          dashboard at:          10.6.105.15:40935
2025-09-03 10:52:52,603 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,603 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,603 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,603 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,603 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-07kltm9l
2025-09-03 10:52:52,603 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,610 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:34783
2025-09-03 10:52:52,610 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:34783
2025-09-03 10:52:52,610 - distributed.worker - INFO -          dashboard at:          10.6.105.15:40197
2025-09-03 10:52:52,610 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,610 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,610 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,610 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,610 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-cr1ay682
2025-09-03 10:52:52,610 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,620 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:43281
2025-09-03 10:52:52,620 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:43281
2025-09-03 10:52:52,620 - distributed.worker - INFO -          dashboard at:          10.6.105.15:42219
2025-09-03 10:52:52,620 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,620 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,620 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,620 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,620 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-rfzvis5y
2025-09-03 10:52:52,620 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,633 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:34643
2025-09-03 10:52:52,633 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:34643
2025-09-03 10:52:52,633 - distributed.worker - INFO -          dashboard at:          10.6.105.15:45051
2025-09-03 10:52:52,633 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,633 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,633 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,633 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,633 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-7mg3eq10
2025-09-03 10:52:52,633 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,635 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:45897
2025-09-03 10:52:52,635 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:45897
2025-09-03 10:52:52,635 - distributed.worker - INFO -          dashboard at:          10.6.105.15:45871
2025-09-03 10:52:52,635 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,635 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,635 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,635 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,635 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-x3j_4pse
2025-09-03 10:52:52,635 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,642 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:38301
2025-09-03 10:52:52,642 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:38301
2025-09-03 10:52:52,642 - distributed.worker - INFO -          dashboard at:          10.6.105.15:41565
2025-09-03 10:52:52,642 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,642 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,642 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,642 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,642 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-qomzi_ru
2025-09-03 10:52:52,642 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,643 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:36723
2025-09-03 10:52:52,643 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:36723
2025-09-03 10:52:52,643 - distributed.worker - INFO -          dashboard at:          10.6.105.15:43097
2025-09-03 10:52:52,643 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,643 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,643 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,643 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,643 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_iqlr9pz
2025-09-03 10:52:52,643 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,645 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:34763
2025-09-03 10:52:52,645 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:34763
2025-09-03 10:52:52,645 - distributed.worker - INFO -          dashboard at:          10.6.105.15:35249
2025-09-03 10:52:52,645 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,645 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,645 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,645 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,645 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-3fb1p4mv
2025-09-03 10:52:52,645 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,649 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:41923
2025-09-03 10:52:52,649 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:41923
2025-09-03 10:52:52,649 - distributed.worker - INFO -          dashboard at:          10.6.105.15:33551
2025-09-03 10:52:52,649 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,649 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,649 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,649 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,649 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-zj0gnzv5
2025-09-03 10:52:52,649 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,692 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:39623
2025-09-03 10:52:52,692 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:39623
2025-09-03 10:52:52,692 - distributed.worker - INFO -          dashboard at:          10.6.105.15:40733
2025-09-03 10:52:52,692 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,692 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,692 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,692 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,692 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-c6bxhfgz
2025-09-03 10:52:52,692 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,708 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:40549
2025-09-03 10:52:52,708 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:40549
2025-09-03 10:52:52,708 - distributed.worker - INFO -          dashboard at:          10.6.105.15:43509
2025-09-03 10:52:52,708 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,708 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,708 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,708 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,708 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-th2iyd3u
2025-09-03 10:52:52,708 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,725 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:43527
2025-09-03 10:52:52,725 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:43527
2025-09-03 10:52:52,725 - distributed.worker - INFO -          dashboard at:          10.6.105.15:43787
2025-09-03 10:52:52,725 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,725 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,725 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,725 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,725 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-z_dyuf7p
2025-09-03 10:52:52,725 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,763 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:52,765 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,765 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,766 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:52,780 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:52,781 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,781 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,783 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:52,812 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:52,813 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,813 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,814 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:38743
2025-09-03 10:52:52,815 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:38743
2025-09-03 10:52:52,815 - distributed.worker - INFO -          dashboard at:          10.6.105.15:34337
2025-09-03 10:52:52,815 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,815 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,815 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,815 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,815 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-dn7dzww9
2025-09-03 10:52:52,815 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,815 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:52,815 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:32787
2025-09-03 10:52:52,815 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:32787
2025-09-03 10:52:52,816 - distributed.worker - INFO -          dashboard at:          10.6.105.15:35969
2025-09-03 10:52:52,816 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,816 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,816 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,816 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,816 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-7wajb6vs
2025-09-03 10:52:52,816 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,850 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:41003
2025-09-03 10:52:52,850 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:41003
2025-09-03 10:52:52,850 - distributed.worker - INFO -          dashboard at:          10.6.105.15:43003
2025-09-03 10:52:52,850 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,850 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,850 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,850 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,850 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-tf_7h912
2025-09-03 10:52:52,851 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,951 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:38549
2025-09-03 10:52:52,952 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:38549
2025-09-03 10:52:52,952 - distributed.worker - INFO -          dashboard at:          10.6.105.15:46013
2025-09-03 10:52:52,952 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,952 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,952 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,952 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,952 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-cgd_lh1y
2025-09-03 10:52:52,952 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,958 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:45825
2025-09-03 10:52:52,958 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:45825
2025-09-03 10:52:52,958 - distributed.worker - INFO -          dashboard at:          10.6.105.15:38581
2025-09-03 10:52:52,958 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,958 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,958 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,958 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,958 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-40cr2k85
2025-09-03 10:52:52,958 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,964 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:42913
2025-09-03 10:52:52,964 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:42913
2025-09-03 10:52:52,964 - distributed.worker - INFO -          dashboard at:          10.6.105.15:34735
2025-09-03 10:52:52,964 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,964 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,964 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,964 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,964 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-jzlq908k
2025-09-03 10:52:52,964 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,966 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:44155
2025-09-03 10:52:52,966 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:44155
2025-09-03 10:52:52,966 - distributed.worker - INFO -          dashboard at:          10.6.105.15:34785
2025-09-03 10:52:52,967 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,967 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,967 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:44515
2025-09-03 10:52:52,967 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,967 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,967 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:44515
2025-09-03 10:52:52,967 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-e8xuiinp
2025-09-03 10:52:52,967 - distributed.worker - INFO -          dashboard at:          10.6.105.15:44199
2025-09-03 10:52:52,967 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,967 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,967 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,967 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,967 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,967 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-fd76alla
2025-09-03 10:52:52,967 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,979 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:36181
2025-09-03 10:52:52,980 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:36181
2025-09-03 10:52:52,980 - distributed.worker - INFO -          dashboard at:          10.6.105.15:46637
2025-09-03 10:52:52,980 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,980 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,980 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,980 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,980 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-spfiz98v
2025-09-03 10:52:52,980 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,982 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:41759
2025-09-03 10:52:52,982 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:41759
2025-09-03 10:52:52,982 - distributed.worker - INFO -          dashboard at:          10.6.105.15:45593
2025-09-03 10:52:52,982 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,982 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,982 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,982 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,982 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-0epszr6e
2025-09-03 10:52:52,982 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,984 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:43799
2025-09-03 10:52:52,984 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:43799
2025-09-03 10:52:52,984 - distributed.worker - INFO -          dashboard at:          10.6.105.15:42411
2025-09-03 10:52:52,984 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,984 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,984 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,984 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,984 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-h_pmf4rh
2025-09-03 10:52:52,984 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,990 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:33915
2025-09-03 10:52:52,990 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:33915
2025-09-03 10:52:52,990 - distributed.worker - INFO -          dashboard at:          10.6.105.15:34453
2025-09-03 10:52:52,990 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,990 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,990 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,990 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,990 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-o2k75kwh
2025-09-03 10:52:52,990 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,000 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:41715
2025-09-03 10:52:53,000 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:41715
2025-09-03 10:52:53,000 - distributed.worker - INFO -          dashboard at:          10.6.105.15:39571
2025-09-03 10:52:53,000 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,000 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,000 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,000 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,000 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-gg9z1381
2025-09-03 10:52:53,000 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:44067
2025-09-03 10:52:53,000 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,000 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:44067
2025-09-03 10:52:53,000 - distributed.worker - INFO -          dashboard at:          10.6.105.15:36265
2025-09-03 10:52:53,000 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,000 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,000 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,000 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,000 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-4f8n6xci
2025-09-03 10:52:53,000 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,001 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:43963
2025-09-03 10:52:53,001 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:43963
2025-09-03 10:52:53,001 - distributed.worker - INFO -          dashboard at:          10.6.105.15:38441
2025-09-03 10:52:53,001 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,001 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,001 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,001 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,001 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-y5eaz30j
2025-09-03 10:52:53,001 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,002 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:46355
2025-09-03 10:52:53,003 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:46355
2025-09-03 10:52:53,003 - distributed.worker - INFO -          dashboard at:          10.6.105.15:43415
2025-09-03 10:52:53,003 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,003 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,003 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,003 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,003 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-yemkyi_0
2025-09-03 10:52:53,003 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,006 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:42009
2025-09-03 10:52:53,006 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:42009
2025-09-03 10:52:53,006 - distributed.worker - INFO -          dashboard at:          10.6.105.15:41339
2025-09-03 10:52:53,006 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,006 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,006 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,006 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,006 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-bia8rcek
2025-09-03 10:52:53,006 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,009 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:40973
2025-09-03 10:52:53,009 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:40973
2025-09-03 10:52:53,009 - distributed.worker - INFO -          dashboard at:          10.6.105.15:44583
2025-09-03 10:52:53,009 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,009 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,009 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,009 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,009 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-yimsfeof
2025-09-03 10:52:53,009 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,021 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:35215
2025-09-03 10:52:53,021 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:35215
2025-09-03 10:52:53,021 - distributed.worker - INFO -          dashboard at:          10.6.105.15:36449
2025-09-03 10:52:53,021 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,021 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,021 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,021 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,021 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-lgfi2hkr
2025-09-03 10:52:53,021 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,024 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:41497
2025-09-03 10:52:53,024 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:41497
2025-09-03 10:52:53,025 - distributed.worker - INFO -          dashboard at:          10.6.105.15:34107
2025-09-03 10:52:53,025 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,025 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,025 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,025 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,025 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-8nj98301
2025-09-03 10:52:53,025 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,112 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:37595
2025-09-03 10:52:53,112 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:37595
2025-09-03 10:52:53,112 - distributed.worker - INFO -          dashboard at:          10.6.105.15:35747
2025-09-03 10:52:53,112 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,112 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,112 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,112 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,113 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-dv635szz
2025-09-03 10:52:53,113 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,114 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:44785
2025-09-03 10:52:53,115 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:44785
2025-09-03 10:52:53,115 - distributed.worker - INFO -          dashboard at:          10.6.105.15:32791
2025-09-03 10:52:53,115 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,115 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,115 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,115 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,115 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_67r4oaa
2025-09-03 10:52:53,115 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,122 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:46501
2025-09-03 10:52:53,122 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:46501
2025-09-03 10:52:53,122 - distributed.worker - INFO -          dashboard at:          10.6.105.15:38363
2025-09-03 10:52:53,122 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,122 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,122 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,122 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,122 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-043hu9sk
2025-09-03 10:52:53,122 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,124 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:39943
2025-09-03 10:52:53,125 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:39943
2025-09-03 10:52:53,125 - distributed.worker - INFO -          dashboard at:          10.6.105.15:36369
2025-09-03 10:52:53,125 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,125 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,125 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,125 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,125 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-xixrvomm
2025-09-03 10:52:53,125 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,125 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:41595
2025-09-03 10:52:53,125 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:41595
2025-09-03 10:52:53,125 - distributed.worker - INFO -          dashboard at:          10.6.105.15:46447
2025-09-03 10:52:53,125 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,125 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,125 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,125 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,125 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-8gnkcbmc
2025-09-03 10:52:53,125 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,137 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:39957
2025-09-03 10:52:53,137 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:39957
2025-09-03 10:52:53,137 - distributed.worker - INFO -          dashboard at:          10.6.105.15:33013
2025-09-03 10:52:53,137 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,137 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,137 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,137 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,137 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-vxwpo4vk
2025-09-03 10:52:53,137 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,138 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:38787
2025-09-03 10:52:53,138 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:38787
2025-09-03 10:52:53,138 - distributed.worker - INFO -          dashboard at:          10.6.105.15:44321
2025-09-03 10:52:53,138 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,138 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,138 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,138 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,138 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-p4tbk02u
2025-09-03 10:52:53,138 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,142 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:37035
2025-09-03 10:52:53,142 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:37035
2025-09-03 10:52:53,142 - distributed.worker - INFO -          dashboard at:          10.6.105.15:44421
2025-09-03 10:52:53,142 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,142 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,142 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,142 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,142 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-1qq_7dfl
2025-09-03 10:52:53,142 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,146 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:44759
2025-09-03 10:52:53,147 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:44759
2025-09-03 10:52:53,147 - distributed.worker - INFO -          dashboard at:          10.6.105.15:44673
2025-09-03 10:52:53,147 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,147 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,147 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,147 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,147 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-g08dp9bh
2025-09-03 10:52:53,147 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,152 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.15:45077
2025-09-03 10:52:53,152 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.15:45077
2025-09-03 10:52:53,152 - distributed.worker - INFO -          dashboard at:          10.6.105.15:40421
2025-09-03 10:52:53,152 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,152 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,152 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,152 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,152 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-xg4tlpom
2025-09-03 10:52:53,152 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,580 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,581 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,582 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,583 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,598 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,599 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,599 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,600 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,614 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,615 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,616 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,617 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,631 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,632 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,632 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,633 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,647 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,648 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,648 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,649 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,663 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,664 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,664 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,666 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,678 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,679 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,679 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,681 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,695 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,696 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,696 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,697 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,711 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,712 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,712 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,713 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,727 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,728 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,728 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,730 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,743 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,744 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,744 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,746 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,759 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,760 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,760 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,762 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,778 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,782 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,782 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,787 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,791 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,792 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,793 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,794 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,807 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,808 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,809 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,810 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,824 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,825 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,825 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,826 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,840 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,841 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,841 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,843 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,856 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,857 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,857 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,859 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,872 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,873 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,874 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,875 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,888 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,889 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,890 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,891 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,904 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,905 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,905 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,907 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,920 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,921 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,923 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,936 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,938 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,938 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,940 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,952 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,953 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,954 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,955 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,081 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,082 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,082 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,084 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,820 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,821 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,821 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,823 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,836 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,837 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,837 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,839 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,851 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,853 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,853 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,855 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,867 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,869 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,870 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,047 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,049 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,049 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,051 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,064 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,065 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,065 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,067 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,080 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,081 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,081 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,083 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,096 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,097 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,097 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,099 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,112 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,114 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,114 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,116 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,129 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,130 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,130 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,132 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,145 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,146 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,146 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,148 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,161 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,162 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,162 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,164 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,177 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,179 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,179 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,181 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,194 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,195 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,197 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,520 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,521 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,521 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,523 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,540 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,544 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,544 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,550 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,602 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,604 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,604 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,605 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,356 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,357 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,357 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,359 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:00,203 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:00,204 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:00,204 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:00,206 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:00,222 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:00,223 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:00,223 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:00,225 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,696 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,697 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,698 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,699 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:21,466 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,493 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,514 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,611 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,638 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,643 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,647 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,650 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,657 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,664 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,671 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,672 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,677 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,682 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,690 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,698 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,704 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,706 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,711 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,718 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,729 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,729 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,730 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,733 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,735 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,735 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:21,735 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,816 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,817 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,851 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,953 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,960 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,966 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,968 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,968 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,982 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,983 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,986 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,991 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,001 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,002 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,003 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,004 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,008 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,011 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,022 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,026 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,113 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,116 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,124 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,126 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,127 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,138 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,139 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,144 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,354 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,356 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,356 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,358 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,372 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,374 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,374 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,376 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,390 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,392 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,392 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,394 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,409 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,410 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,410 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,412 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,445 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,446 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,446 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,448 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,462 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,464 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,466 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,481 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,482 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,482 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,484 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,516 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,518 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,518 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,520 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,539 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,544 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,544 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,549 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,571 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,573 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,573 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,575 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,589 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,590 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,590 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,592 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,607 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,609 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,609 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,611 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,624 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,626 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,626 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,628 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,642 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,644 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,644 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,646 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,660 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,662 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,662 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,664 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,697 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,699 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,699 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,701 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,786 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,788 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,788 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,790 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,804 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,806 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,806 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,808 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,822 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,823 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,823 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,825 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,817 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:23,840 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,842 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,842 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,843 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,858 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,859 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,860 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,861 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,875 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,876 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,878 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,893 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,895 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,895 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,897 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,929 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,931 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,931 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,932 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,946 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,948 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,948 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,950 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,964 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,966 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,966 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,968 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,985 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,988 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,988 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,994 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:24,668 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:24,684 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:24,686 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:24,686 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:24,688 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:24,766 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:24,787 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:24,797 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:24,813 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:24,830 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:24,846 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:24,861 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:24,878 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:24,893 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:24,909 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:24,926 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:24,942 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:24,958 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:25,392 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:25,394 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,394 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:25,395 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:25,410 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:25,412 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,412 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:25,414 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:25,427 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:25,429 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,429 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:25,431 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:25,445 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:25,447 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,447 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:25,448 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:25,464 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:25,465 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:25,467 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:25,482 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:25,484 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,484 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:25,486 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:25,500 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:25,502 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,502 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:25,504 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:25,518 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:25,520 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,520 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:25,522 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:25,554 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:25,556 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,556 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:25,557 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:25,873 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:27,538 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:27,539 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:27,539 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:27,541 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:27,556 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:27,558 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:27,558 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:27,560 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:27,574 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:27,576 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:27,576 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:27,578 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:27,613 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:27,614 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:27,614 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:27,616 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:27,630 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:27,632 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:27,632 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:27,634 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:27,648 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:27,650 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:27,650 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:27,652 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:27,666 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:27,668 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:27,668 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:27,670 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:28,482 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:28,484 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:28,484 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:28,485 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:33,479 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:33,480 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:33,480 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:33,482 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:33,496 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:33,498 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:33,498 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:33,500 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:33,514 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:33,515 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:33,515 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:33,517 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:33,531 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:33,532 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:33,533 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:33,534 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:33,619 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:33,620 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:33,620 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:33,622 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:33,635 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:33,637 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:33,637 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:33,638 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:33,653 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:33,654 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:33,654 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:33,656 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:33,670 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:33,671 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:33,671 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:33,673 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:46,345 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:46,346 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:46,346 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:46,348 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:46,362 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:46,364 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:46,364 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:46,365 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:56:38,400 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,406 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,684 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,686 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,704 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,709 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,312 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,318 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,616 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,617 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,619 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,621 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,681 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,686 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,877 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,879 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,953 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,956 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,986 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,988 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,202 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,205 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,359 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,365 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,667 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,669 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,762 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,767 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,767 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,768 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,356 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,358 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,623 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,625 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,940 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,941 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,982 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,986 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,002 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,003 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,549 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,557 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,620 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,622 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,932 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,934 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,147 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,152 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,242 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,244 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,270 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,272 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,450 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,457 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,752 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,758 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,938 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,939 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,206 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,211 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,438 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,439 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,486 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,490 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,505 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,505 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,507 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,511 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,555 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,557 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,855 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,856 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,779 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,785 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,787 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,788 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,860 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,861 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,267 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,268 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,348 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,350 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,453 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,455 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,543 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,548 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,555 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,560 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,591 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,593 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,617 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,623 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,774 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,775 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,950 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,955 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,346 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,351 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,442 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,448 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,561 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,561 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,567 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,570 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,611 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,614 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,622 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,623 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,758 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,764 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,776 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,778 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,069 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,074 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,191 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,193 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,764 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,769 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,936 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,941 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,125 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,126 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,270 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,275 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,861 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,864 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,138 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,140 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,247 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,248 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,657 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,663 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,748 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,755 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,874 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,875 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,884 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,886 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,000 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,009 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,318 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,322 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,682 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,684 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,917 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,923 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,989 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,989 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,995 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,996 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,021 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,026 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,358 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,359 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,763 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,768 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,581 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,586 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,622 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,625 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,626 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,628 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,972 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,974 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,149 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,154 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,177 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,182 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,347 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,349 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,394 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,395 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,763 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,769 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,867 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,874 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,407 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,415 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,575 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,576 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:56,063 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:56,068 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:56,075 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:56,076 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:56,181 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:56,184 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:56,539 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:56,545 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:58,207 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:58,213 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:58,324 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:58,330 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:58,780 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:58,786 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:59,123 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:59,128 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:59,574 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:59,579 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:59,687 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:59,689 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:00,087 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:00,096 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:00,828 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:00,830 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:03,399 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:03,402 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:03,430 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:03,431 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:09,992 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:09,995 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,000 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,002 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,002 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,004 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,004 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,007 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,012 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,013 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,014 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,015 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,015 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,020 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,028 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,031 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,336 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,336 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,336 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,338 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,338 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,339 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,361 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,363 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,368 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,370 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,386 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,388 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,493 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,496 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,497 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,498 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,499 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,500 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,500 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,502 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,505 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,507 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,507 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,509 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,518 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,518 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,519 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,520 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,520 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,521 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,525 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,528 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,649 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,651 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,736 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,738 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,749 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,751 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,752 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,754 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,760 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,767 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,881 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,881 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,883 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,883 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,883 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,885 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,896 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,898 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,929 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,931 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,101 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,102 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,104 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,105 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,129 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,131 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,257 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,260 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,261 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,263 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,284 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,286 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,377 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,377 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,379 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,379 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,392 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,392 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,394 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,394 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,405 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,406 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,407 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,408 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,409 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,411 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,417 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,422 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,463 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,465 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,466 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,469 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,486 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,488 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,492 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,497 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,519 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,521 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,539 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,546 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,567 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,569 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,573 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,575 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,581 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,583 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,586 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,588 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,596 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,598 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,601 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,604 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,651 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,653 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,745 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,746 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,747 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,748 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,751 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,753 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,856 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,859 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,908 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,910 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,923 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,925 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,941 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,943 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,972 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,974 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,006 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,011 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,110 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,113 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,169 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,171 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,209 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,211 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,213 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,215 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,385 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,387 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,406 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,408 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,427 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,429 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,470 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,472 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,527 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,532 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,579 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,581 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,584 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,586 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,604 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,607 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,637 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,639 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,670 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,678 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,708 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,710 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,747 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,747 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,749 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,749 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,752 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,754 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,753 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,758 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,799 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,801 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,905 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,907 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,939 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,941 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,130 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,132 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,135 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,140 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,171 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,173 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,198 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,200 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,237 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,239 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,448 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,450 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,463 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,465 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,697 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,699 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:14,158 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:14,160 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:14,581 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:14,584 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:14,688 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:14,691 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:15,058 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:15,061 - distributed.utils - INFO - Reload module qme_vars from .py file
