Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 10:31:58,665 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:44409'
2025-09-03 10:31:58,677 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:37841'
2025-09-03 10:31:58,683 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:38817'
2025-09-03 10:31:58,687 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:37505'
2025-09-03 10:31:58,694 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:34181'
2025-09-03 10:31:58,698 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:43593'
2025-09-03 10:31:58,704 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:33521'
2025-09-03 10:31:58,709 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:35237'
2025-09-03 10:31:58,713 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:44179'
2025-09-03 10:31:58,767 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:34321'
2025-09-03 10:31:58,770 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:44229'
2025-09-03 10:31:58,774 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:42169'
2025-09-03 10:31:58,779 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:41819'
2025-09-03 10:31:58,784 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:35035'
2025-09-03 10:31:58,791 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:35211'
2025-09-03 10:31:58,796 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:43125'
2025-09-03 10:31:58,800 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:35957'
2025-09-03 10:31:58,804 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:36055'
2025-09-03 10:31:58,809 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:34683'
2025-09-03 10:31:58,813 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:42363'
2025-09-03 10:31:58,817 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:44481'
2025-09-03 10:31:58,821 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:43907'
2025-09-03 10:31:58,826 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:40953'
2025-09-03 10:31:58,830 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:32805'
2025-09-03 10:31:58,834 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:37933'
2025-09-03 10:31:58,838 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:36733'
2025-09-03 10:31:58,948 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:44635'
2025-09-03 10:31:58,953 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:36611'
2025-09-03 10:31:58,957 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:38381'
2025-09-03 10:31:58,961 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:45841'
2025-09-03 10:31:58,965 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:37669'
2025-09-03 10:31:58,969 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:43431'
2025-09-03 10:31:58,974 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:33129'
2025-09-03 10:31:58,978 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:38057'
2025-09-03 10:31:58,984 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:38821'
2025-09-03 10:31:58,989 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:37479'
2025-09-03 10:31:58,994 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:34719'
2025-09-03 10:31:58,998 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:36853'
2025-09-03 10:31:59,001 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:39671'
2025-09-03 10:31:59,005 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:45195'
2025-09-03 10:31:59,010 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:41351'
2025-09-03 10:31:59,014 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:37459'
2025-09-03 10:31:59,018 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:44621'
2025-09-03 10:31:59,023 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:43009'
2025-09-03 10:31:59,028 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:43161'
2025-09-03 10:31:59,034 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:32887'
2025-09-03 10:31:59,038 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:41281'
2025-09-03 10:31:59,042 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:33809'
2025-09-03 10:31:59,046 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:34507'
2025-09-03 10:31:59,051 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:35987'
2025-09-03 10:31:59,056 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:41789'
2025-09-03 10:31:59,061 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:39721'
2025-09-03 10:31:59,065 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:34861'
2025-09-03 10:31:59,070 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:46087'
2025-09-03 10:31:59,075 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:43851'
2025-09-03 10:31:59,079 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:41475'
2025-09-03 10:31:59,083 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:39497'
2025-09-03 10:31:59,088 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:46817'
2025-09-03 10:31:59,092 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:38473'
2025-09-03 10:31:59,112 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:40959'
2025-09-03 10:31:59,116 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:43983'
2025-09-03 10:31:59,121 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:42853'
2025-09-03 10:31:59,123 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:35789'
2025-09-03 10:31:59,128 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:40899'
2025-09-03 10:31:59,131 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:40119'
2025-09-03 10:31:59,136 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:43999'
2025-09-03 10:31:59,140 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:36949'
2025-09-03 10:31:59,143 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:46815'
2025-09-03 10:31:59,151 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:43667'
2025-09-03 10:31:59,160 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:35981'
2025-09-03 10:31:59,165 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:43779'
2025-09-03 10:31:59,169 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:43965'
2025-09-03 10:31:59,174 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:33019'
2025-09-03 10:31:59,179 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:43427'
2025-09-03 10:31:59,183 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:43413'
2025-09-03 10:31:59,188 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:37617'
2025-09-03 10:31:59,192 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:37169'
2025-09-03 10:31:59,197 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:37875'
2025-09-03 10:31:59,201 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:41555'
2025-09-03 10:31:59,205 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:45119'
2025-09-03 10:31:59,209 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:44665'
2025-09-03 10:31:59,214 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:37075'
2025-09-03 10:31:59,218 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:36493'
2025-09-03 10:31:59,223 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:35329'
2025-09-03 10:31:59,226 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:34563'
2025-09-03 10:31:59,232 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:42587'
2025-09-03 10:31:59,236 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:42353'
2025-09-03 10:31:59,240 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:37999'
2025-09-03 10:31:59,243 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:44425'
2025-09-03 10:31:59,246 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:33543'
2025-09-03 10:31:59,252 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:44135'
2025-09-03 10:31:59,259 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:42449'
2025-09-03 10:31:59,263 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:41455'
2025-09-03 10:31:59,267 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:38819'
2025-09-03 10:31:59,272 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:41007'
2025-09-03 10:31:59,277 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:42429'
2025-09-03 10:31:59,281 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:33807'
2025-09-03 10:31:59,288 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:43429'
2025-09-03 10:31:59,292 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:37299'
2025-09-03 10:31:59,297 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:37069'
2025-09-03 10:31:59,466 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:33755'
2025-09-03 10:31:59,470 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:39295'
2025-09-03 10:31:59,478 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:34141'
2025-09-03 10:31:59,482 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.32:33519'
2025-09-03 10:32:00,661 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:38969
2025-09-03 10:32:00,661 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:45681
2025-09-03 10:32:00,661 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:43967
2025-09-03 10:32:00,661 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:37957
2025-09-03 10:32:00,661 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:36007
2025-09-03 10:32:00,661 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:38083
2025-09-03 10:32:00,661 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:41157
2025-09-03 10:32:00,661 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:41937
2025-09-03 10:32:00,661 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:38969
2025-09-03 10:32:00,661 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:33377
2025-09-03 10:32:00,661 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:39843
2025-09-03 10:32:00,661 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:42025
2025-09-03 10:32:00,661 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:45681
2025-09-03 10:32:00,661 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:43967
2025-09-03 10:32:00,661 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:37957
2025-09-03 10:32:00,661 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:36007
2025-09-03 10:32:00,661 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:36223
2025-09-03 10:32:00,661 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:38083
2025-09-03 10:32:00,661 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:34997
2025-09-03 10:32:00,661 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:41157
2025-09-03 10:32:00,662 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:39351
2025-09-03 10:32:00,661 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:41937
2025-09-03 10:32:00,661 - distributed.worker - INFO -          dashboard at:          10.6.102.32:41099
2025-09-03 10:32:00,661 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:33377
2025-09-03 10:32:00,661 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:33985
2025-09-03 10:32:00,661 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:39843
2025-09-03 10:32:00,661 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:42025
2025-09-03 10:32:00,661 - distributed.worker - INFO -          dashboard at:          10.6.102.32:33055
2025-09-03 10:32:00,661 - distributed.worker - INFO -          dashboard at:          10.6.102.32:37249
2025-09-03 10:32:00,661 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:39105
2025-09-03 10:32:00,661 - distributed.worker - INFO -          dashboard at:          10.6.102.32:42205
2025-09-03 10:32:00,662 - distributed.worker - INFO -          dashboard at:          10.6.102.32:46027
2025-09-03 10:32:00,661 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:33159
2025-09-03 10:32:00,662 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:36223
2025-09-03 10:32:00,662 - distributed.worker - INFO -          dashboard at:          10.6.102.32:35001
2025-09-03 10:32:00,662 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:34997
2025-09-03 10:32:00,662 - distributed.worker - INFO -          dashboard at:          10.6.102.32:40657
2025-09-03 10:32:00,662 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:39351
2025-09-03 10:32:00,662 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:41567
2025-09-03 10:32:00,662 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:46267
2025-09-03 10:32:00,662 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:38061
2025-09-03 10:32:00,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,662 - distributed.worker - INFO -          dashboard at:          10.6.102.32:43273
2025-09-03 10:32:00,662 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:40955
2025-09-03 10:32:00,662 - distributed.worker - INFO -          dashboard at:          10.6.102.32:38423
2025-09-03 10:32:00,662 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:35347
2025-09-03 10:32:00,662 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:33985
2025-09-03 10:32:00,662 - distributed.worker - INFO -          dashboard at:          10.6.102.32:33645
2025-09-03 10:32:00,662 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:36887
2025-09-03 10:32:00,662 - distributed.worker - INFO -          dashboard at:          10.6.102.32:36265
2025-09-03 10:32:00,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,662 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:40987
2025-09-03 10:32:00,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,662 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:39105
2025-09-03 10:32:00,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,662 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:34247
2025-09-03 10:32:00,662 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:33159
2025-09-03 10:32:00,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,662 - distributed.worker - INFO -          dashboard at:          10.6.102.32:37071
2025-09-03 10:32:00,662 - distributed.worker - INFO -          dashboard at:          10.6.102.32:46097
2025-09-03 10:32:00,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,662 - distributed.worker - INFO -          dashboard at:          10.6.102.32:37551
2025-09-03 10:32:00,662 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:41567
2025-09-03 10:32:00,662 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:46267
2025-09-03 10:32:00,662 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,662 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:38061
2025-09-03 10:32:00,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,662 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:40955
2025-09-03 10:32:00,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,662 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:35347
2025-09-03 10:32:00,662 - distributed.worker - INFO -          dashboard at:          10.6.102.32:33067
2025-09-03 10:32:00,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,662 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,662 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:36887
2025-09-03 10:32:00,662 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,662 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:34959
2025-09-03 10:32:00,662 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:40987
2025-09-03 10:32:00,662 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,662 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:38693
2025-09-03 10:32:00,662 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,662 - distributed.worker - INFO -          dashboard at:          10.6.102.32:46255
2025-09-03 10:32:00,662 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,662 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:34247
2025-09-03 10:32:00,662 - distributed.worker - INFO -          dashboard at:          10.6.102.32:39319
2025-09-03 10:32:00,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,662 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,662 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,662 - distributed.worker - INFO -          dashboard at:          10.6.102.32:38701
2025-09-03 10:32:00,662 - distributed.worker - INFO -          dashboard at:          10.6.102.32:36309
2025-09-03 10:32:00,662 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,662 - distributed.worker - INFO -          dashboard at:          10.6.102.32:34953
2025-09-03 10:32:00,662 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,662 - distributed.worker - INFO -          dashboard at:          10.6.102.32:39581
2025-09-03 10:32:00,662 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,662 - distributed.worker - INFO -          dashboard at:          10.6.102.32:33087
2025-09-03 10:32:00,662 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,662 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,662 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,662 - distributed.worker - INFO -          dashboard at:          10.6.102.32:35071
2025-09-03 10:32:00,662 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:34959
2025-09-03 10:32:00,662 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,662 - distributed.worker - INFO -          dashboard at:          10.6.102.32:39071
2025-09-03 10:32:00,662 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,662 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:38693
2025-09-03 10:32:00,662 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,662 - distributed.worker - INFO -          dashboard at:          10.6.102.32:44293
2025-09-03 10:32:00,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,662 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,662 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,662 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,662 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,662 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,662 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,662 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,663 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO -          dashboard at:          10.6.102.32:45309
2025-09-03 10:32:00,663 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO -          dashboard at:          10.6.102.32:45183
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,663 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,663 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,663 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-o_66q2zk
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-edz3apbk
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2_4jz0j1
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-zzb_n_9a
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3wp7zs01
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-fh4ryfho
2025-09-03 10:32:00,663 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,663 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ie1k6wz0
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,663 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-y_gsd0oz
2025-09-03 10:32:00,663 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-i0ncrfxj
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4cb095bg
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,663 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-bulk9ub5
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ayob5s9h
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-vgmj8rfj
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-d7k95sb9
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-wl7x8yjm
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-whznyu49
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-i92gvkav
2025-09-03 10:32:00,663 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-n13zrrbb
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-jdthn3fr
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:36241
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-gmea0r5_
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-tnzhfph7
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-d_gn_zcd
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_0g7hb23
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-y9tivllp
2025-09-03 10:32:00,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,664 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,664 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,664 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:36241
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-m0sqap4_
2025-09-03 10:32:00,664 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,664 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,664 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,664 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-drss4ldz
2025-09-03 10:32:00,664 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,664 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-omsk2mzq
2025-09-03 10:32:00,664 - distributed.worker - INFO -          dashboard at:          10.6.102.32:46355
2025-09-03 10:32:00,664 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,663 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:36577
2025-09-03 10:32:00,664 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,664 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,664 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,664 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:42029
2025-09-03 10:32:00,664 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:36577
2025-09-03 10:32:00,664 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,664 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:42029
2025-09-03 10:32:00,664 - distributed.worker - INFO -          dashboard at:          10.6.102.32:36107
2025-09-03 10:32:00,664 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,664 - distributed.worker - INFO -          dashboard at:          10.6.102.32:38901
2025-09-03 10:32:00,664 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,664 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,664 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,664 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,664 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,664 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,664 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-5nn3m87x
2025-09-03 10:32:00,664 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,664 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,664 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6pw10j3m
2025-09-03 10:32:00,664 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,664 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,664 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,664 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-j_imqr1l
2025-09-03 10:32:00,664 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,665 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:35121
2025-09-03 10:32:00,666 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:35121
2025-09-03 10:32:00,666 - distributed.worker - INFO -          dashboard at:          10.6.102.32:39609
2025-09-03 10:32:00,666 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,666 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,666 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,666 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,666 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_tje0q2y
2025-09-03 10:32:00,666 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,671 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:39741
2025-09-03 10:32:00,671 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:39741
2025-09-03 10:32:00,671 - distributed.worker - INFO -          dashboard at:          10.6.102.32:39611
2025-09-03 10:32:00,672 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,672 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,672 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,672 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,672 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4g636n3e
2025-09-03 10:32:00,672 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,673 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:45595
2025-09-03 10:32:00,673 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:45595
2025-09-03 10:32:00,673 - distributed.worker - INFO -          dashboard at:          10.6.102.32:35171
2025-09-03 10:32:00,673 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,673 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,673 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,673 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-cno_mkj_
2025-09-03 10:32:00,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,675 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:45135
2025-09-03 10:32:00,675 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:45135
2025-09-03 10:32:00,675 - distributed.worker - INFO -          dashboard at:          10.6.102.32:36947
2025-09-03 10:32:00,675 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,675 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,675 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,675 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,675 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-azt3ddgj
2025-09-03 10:32:00,675 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,680 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:45167
2025-09-03 10:32:00,680 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:45167
2025-09-03 10:32:00,680 - distributed.worker - INFO -          dashboard at:          10.6.102.32:36409
2025-09-03 10:32:00,680 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,680 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,680 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,680 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,681 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-nerwz1gq
2025-09-03 10:32:00,681 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,260 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:42733
2025-09-03 10:32:01,261 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:42733
2025-09-03 10:32:01,261 - distributed.worker - INFO -          dashboard at:          10.6.102.32:42079
2025-09-03 10:32:01,261 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,261 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,261 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,261 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,261 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-02lz5otp
2025-09-03 10:32:01,261 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,322 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:33863
2025-09-03 10:32:01,322 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:33863
2025-09-03 10:32:01,322 - distributed.worker - INFO -          dashboard at:          10.6.102.32:40407
2025-09-03 10:32:01,322 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,322 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,322 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,323 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,323 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-qg3ogss9
2025-09-03 10:32:01,323 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,362 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:38253
2025-09-03 10:32:01,363 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:38253
2025-09-03 10:32:01,363 - distributed.worker - INFO -          dashboard at:          10.6.102.32:38679
2025-09-03 10:32:01,363 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,363 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,363 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,363 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,363 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-a2h7ikf4
2025-09-03 10:32:01,363 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,369 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:46439
2025-09-03 10:32:01,369 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:46439
2025-09-03 10:32:01,369 - distributed.worker - INFO -          dashboard at:          10.6.102.32:35229
2025-09-03 10:32:01,370 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,370 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,370 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,370 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,370 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-l42p9anz
2025-09-03 10:32:01,370 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,451 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:35417
2025-09-03 10:32:01,451 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:35417
2025-09-03 10:32:01,451 - distributed.worker - INFO -          dashboard at:          10.6.102.32:38475
2025-09-03 10:32:01,451 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,451 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,451 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,451 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,451 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-0ihhyucy
2025-09-03 10:32:01,451 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,459 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:38557
2025-09-03 10:32:01,459 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:38557
2025-09-03 10:32:01,459 - distributed.worker - INFO -          dashboard at:          10.6.102.32:34245
2025-09-03 10:32:01,459 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,459 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,459 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,459 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,459 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-o87uhpvo
2025-09-03 10:32:01,459 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,480 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:42389
2025-09-03 10:32:01,480 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:42389
2025-09-03 10:32:01,480 - distributed.worker - INFO -          dashboard at:          10.6.102.32:36519
2025-09-03 10:32:01,480 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,480 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,480 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,480 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,480 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hvrqfvsb
2025-09-03 10:32:01,480 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,522 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:46779
2025-09-03 10:32:01,522 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:46779
2025-09-03 10:32:01,522 - distributed.worker - INFO -          dashboard at:          10.6.102.32:39363
2025-09-03 10:32:01,522 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,522 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,522 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,522 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,522 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-jq0avb2i
2025-09-03 10:32:01,522 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,525 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:44725
2025-09-03 10:32:01,525 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:44725
2025-09-03 10:32:01,525 - distributed.worker - INFO -          dashboard at:          10.6.102.32:38549
2025-09-03 10:32:01,525 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,525 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,526 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,526 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,526 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-95cnfglt
2025-09-03 10:32:01,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,541 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:45307
2025-09-03 10:32:01,541 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:45307
2025-09-03 10:32:01,541 - distributed.worker - INFO -          dashboard at:          10.6.102.32:45529
2025-09-03 10:32:01,541 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,541 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,541 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,541 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,541 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hi0s3620
2025-09-03 10:32:01,541 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,560 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:40459
2025-09-03 10:32:01,560 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:40459
2025-09-03 10:32:01,560 - distributed.worker - INFO -          dashboard at:          10.6.102.32:33343
2025-09-03 10:32:01,560 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,560 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,560 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,560 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,560 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-zjjocu03
2025-09-03 10:32:01,560 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,565 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:33905
2025-09-03 10:32:01,565 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:33905
2025-09-03 10:32:01,565 - distributed.worker - INFO -          dashboard at:          10.6.102.32:36387
2025-09-03 10:32:01,566 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,566 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,566 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,566 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,566 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-gbx6t54i
2025-09-03 10:32:01,566 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,591 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:42977
2025-09-03 10:32:01,591 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:42977
2025-09-03 10:32:01,591 - distributed.worker - INFO -          dashboard at:          10.6.102.32:43985
2025-09-03 10:32:01,591 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,591 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,591 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,591 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,591 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-g76kybmv
2025-09-03 10:32:01,591 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,594 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:38169
2025-09-03 10:32:01,594 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:38169
2025-09-03 10:32:01,594 - distributed.worker - INFO -          dashboard at:          10.6.102.32:39397
2025-09-03 10:32:01,594 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,594 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,594 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,594 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,594 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-v24ovwky
2025-09-03 10:32:01,594 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,601 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:42687
2025-09-03 10:32:01,601 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:42687
2025-09-03 10:32:01,601 - distributed.worker - INFO -          dashboard at:          10.6.102.32:38735
2025-09-03 10:32:01,601 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,601 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,601 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,601 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,601 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3sbmig8v
2025-09-03 10:32:01,602 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,613 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:34493
2025-09-03 10:32:01,613 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:34493
2025-09-03 10:32:01,613 - distributed.worker - INFO -          dashboard at:          10.6.102.32:35459
2025-09-03 10:32:01,613 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,613 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,613 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,613 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,613 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hfu5l17a
2025-09-03 10:32:01,613 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,614 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:46881
2025-09-03 10:32:01,614 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:46881
2025-09-03 10:32:01,614 - distributed.worker - INFO -          dashboard at:          10.6.102.32:35603
2025-09-03 10:32:01,614 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,614 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,614 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,614 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,614 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-63rf5de3
2025-09-03 10:32:01,614 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,618 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:34879
2025-09-03 10:32:01,618 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:34879
2025-09-03 10:32:01,618 - distributed.worker - INFO -          dashboard at:          10.6.102.32:41605
2025-09-03 10:32:01,618 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,618 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,618 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,618 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,618 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-cd2bm4t5
2025-09-03 10:32:01,618 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,620 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:34349
2025-09-03 10:32:01,620 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:34349
2025-09-03 10:32:01,620 - distributed.worker - INFO -          dashboard at:          10.6.102.32:36023
2025-09-03 10:32:01,620 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,620 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,621 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,621 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,621 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-i7ym2fzy
2025-09-03 10:32:01,621 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,647 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:42883
2025-09-03 10:32:01,647 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:42883
2025-09-03 10:32:01,647 - distributed.worker - INFO -          dashboard at:          10.6.102.32:40301
2025-09-03 10:32:01,647 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,647 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,647 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,647 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,647 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-sdqceslw
2025-09-03 10:32:01,647 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,651 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:39183
2025-09-03 10:32:01,651 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:39183
2025-09-03 10:32:01,651 - distributed.worker - INFO -          dashboard at:          10.6.102.32:42039
2025-09-03 10:32:01,651 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,651 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,651 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,651 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,651 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-85rmkdh1
2025-09-03 10:32:01,651 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,652 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:40017
2025-09-03 10:32:01,652 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:40017
2025-09-03 10:32:01,652 - distributed.worker - INFO -          dashboard at:          10.6.102.32:34375
2025-09-03 10:32:01,652 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,652 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,652 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,652 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,652 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-0kz4s67q
2025-09-03 10:32:01,652 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,655 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:39887
2025-09-03 10:32:01,655 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:39887
2025-09-03 10:32:01,655 - distributed.worker - INFO -          dashboard at:          10.6.102.32:34503
2025-09-03 10:32:01,655 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,655 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,655 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,655 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,655 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-izdh4brk
2025-09-03 10:32:01,656 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,661 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:42089
2025-09-03 10:32:01,661 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:42089
2025-09-03 10:32:01,661 - distributed.worker - INFO -          dashboard at:          10.6.102.32:35715
2025-09-03 10:32:01,661 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,661 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,661 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,661 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,661 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-jaobn3j6
2025-09-03 10:32:01,661 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,689 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:39091
2025-09-03 10:32:01,689 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:39091
2025-09-03 10:32:01,689 - distributed.worker - INFO -          dashboard at:          10.6.102.32:34635
2025-09-03 10:32:01,689 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,689 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,689 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,689 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,689 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-b0w52may
2025-09-03 10:32:01,689 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,702 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:37721
2025-09-03 10:32:01,702 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:37721
2025-09-03 10:32:01,702 - distributed.worker - INFO -          dashboard at:          10.6.102.32:46463
2025-09-03 10:32:01,702 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,702 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,702 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,702 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,702 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4x9xxfo4
2025-09-03 10:32:01,702 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,774 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:46179
2025-09-03 10:32:01,774 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:46179
2025-09-03 10:32:01,774 - distributed.worker - INFO -          dashboard at:          10.6.102.32:42095
2025-09-03 10:32:01,774 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,774 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,774 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,774 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,774 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-njcqy0s7
2025-09-03 10:32:01,774 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,811 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:37057
2025-09-03 10:32:01,812 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:37057
2025-09-03 10:32:01,812 - distributed.worker - INFO -          dashboard at:          10.6.102.32:38311
2025-09-03 10:32:01,812 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,812 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,812 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,812 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,812 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-9e_1hmr1
2025-09-03 10:32:01,812 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,886 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:33381
2025-09-03 10:32:01,886 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:33381
2025-09-03 10:32:01,886 - distributed.worker - INFO -          dashboard at:          10.6.102.32:39335
2025-09-03 10:32:01,886 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,886 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,886 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,886 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,886 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7cojp76q
2025-09-03 10:32:01,886 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,902 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:38605
2025-09-03 10:32:01,902 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:38605
2025-09-03 10:32:01,902 - distributed.worker - INFO -          dashboard at:          10.6.102.32:45739
2025-09-03 10:32:01,902 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,902 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,902 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,902 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,902 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-z557jsws
2025-09-03 10:32:01,902 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,912 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:37699
2025-09-03 10:32:01,912 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:37699
2025-09-03 10:32:01,912 - distributed.worker - INFO -          dashboard at:          10.6.102.32:37567
2025-09-03 10:32:01,912 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,912 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,912 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,912 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,912 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ferc48gc
2025-09-03 10:32:01,913 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,943 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:34279
2025-09-03 10:32:01,944 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:34279
2025-09-03 10:32:01,944 - distributed.worker - INFO -          dashboard at:          10.6.102.32:42485
2025-09-03 10:32:01,944 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,944 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,944 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,944 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,944 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-0spc11c9
2025-09-03 10:32:01,944 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,965 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:37711
2025-09-03 10:32:01,965 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:37711
2025-09-03 10:32:01,965 - distributed.worker - INFO -          dashboard at:          10.6.102.32:37269
2025-09-03 10:32:01,965 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,965 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,965 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,965 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,965 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3hi9r30q
2025-09-03 10:32:01,965 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,996 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:40785
2025-09-03 10:32:01,996 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:40785
2025-09-03 10:32:01,996 - distributed.worker - INFO -          dashboard at:          10.6.102.32:33909
2025-09-03 10:32:01,996 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,996 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,996 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:01,996 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:01,996 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ckg649oe
2025-09-03 10:32:01,996 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,002 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:46769
2025-09-03 10:32:02,002 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:46769
2025-09-03 10:32:02,002 - distributed.worker - INFO -          dashboard at:          10.6.102.32:41217
2025-09-03 10:32:02,002 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,002 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,002 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,002 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,002 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ek6vbixp
2025-09-03 10:32:02,003 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,020 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:42365
2025-09-03 10:32:02,020 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:42365
2025-09-03 10:32:02,020 - distributed.worker - INFO -          dashboard at:          10.6.102.32:43887
2025-09-03 10:32:02,020 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,020 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,020 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,020 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,020 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6lgk1zvg
2025-09-03 10:32:02,020 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,026 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:33815
2025-09-03 10:32:02,026 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:33815
2025-09-03 10:32:02,026 - distributed.worker - INFO -          dashboard at:          10.6.102.32:46379
2025-09-03 10:32:02,026 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,026 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,026 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,027 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,027 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-mxyi5mtd
2025-09-03 10:32:02,027 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,040 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:38601
2025-09-03 10:32:02,041 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:38601
2025-09-03 10:32:02,041 - distributed.worker - INFO -          dashboard at:          10.6.102.32:44273
2025-09-03 10:32:02,041 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,041 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,041 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,041 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,041 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-5_2e2ur5
2025-09-03 10:32:02,041 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,147 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:33823
2025-09-03 10:32:02,147 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:33823
2025-09-03 10:32:02,147 - distributed.worker - INFO -          dashboard at:          10.6.102.32:37021
2025-09-03 10:32:02,147 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,147 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,147 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,147 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,147 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-svy3n48f
2025-09-03 10:32:02,147 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,209 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:45859
2025-09-03 10:32:02,209 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:45859
2025-09-03 10:32:02,209 - distributed.worker - INFO -          dashboard at:          10.6.102.32:40345
2025-09-03 10:32:02,209 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,209 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,209 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,209 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,209 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-rtkayqkt
2025-09-03 10:32:02,209 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,222 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:39683
2025-09-03 10:32:02,222 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:39683
2025-09-03 10:32:02,222 - distributed.worker - INFO -          dashboard at:          10.6.102.32:42145
2025-09-03 10:32:02,222 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,222 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,222 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,222 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,222 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-b9f1zvut
2025-09-03 10:32:02,222 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,226 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:35169
2025-09-03 10:32:02,226 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:35169
2025-09-03 10:32:02,226 - distributed.worker - INFO -          dashboard at:          10.6.102.32:41219
2025-09-03 10:32:02,226 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,226 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,226 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,226 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,226 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7_ug81zt
2025-09-03 10:32:02,226 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,228 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:46803
2025-09-03 10:32:02,228 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:46803
2025-09-03 10:32:02,228 - distributed.worker - INFO -          dashboard at:          10.6.102.32:33263
2025-09-03 10:32:02,228 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,228 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,228 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,229 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,229 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-frzpteb7
2025-09-03 10:32:02,229 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:46269
2025-09-03 10:32:02,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:46269
2025-09-03 10:32:02,236 - distributed.worker - INFO -          dashboard at:          10.6.102.32:41747
2025-09-03 10:32:02,236 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,236 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,236 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,236 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,236 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2jchvgjx
2025-09-03 10:32:02,236 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,255 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:39089
2025-09-03 10:32:02,255 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:39089
2025-09-03 10:32:02,255 - distributed.worker - INFO -          dashboard at:          10.6.102.32:44427
2025-09-03 10:32:02,255 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,255 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,255 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,255 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,255 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-oefjlttl
2025-09-03 10:32:02,255 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,399 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:37111
2025-09-03 10:32:02,399 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:37111
2025-09-03 10:32:02,399 - distributed.worker - INFO -          dashboard at:          10.6.102.32:45427
2025-09-03 10:32:02,399 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,400 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,400 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,400 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,400 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-y6djqbw1
2025-09-03 10:32:02,400 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,435 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:40985
2025-09-03 10:32:02,435 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:40985
2025-09-03 10:32:02,435 - distributed.worker - INFO -          dashboard at:          10.6.102.32:34667
2025-09-03 10:32:02,435 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,435 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,435 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,435 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,435 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-9qlg801v
2025-09-03 10:32:02,436 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,436 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:44969
2025-09-03 10:32:02,436 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:44969
2025-09-03 10:32:02,436 - distributed.worker - INFO -          dashboard at:          10.6.102.32:40983
2025-09-03 10:32:02,436 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,436 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,436 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,436 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,436 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_uiz7ihp
2025-09-03 10:32:02,436 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,441 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:33323
2025-09-03 10:32:02,441 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:33323
2025-09-03 10:32:02,441 - distributed.worker - INFO -          dashboard at:          10.6.102.32:45589
2025-09-03 10:32:02,441 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,441 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,441 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,441 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,441 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-bkug62v6
2025-09-03 10:32:02,441 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,442 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:36873
2025-09-03 10:32:02,443 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:36873
2025-09-03 10:32:02,443 - distributed.worker - INFO -          dashboard at:          10.6.102.32:43423
2025-09-03 10:32:02,443 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,443 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,443 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,443 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,443 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-xwvi_7mx
2025-09-03 10:32:02,443 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,443 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:41821
2025-09-03 10:32:02,444 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:41821
2025-09-03 10:32:02,444 - distributed.worker - INFO -          dashboard at:          10.6.102.32:44883
2025-09-03 10:32:02,444 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,444 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,444 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,444 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,444 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hw4m8_nf
2025-09-03 10:32:02,444 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,445 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:38257
2025-09-03 10:32:02,445 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:38257
2025-09-03 10:32:02,445 - distributed.worker - INFO -          dashboard at:          10.6.102.32:37395
2025-09-03 10:32:02,445 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,445 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,445 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,445 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,445 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-1s7k4mah
2025-09-03 10:32:02,445 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,447 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:34231
2025-09-03 10:32:02,447 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:36315
2025-09-03 10:32:02,447 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:34231
2025-09-03 10:32:02,447 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:36315
2025-09-03 10:32:02,447 - distributed.worker - INFO -          dashboard at:          10.6.102.32:35677
2025-09-03 10:32:02,447 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,447 - distributed.worker - INFO -          dashboard at:          10.6.102.32:43325
2025-09-03 10:32:02,447 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,447 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,447 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,447 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,447 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,447 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,447 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,447 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-1s041i5r
2025-09-03 10:32:02,447 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-zz6w9bb0
2025-09-03 10:32:02,447 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,447 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,448 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:42987
2025-09-03 10:32:02,449 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:42987
2025-09-03 10:32:02,449 - distributed.worker - INFO -          dashboard at:          10.6.102.32:35107
2025-09-03 10:32:02,449 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,449 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,449 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,449 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,449 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-osartaot
2025-09-03 10:32:02,449 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,449 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:45111
2025-09-03 10:32:02,449 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:45111
2025-09-03 10:32:02,449 - distributed.worker - INFO -          dashboard at:          10.6.102.32:33033
2025-09-03 10:32:02,449 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,449 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,449 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,449 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,449 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6nqqq6q5
2025-09-03 10:32:02,449 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,450 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:41925
2025-09-03 10:32:02,450 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:41925
2025-09-03 10:32:02,450 - distributed.worker - INFO -          dashboard at:          10.6.102.32:34933
2025-09-03 10:32:02,450 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,450 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,450 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,450 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,450 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ykj2yjm2
2025-09-03 10:32:02,450 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,453 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:44667
2025-09-03 10:32:02,453 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:44667
2025-09-03 10:32:02,453 - distributed.worker - INFO -          dashboard at:          10.6.102.32:35479
2025-09-03 10:32:02,453 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,453 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,453 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,453 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,453 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3xf86orp
2025-09-03 10:32:02,453 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,457 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:42941
2025-09-03 10:32:02,457 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:42941
2025-09-03 10:32:02,457 - distributed.worker - INFO -          dashboard at:          10.6.102.32:38851
2025-09-03 10:32:02,457 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:38823
2025-09-03 10:32:02,457 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,457 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,457 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:38823
2025-09-03 10:32:02,457 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,457 - distributed.worker - INFO -          dashboard at:          10.6.102.32:45725
2025-09-03 10:32:02,457 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,457 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,457 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-buzs7d3x
2025-09-03 10:32:02,457 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,457 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,457 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,457 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,457 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ij9tsxcc
2025-09-03 10:32:02,457 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,461 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:34003
2025-09-03 10:32:02,461 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:34003
2025-09-03 10:32:02,461 - distributed.worker - INFO -          dashboard at:          10.6.102.32:36377
2025-09-03 10:32:02,461 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,461 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,461 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,461 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,461 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-gggqdkbr
2025-09-03 10:32:02,461 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,462 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:37597
2025-09-03 10:32:02,462 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:37597
2025-09-03 10:32:02,462 - distributed.worker - INFO -          dashboard at:          10.6.102.32:45485
2025-09-03 10:32:02,462 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,462 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,462 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,462 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,462 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:33285
2025-09-03 10:32:02,462 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-wypt0y92
2025-09-03 10:32:02,462 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:33285
2025-09-03 10:32:02,462 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,462 - distributed.worker - INFO -          dashboard at:          10.6.102.32:34119
2025-09-03 10:32:02,462 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,462 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,462 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,462 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,462 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-d0jxhdak
2025-09-03 10:32:02,462 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,463 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:42615
2025-09-03 10:32:02,463 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:42615
2025-09-03 10:32:02,463 - distributed.worker - INFO -          dashboard at:          10.6.102.32:43877
2025-09-03 10:32:02,463 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,463 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,463 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,463 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,463 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ucqr3j3b
2025-09-03 10:32:02,463 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,463 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:39251
2025-09-03 10:32:02,464 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:39251
2025-09-03 10:32:02,464 - distributed.worker - INFO -          dashboard at:          10.6.102.32:39171
2025-09-03 10:32:02,464 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,464 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,464 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,464 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-jv110rfh
2025-09-03 10:32:02,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,468 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:38981
2025-09-03 10:32:02,468 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:38981
2025-09-03 10:32:02,468 - distributed.worker - INFO -          dashboard at:          10.6.102.32:45159
2025-09-03 10:32:02,468 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,468 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,468 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,468 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,469 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_ep4oklc
2025-09-03 10:32:02,469 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,472 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:40989
2025-09-03 10:32:02,472 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:40989
2025-09-03 10:32:02,473 - distributed.worker - INFO -          dashboard at:          10.6.102.32:46341
2025-09-03 10:32:02,473 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,473 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,473 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,473 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,473 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-1unt2aed
2025-09-03 10:32:02,473 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,473 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:35337
2025-09-03 10:32:02,473 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:35337
2025-09-03 10:32:02,473 - distributed.worker - INFO -          dashboard at:          10.6.102.32:36489
2025-09-03 10:32:02,473 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,473 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,473 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,473 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,474 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-cv2ay03f
2025-09-03 10:32:02,474 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,482 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.32:46447
2025-09-03 10:32:02,482 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.32:46447
2025-09-03 10:32:02,482 - distributed.worker - INFO -          dashboard at:          10.6.102.32:36379
2025-09-03 10:32:02,482 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,482 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,482 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:02,482 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:02,482 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-q0jwsu60
2025-09-03 10:32:02,482 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,804 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,805 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,806 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,807 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:03,020 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:03,021 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:03,021 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:03,023 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:03,067 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:03,068 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:03,068 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:03,070 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:03,082 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:03,083 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:03,083 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:03,084 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:03,284 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:03,285 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:03,286 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:03,287 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:03,299 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:03,300 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:03,301 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:03,302 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,167 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,169 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,169 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,170 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,535 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,536 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,536 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,538 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,855 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,856 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,857 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,859 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,870 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,872 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,872 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,874 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,885 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,887 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,887 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,889 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,901 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,902 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,902 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,904 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,917 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,918 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,919 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,921 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,933 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,934 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,934 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,936 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,954 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,959 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,959 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,963 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,965 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,965 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,965 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,967 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,979 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,980 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,980 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,982 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,994 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,996 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,996 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,998 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:05,009 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:05,010 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:05,010 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:05,012 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:05,025 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:05,026 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:05,027 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:05,029 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:05,040 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:05,042 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:05,042 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:05,044 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:05,056 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:05,057 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:05,057 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:05,059 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:05,071 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:05,073 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:05,073 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:05,075 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:05,087 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:05,088 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:05,088 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:05,090 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:05,103 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:05,104 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:05,104 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:05,106 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:05,118 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:05,120 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:05,120 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:05,122 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:05,134 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:05,135 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:05,135 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:05,137 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:05,149 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:05,150 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:05,150 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:05,152 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:05,164 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:05,165 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:05,165 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:05,167 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:05,180 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:05,181 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:05,181 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:05,183 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:05,307 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:05,308 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:05,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:05,311 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:05,322 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:05,323 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:05,323 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:05,325 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:05,708 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:05,709 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:05,710 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:05,712 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:05,723 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:05,724 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:05,725 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:05,726 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:05,978 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:05,979 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:05,980 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:05,982 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:05,993 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:05,994 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:05,994 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:05,996 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:06,009 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:06,010 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:06,011 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:06,012 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:06,400 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:06,401 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:06,402 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:06,404 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:06,416 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:06,417 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:06,418 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:06,419 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:06,431 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:06,433 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:06,433 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:06,435 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:06,447 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:06,449 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:06,449 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:06,451 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:06,463 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:06,464 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:06,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:06,466 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:06,478 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:06,479 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:06,479 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:06,481 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:09,646 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:09,648 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:09,648 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:09,650 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:09,663 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:09,664 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:09,664 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:09,666 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:09,727 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:09,728 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:09,728 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:09,730 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,425 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,427 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,427 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,428 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,441 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,443 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,443 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,445 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,458 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,460 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,461 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,465 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,474 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,477 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,477 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,481 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,489 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,491 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,491 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,493 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,504 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,506 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,506 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,508 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,521 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,522 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,522 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,524 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,536 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,538 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,538 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,540 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,553 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,556 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,556 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,560 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,567 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,569 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,569 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,571 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,584 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,585 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,585 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,587 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,599 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,601 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,601 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,603 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,615 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,617 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,617 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,619 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,631 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,633 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,633 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,635 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,647 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,649 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,649 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,651 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,663 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,664 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,665 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,666 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,679 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,681 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,681 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,683 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,695 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,696 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,697 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,698 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,712 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,715 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,715 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,719 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,726 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,728 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,728 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,730 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,742 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,743 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,744 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,745 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,758 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,759 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,760 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,761 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,774 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,776 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,776 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,778 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,790 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,791 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,791 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,793 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,807 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,809 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,809 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,814 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,821 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,823 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,823 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,825 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,837 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,838 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,838 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,840 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,853 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,854 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,854 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,856 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,869 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,870 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,871 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,872 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,884 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,886 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,886 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,888 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,900 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,902 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,902 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,904 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,916 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,918 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,918 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,920 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,932 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,934 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,934 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,935 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,948 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,949 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,950 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,951 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:10,965 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:10,968 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:10,968 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:10,972 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,115 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,117 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,117 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,118 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,180 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,181 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,181 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,183 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,196 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,198 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,199 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,213 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,215 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,215 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,216 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,229 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,231 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,231 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,233 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,403 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,405 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,405 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,407 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,531 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,533 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,533 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,534 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:14,998 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:15,000 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:15,000 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:15,001 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:15,014 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:15,015 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:15,015 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:15,017 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:17,074 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:17,075 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:17,075 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:17,077 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:25,236 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:25,237 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:25,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:25,239 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:25,253 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:25,254 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:25,254 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:25,256 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:25,270 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:25,272 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:25,272 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:25,274 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:25,287 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:25,288 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:25,288 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:25,290 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:25,304 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:25,306 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:25,306 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:25,308 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:31,595 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:31,603 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:31,614 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:31,616 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:31,619 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:31,622 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:31,649 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:31,653 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:33,809 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:34,291 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:34,305 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:36,316 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:36,729 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:36,984 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:36,999 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:37,017 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:37,722 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:37,723 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:37,723 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:37,725 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:37,774 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:37,775 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:37,775 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:37,777 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:37,947 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:37,947 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:37,947 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:37,948 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:37,982 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:37,983 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:37,983 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:37,985 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:37,999 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:38,000 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:38,001 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:38,002 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:38,034 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:38,035 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:38,035 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:38,037 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:38,086 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:38,087 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:38,087 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:38,088 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:38,172 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:38,174 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:38,174 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:38,176 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:41,510 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:41,529 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:41,542 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:41,560 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:41,572 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:41,589 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:41,720 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:41,733 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:41,750 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:41,764 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:41,780 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:41,796 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:41,813 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:41,829 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:41,843 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:41,859 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:41,876 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:41,891 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:41,908 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:42,236 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:48,080 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:48,194 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TimeoutError: [Errno 110] Connection timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.32:60696 remote=tcp://10.6.82.69:8735>: TimeoutError: [Errno 110] Connection timed out
2025-09-03 10:35:51,083 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,093 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,104 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,107 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,469 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,474 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,492 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,493 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,895 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,900 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,943 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,954 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,087 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,091 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,114 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,120 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,123 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,126 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,187 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,193 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,239 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,242 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,245 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,250 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,756 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,760 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,076 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,081 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,133 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,135 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,318 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,320 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,347 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,349 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,428 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,429 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,482 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,490 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,492 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,501 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,540 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,541 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,574 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,576 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,010 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,012 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,039 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,040 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,247 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,252 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,289 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,290 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,299 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,306 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,327 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,332 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,361 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,366 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,389 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,397 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,399 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,405 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,437 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,440 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,486 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,491 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,760 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,762 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,971 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,976 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,020 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,021 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,104 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,106 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,154 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,159 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,291 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,296 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,411 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,412 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,413 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,418 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,570 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,578 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,716 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,718 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,778 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,783 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,793 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,795 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,827 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,832 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,943 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,944 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,040 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,043 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,094 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,095 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,229 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,231 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,330 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,336 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,372 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,382 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,446 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,451 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,576 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,578 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,619 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,624 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,625 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,625 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,841 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,842 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,943 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,949 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,041 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,043 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,132 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,133 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,146 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,147 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,216 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,222 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,227 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,232 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,417 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,422 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,566 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,571 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,679 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,680 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,710 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,717 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,732 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,733 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,744 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,749 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,965 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,967 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,998 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,003 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,031 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,037 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,372 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,378 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,572 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,582 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,625 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,626 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,628 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,630 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,849 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,852 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,231 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,233 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,538 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,544 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,587 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,593 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,732 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,737 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,799 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,805 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,842 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,843 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,945 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,950 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,100 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,106 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,351 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,356 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,380 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,381 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,638 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,639 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,711 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,713 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,922 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,927 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,096 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,097 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,485 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,486 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,538 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,545 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,737 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,740 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,826 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,827 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,153 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,154 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,226 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,228 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,643 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,645 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:03,556 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:03,558 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:05,867 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:05,873 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:06,221 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:06,222 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:06,457 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:06,459 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:10,658 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:10,660 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:12,426 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:12,428 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:22,882 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,884 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,886 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,895 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,896 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,897 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,898 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,899 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,901 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,903 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,904 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,906 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,923 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,926 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,935 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,937 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,937 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,940 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,942 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,943 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,945 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,950 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,950 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,952 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,390 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,392 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,397 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,400 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,402 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,403 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,409 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,408 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,413 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,507 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,512 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,524 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,526 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,541 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,543 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,544 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,546 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,788 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,790 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,792 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,793 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,795 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,798 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,831 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,832 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,833 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,834 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,837 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,839 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,839 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,840 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,841 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,842 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,850 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,852 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,861 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,863 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,863 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,865 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,865 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,867 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,872 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,874 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,878 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,882 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,890 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,891 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,899 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,901 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,911 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,913 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,018 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,024 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,030 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,032 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,067 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,068 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,069 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,070 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,078 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,077 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,080 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,082 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,082 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,084 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,091 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,093 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,097 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,099 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,099 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,101 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,121 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,128 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,337 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,338 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,339 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,340 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,388 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,390 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,392 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,394 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,400 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,437 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,438 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,439 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,440 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,441 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,442 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,455 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,457 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,464 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,466 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,503 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,506 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,506 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,508 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,508 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,510 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,581 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,583 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,595 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,596 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,626 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,628 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,636 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,639 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,664 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,665 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,666 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,667 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,675 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,676 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,680 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,682 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,688 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,689 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,740 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,742 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,747 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,749 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,750 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,752 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,758 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,760 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,792 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,795 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,854 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,856 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,857 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,859 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,895 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,897 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,898 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,898 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,899 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,900 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,902 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,904 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,920 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,922 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,929 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,931 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,939 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,940 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,965 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,966 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,027 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,029 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,030 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,032 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,051 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,053 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,081 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,082 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,120 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,122 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,129 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,131 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,252 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,263 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,283 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,285 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,296 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,298 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,374 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,380 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,481 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,486 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,648 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,650 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,137 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,139 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,226 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,228 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,379 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,386 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,640 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,642 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:27,134 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:27,135 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:28,224 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:28,226 - distributed.utils - INFO - Reload module qme_vars from .py file
