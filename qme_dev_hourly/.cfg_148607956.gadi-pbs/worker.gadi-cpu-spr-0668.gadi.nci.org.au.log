Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 10:52:54,129 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:38903'
2025-09-03 10:52:54,139 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:46527'
2025-09-03 10:52:54,145 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:36471'
2025-09-03 10:52:54,149 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:37019'
2025-09-03 10:52:54,157 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:36877'
2025-09-03 10:52:54,290 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:46455'
2025-09-03 10:52:54,294 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:38233'
2025-09-03 10:52:54,298 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:46847'
2025-09-03 10:52:54,304 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:35819'
2025-09-03 10:52:54,308 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:41241'
2025-09-03 10:52:54,313 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:39893'
2025-09-03 10:52:54,318 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:37479'
2025-09-03 10:52:54,323 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:38675'
2025-09-03 10:52:54,330 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:35391'
2025-09-03 10:52:54,335 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:39797'
2025-09-03 10:52:54,340 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:36207'
2025-09-03 10:52:54,345 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:42483'
2025-09-03 10:52:54,350 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:39085'
2025-09-03 10:52:54,354 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:39975'
2025-09-03 10:52:54,359 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:42701'
2025-09-03 10:52:54,364 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:35891'
2025-09-03 10:52:54,369 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:41263'
2025-09-03 10:52:54,373 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:39685'
2025-09-03 10:52:54,378 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:35013'
2025-09-03 10:52:54,383 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:44845'
2025-09-03 10:52:54,388 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:34253'
2025-09-03 10:52:54,393 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:33691'
2025-09-03 10:52:54,397 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:36973'
2025-09-03 10:52:54,401 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:36437'
2025-09-03 10:52:54,405 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:37335'
2025-09-03 10:52:54,409 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:41683'
2025-09-03 10:52:54,414 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:37459'
2025-09-03 10:52:54,418 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:39071'
2025-09-03 10:52:54,422 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:39549'
2025-09-03 10:52:54,427 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:37207'
2025-09-03 10:52:54,432 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:37103'
2025-09-03 10:52:54,436 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:42607'
2025-09-03 10:52:54,440 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:45261'
2025-09-03 10:52:54,445 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:33727'
2025-09-03 10:52:54,448 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:45413'
2025-09-03 10:52:54,453 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:40067'
2025-09-03 10:52:54,457 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:42935'
2025-09-03 10:52:54,531 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:36983'
2025-09-03 10:52:54,538 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:39713'
2025-09-03 10:52:54,542 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:35023'
2025-09-03 10:52:54,547 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:34063'
2025-09-03 10:52:54,552 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:44449'
2025-09-03 10:52:54,558 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:43641'
2025-09-03 10:52:54,562 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:41269'
2025-09-03 10:52:54,566 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:36341'
2025-09-03 10:52:54,571 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:32919'
2025-09-03 10:52:54,576 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:42165'
2025-09-03 10:52:54,581 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:33643'
2025-09-03 10:52:54,587 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:39545'
2025-09-03 10:52:54,593 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:40263'
2025-09-03 10:52:54,597 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:39777'
2025-09-03 10:52:55,394 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:33679
2025-09-03 10:52:55,394 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:33679
2025-09-03 10:52:55,394 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:38469
2025-09-03 10:52:55,394 - distributed.worker - INFO -          dashboard at:          10.6.105.20:35091
2025-09-03 10:52:55,394 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:38469
2025-09-03 10:52:55,394 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,394 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,394 - distributed.worker - INFO -          dashboard at:          10.6.105.20:39053
2025-09-03 10:52:55,394 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,394 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,394 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,394 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,394 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,394 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_cbvlpcb
2025-09-03 10:52:55,394 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,394 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,394 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-y0zs0aw5
2025-09-03 10:52:55,395 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,399 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:43421
2025-09-03 10:52:55,400 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:43421
2025-09-03 10:52:55,400 - distributed.worker - INFO -          dashboard at:          10.6.105.20:39695
2025-09-03 10:52:55,400 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,400 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,400 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,400 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,400 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-1kzpysbo
2025-09-03 10:52:55,400 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,403 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:36081
2025-09-03 10:52:55,403 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:36081
2025-09-03 10:52:55,403 - distributed.worker - INFO -          dashboard at:          10.6.105.20:36637
2025-09-03 10:52:55,403 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,403 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,404 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,404 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,404 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9uutac99
2025-09-03 10:52:55,404 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,657 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:36297
2025-09-03 10:52:55,657 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:36297
2025-09-03 10:52:55,657 - distributed.worker - INFO -          dashboard at:          10.6.105.20:33425
2025-09-03 10:52:55,657 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,657 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,657 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,657 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,657 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-atck4vp_
2025-09-03 10:52:55,657 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,662 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:35685
2025-09-03 10:52:55,662 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:35685
2025-09-03 10:52:55,662 - distributed.worker - INFO -          dashboard at:          10.6.105.20:34863
2025-09-03 10:52:55,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,662 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,662 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,662 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,662 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-tw_v2yrl
2025-09-03 10:52:55,662 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,744 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:35587
2025-09-03 10:52:55,745 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:35587
2025-09-03 10:52:55,745 - distributed.worker - INFO -          dashboard at:          10.6.105.20:43505
2025-09-03 10:52:55,745 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,745 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,745 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,745 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,745 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ntjt4ja5
2025-09-03 10:52:55,745 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,787 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:46781
2025-09-03 10:52:55,787 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:46781
2025-09-03 10:52:55,787 - distributed.worker - INFO -          dashboard at:          10.6.105.20:43507
2025-09-03 10:52:55,787 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,787 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,787 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,787 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,787 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-iwgc6xz0
2025-09-03 10:52:55,787 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,857 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:35705
2025-09-03 10:52:55,857 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:35705
2025-09-03 10:52:55,857 - distributed.worker - INFO -          dashboard at:          10.6.105.20:43259
2025-09-03 10:52:55,857 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,857 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,857 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,857 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,857 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-u1zi8ciq
2025-09-03 10:52:55,857 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,935 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:33035
2025-09-03 10:52:55,935 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:33035
2025-09-03 10:52:55,935 - distributed.worker - INFO -          dashboard at:          10.6.105.20:38045
2025-09-03 10:52:55,935 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,935 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,935 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,935 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,935 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-5zbyz5w2
2025-09-03 10:52:55,936 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,949 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:43617
2025-09-03 10:52:55,949 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:43617
2025-09-03 10:52:55,949 - distributed.worker - INFO -          dashboard at:          10.6.105.20:34761
2025-09-03 10:52:55,949 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,949 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,949 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,949 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,949 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-42ed_cru
2025-09-03 10:52:55,949 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,071 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:44823
2025-09-03 10:52:56,071 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:44823
2025-09-03 10:52:56,071 - distributed.worker - INFO -          dashboard at:          10.6.105.20:40187
2025-09-03 10:52:56,071 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,071 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,071 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,071 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,071 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-e864_j86
2025-09-03 10:52:56,071 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,178 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:44759
2025-09-03 10:52:56,178 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:44759
2025-09-03 10:52:56,178 - distributed.worker - INFO -          dashboard at:          10.6.105.20:46453
2025-09-03 10:52:56,178 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,178 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,178 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,178 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,178 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-k4a4k917
2025-09-03 10:52:56,178 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,188 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:41759
2025-09-03 10:52:56,188 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:41759
2025-09-03 10:52:56,188 - distributed.worker - INFO -          dashboard at:          10.6.105.20:36487
2025-09-03 10:52:56,188 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,188 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,188 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,188 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,188 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-y0eifj29
2025-09-03 10:52:56,189 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,198 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:41955
2025-09-03 10:52:56,198 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:41955
2025-09-03 10:52:56,198 - distributed.worker - INFO -          dashboard at:          10.6.105.20:43045
2025-09-03 10:52:56,198 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,198 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,198 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,198 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-liwvjkiu
2025-09-03 10:52:56,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,255 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:44371
2025-09-03 10:52:56,255 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:44371
2025-09-03 10:52:56,255 - distributed.worker - INFO -          dashboard at:          10.6.105.20:40779
2025-09-03 10:52:56,255 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,255 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,255 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,255 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,255 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-3gkxlxqp
2025-09-03 10:52:56,256 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,272 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:43015
2025-09-03 10:52:56,272 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:43015
2025-09-03 10:52:56,272 - distributed.worker - INFO -          dashboard at:          10.6.105.20:40367
2025-09-03 10:52:56,272 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,272 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,272 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,272 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,272 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_qdhidn0
2025-09-03 10:52:56,272 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,282 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:32837
2025-09-03 10:52:56,282 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:32837
2025-09-03 10:52:56,282 - distributed.worker - INFO -          dashboard at:          10.6.105.20:41509
2025-09-03 10:52:56,282 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,282 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,282 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,282 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,282 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-los5x05f
2025-09-03 10:52:56,282 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,319 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:38531
2025-09-03 10:52:56,319 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:38531
2025-09-03 10:52:56,319 - distributed.worker - INFO -          dashboard at:          10.6.105.20:44665
2025-09-03 10:52:56,319 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,319 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,319 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,319 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,319 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-uuabyidu
2025-09-03 10:52:56,319 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,323 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:42853
2025-09-03 10:52:56,324 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:42853
2025-09-03 10:52:56,324 - distributed.worker - INFO -          dashboard at:          10.6.105.20:40201
2025-09-03 10:52:56,324 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:42699
2025-09-03 10:52:56,324 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,324 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,324 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:42699
2025-09-03 10:52:56,324 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,324 - distributed.worker - INFO -          dashboard at:          10.6.105.20:37073
2025-09-03 10:52:56,324 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,324 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,324 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-zjem3o89
2025-09-03 10:52:56,324 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,324 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,324 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,324 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,324 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-huwrmqkw
2025-09-03 10:52:56,324 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,341 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:33143
2025-09-03 10:52:56,341 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:33143
2025-09-03 10:52:56,341 - distributed.worker - INFO -          dashboard at:          10.6.105.20:39511
2025-09-03 10:52:56,341 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,341 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,341 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,341 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,341 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-xjy84o98
2025-09-03 10:52:56,341 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,344 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:37261
2025-09-03 10:52:56,344 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:37261
2025-09-03 10:52:56,344 - distributed.worker - INFO -          dashboard at:          10.6.105.20:33635
2025-09-03 10:52:56,344 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,344 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,344 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,344 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,344 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-1xag3da0
2025-09-03 10:52:56,344 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,422 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:42661
2025-09-03 10:52:56,422 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:42661
2025-09-03 10:52:56,422 - distributed.worker - INFO -          dashboard at:          10.6.105.20:34203
2025-09-03 10:52:56,422 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,422 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,422 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,422 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,422 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-t9_h0hi7
2025-09-03 10:52:56,422 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,472 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:36291
2025-09-03 10:52:56,472 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:36291
2025-09-03 10:52:56,472 - distributed.worker - INFO -          dashboard at:          10.6.105.20:33571
2025-09-03 10:52:56,472 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,472 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,472 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,472 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,472 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-h1xq252u
2025-09-03 10:52:56,472 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,475 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:35837
2025-09-03 10:52:56,475 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:35837
2025-09-03 10:52:56,475 - distributed.worker - INFO -          dashboard at:          10.6.105.20:39743
2025-09-03 10:52:56,475 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,475 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,475 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,475 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,475 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-l6h9kwz1
2025-09-03 10:52:56,475 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,477 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:38155
2025-09-03 10:52:56,477 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:38155
2025-09-03 10:52:56,477 - distributed.worker - INFO -          dashboard at:          10.6.105.20:35505
2025-09-03 10:52:56,477 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,477 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,477 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,477 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,477 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-aep1vy6w
2025-09-03 10:52:56,477 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,485 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:40209
2025-09-03 10:52:56,485 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:40209
2025-09-03 10:52:56,485 - distributed.worker - INFO -          dashboard at:          10.6.105.20:41279
2025-09-03 10:52:56,485 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,485 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,485 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,485 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,485 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-dtda6egv
2025-09-03 10:52:56,485 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,487 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:42087
2025-09-03 10:52:56,487 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:42087
2025-09-03 10:52:56,487 - distributed.worker - INFO -          dashboard at:          10.6.105.20:41285
2025-09-03 10:52:56,487 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,487 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,487 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,487 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,487 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-q1hfn9cy
2025-09-03 10:52:56,487 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,490 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:46109
2025-09-03 10:52:56,490 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:46109
2025-09-03 10:52:56,490 - distributed.worker - INFO -          dashboard at:          10.6.105.20:38551
2025-09-03 10:52:56,490 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,490 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,490 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,490 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,490 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-jsyoicq_
2025-09-03 10:52:56,490 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,494 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:33031
2025-09-03 10:52:56,494 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:33031
2025-09-03 10:52:56,494 - distributed.worker - INFO -          dashboard at:          10.6.105.20:43425
2025-09-03 10:52:56,494 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,494 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,494 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,494 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,494 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-y3giopqe
2025-09-03 10:52:56,494 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,495 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:41555
2025-09-03 10:52:56,495 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:41555
2025-09-03 10:52:56,495 - distributed.worker - INFO -          dashboard at:          10.6.105.20:44243
2025-09-03 10:52:56,496 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,496 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,496 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,496 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,496 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9n8e0184
2025-09-03 10:52:56,496 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,505 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:33469
2025-09-03 10:52:56,505 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:33469
2025-09-03 10:52:56,505 - distributed.worker - INFO -          dashboard at:          10.6.105.20:46411
2025-09-03 10:52:56,505 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,505 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,505 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,505 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,505 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-4rteg9ty
2025-09-03 10:52:56,505 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,507 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:46031
2025-09-03 10:52:56,507 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:46031
2025-09-03 10:52:56,507 - distributed.worker - INFO -          dashboard at:          10.6.105.20:44955
2025-09-03 10:52:56,507 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,507 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,507 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,507 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-onhi8596
2025-09-03 10:52:56,508 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,515 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:35075
2025-09-03 10:52:56,515 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:35075
2025-09-03 10:52:56,515 - distributed.worker - INFO -          dashboard at:          10.6.105.20:39413
2025-09-03 10:52:56,515 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,515 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,515 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,515 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,515 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9jobt_zb
2025-09-03 10:52:56,515 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,520 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:37605
2025-09-03 10:52:56,521 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:37605
2025-09-03 10:52:56,521 - distributed.worker - INFO -          dashboard at:          10.6.105.20:38991
2025-09-03 10:52:56,521 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,521 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,521 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,521 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,521 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-mkg3nk4s
2025-09-03 10:52:56,521 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,523 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:43707
2025-09-03 10:52:56,523 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:43707
2025-09-03 10:52:56,523 - distributed.worker - INFO -          dashboard at:          10.6.105.20:40439
2025-09-03 10:52:56,523 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,523 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,523 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,523 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,523 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-13ibpg0o
2025-09-03 10:52:56,523 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,524 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:46065
2025-09-03 10:52:56,525 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:46065
2025-09-03 10:52:56,525 - distributed.worker - INFO -          dashboard at:          10.6.105.20:34927
2025-09-03 10:52:56,525 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,525 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,525 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,525 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,525 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-7sioc19e
2025-09-03 10:52:56,525 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,527 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:38603
2025-09-03 10:52:56,527 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:38603
2025-09-03 10:52:56,527 - distributed.worker - INFO -          dashboard at:          10.6.105.20:43447
2025-09-03 10:52:56,527 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,527 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,527 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9lik38p0
2025-09-03 10:52:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,529 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:34289
2025-09-03 10:52:56,529 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:34289
2025-09-03 10:52:56,529 - distributed.worker - INFO -          dashboard at:          10.6.105.20:40333
2025-09-03 10:52:56,530 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,530 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,530 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,530 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,530 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-l_ajc8ui
2025-09-03 10:52:56,530 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,530 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:33175
2025-09-03 10:52:56,530 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:33175
2025-09-03 10:52:56,530 - distributed.worker - INFO -          dashboard at:          10.6.105.20:33025
2025-09-03 10:52:56,530 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,530 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,530 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,530 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,530 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-u7x56rjz
2025-09-03 10:52:56,530 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,532 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:33111
2025-09-03 10:52:56,532 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:33111
2025-09-03 10:52:56,532 - distributed.worker - INFO -          dashboard at:          10.6.105.20:34181
2025-09-03 10:52:56,532 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,532 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,532 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,532 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:38225
2025-09-03 10:52:56,532 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,532 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:38225
2025-09-03 10:52:56,532 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-g94f7c1z
2025-09-03 10:52:56,532 - distributed.worker - INFO -          dashboard at:          10.6.105.20:35197
2025-09-03 10:52:56,532 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,532 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,532 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,532 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,532 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,532 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-f9vszpnp
2025-09-03 10:52:56,532 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,536 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:36289
2025-09-03 10:52:56,536 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:36289
2025-09-03 10:52:56,536 - distributed.worker - INFO -          dashboard at:          10.6.105.20:33355
2025-09-03 10:52:56,536 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,536 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,536 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,536 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,536 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-swhp3g1t
2025-09-03 10:52:56,536 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,541 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:42215
2025-09-03 10:52:56,542 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:42215
2025-09-03 10:52:56,542 - distributed.worker - INFO -          dashboard at:          10.6.105.20:43477
2025-09-03 10:52:56,542 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,542 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,542 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,542 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,542 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-7s426b9v
2025-09-03 10:52:56,542 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,542 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:33225
2025-09-03 10:52:56,542 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:33225
2025-09-03 10:52:56,542 - distributed.worker - INFO -          dashboard at:          10.6.105.20:43257
2025-09-03 10:52:56,542 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,542 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,542 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,542 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,542 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-8jwaui4e
2025-09-03 10:52:56,542 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,547 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:32785
2025-09-03 10:52:56,547 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:32785
2025-09-03 10:52:56,547 - distributed.worker - INFO -          dashboard at:          10.6.105.20:44807
2025-09-03 10:52:56,547 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,547 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,547 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,547 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,547 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-q0ccn83j
2025-09-03 10:52:56,547 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,550 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:38265
2025-09-03 10:52:56,550 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:38265
2025-09-03 10:52:56,550 - distributed.worker - INFO -          dashboard at:          10.6.105.20:35849
2025-09-03 10:52:56,550 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,551 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,551 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,551 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,551 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-hzpr3q5g
2025-09-03 10:52:56,551 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,552 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:44229
2025-09-03 10:52:56,552 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:44229
2025-09-03 10:52:56,552 - distributed.worker - INFO -          dashboard at:          10.6.105.20:36575
2025-09-03 10:52:56,552 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,552 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,552 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,552 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,552 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-dy49rfb7
2025-09-03 10:52:56,552 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,553 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:46515
2025-09-03 10:52:56,553 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:46515
2025-09-03 10:52:56,553 - distributed.worker - INFO -          dashboard at:          10.6.105.20:42431
2025-09-03 10:52:56,553 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,553 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,553 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,553 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,553 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-91wn9esu
2025-09-03 10:52:56,553 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,553 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:46457
2025-09-03 10:52:56,553 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:46457
2025-09-03 10:52:56,553 - distributed.worker - INFO -          dashboard at:          10.6.105.20:34783
2025-09-03 10:52:56,553 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,553 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,554 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,554 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,554 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ytscfddp
2025-09-03 10:52:56,554 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,554 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:34149
2025-09-03 10:52:56,554 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:34149
2025-09-03 10:52:56,554 - distributed.worker - INFO -          dashboard at:          10.6.105.20:38507
2025-09-03 10:52:56,555 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,555 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,555 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,555 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,555 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-o5f874el
2025-09-03 10:52:56,555 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,557 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:33259
2025-09-03 10:52:56,557 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:33259
2025-09-03 10:52:56,557 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:46275
2025-09-03 10:52:56,557 - distributed.worker - INFO -          dashboard at:          10.6.105.20:42115
2025-09-03 10:52:56,557 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:46275
2025-09-03 10:52:56,557 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,557 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,557 - distributed.worker - INFO -          dashboard at:          10.6.105.20:38711
2025-09-03 10:52:56,557 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,557 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,557 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,557 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,557 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,557 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-q9htnvbv
2025-09-03 10:52:56,557 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,557 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,557 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-zi15edtl
2025-09-03 10:52:56,558 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,564 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:46493
2025-09-03 10:52:56,564 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:46493
2025-09-03 10:52:56,564 - distributed.worker - INFO -          dashboard at:          10.6.105.20:46247
2025-09-03 10:52:56,564 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,564 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,564 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,564 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,564 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-a78kfhql
2025-09-03 10:52:56,565 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,573 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:44361
2025-09-03 10:52:56,573 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:44361
2025-09-03 10:52:56,573 - distributed.worker - INFO -          dashboard at:          10.6.105.20:44065
2025-09-03 10:52:56,573 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,573 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,573 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,573 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,573 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-e1t82zv0
2025-09-03 10:52:56,573 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,726 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:40903'
2025-09-03 10:52:57,730 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:37097'
2025-09-03 10:52:57,735 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:35627'
2025-09-03 10:52:57,740 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:45023'
2025-09-03 10:52:57,744 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:32963'
2025-09-03 10:52:57,749 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:37245'
2025-09-03 10:52:57,755 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:44471'
2025-09-03 10:52:57,760 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:39619'
2025-09-03 10:52:57,764 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:38183'
2025-09-03 10:52:57,770 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:45251'
2025-09-03 10:52:57,774 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:41083'
2025-09-03 10:52:57,780 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:43615'
2025-09-03 10:52:57,785 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:40923'
2025-09-03 10:52:57,789 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:39165'
2025-09-03 10:52:57,794 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:40347'
2025-09-03 10:52:57,799 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:34631'
2025-09-03 10:52:57,805 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:44525'
2025-09-03 10:52:57,812 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:43449'
2025-09-03 10:52:57,817 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:37655'
2025-09-03 10:52:57,824 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:42409'
2025-09-03 10:52:57,829 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:35517'
2025-09-03 10:52:58,368 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,369 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,369 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,371 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,387 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,388 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,389 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,390 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,593 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:39815
2025-09-03 10:52:58,593 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:39815
2025-09-03 10:52:58,593 - distributed.worker - INFO -          dashboard at:          10.6.105.20:41571
2025-09-03 10:52:58,593 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,593 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,593 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,593 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,593 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-2ly9tuqq
2025-09-03 10:52:58,593 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,627 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:35343
2025-09-03 10:52:58,627 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:35343
2025-09-03 10:52:58,627 - distributed.worker - INFO -          dashboard at:          10.6.105.20:45893
2025-09-03 10:52:58,627 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,627 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,627 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,627 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,627 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-rqtlsv5c
2025-09-03 10:52:58,627 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,643 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:41129
2025-09-03 10:52:58,643 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:41129
2025-09-03 10:52:58,643 - distributed.worker - INFO -          dashboard at:          10.6.105.20:46399
2025-09-03 10:52:58,643 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,643 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,643 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,643 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,643 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-hp60x9ui
2025-09-03 10:52:58,643 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,661 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:37337
2025-09-03 10:52:58,661 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:37337
2025-09-03 10:52:58,661 - distributed.worker - INFO -          dashboard at:          10.6.105.20:34529
2025-09-03 10:52:58,661 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,661 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,661 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,661 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,661 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-od88libj
2025-09-03 10:52:58,661 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,689 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:41983
2025-09-03 10:52:58,689 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:41983
2025-09-03 10:52:58,689 - distributed.worker - INFO -          dashboard at:          10.6.105.20:37843
2025-09-03 10:52:58,689 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,689 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,689 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,689 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,689 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-lyfo4wrm
2025-09-03 10:52:58,689 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,696 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:35495
2025-09-03 10:52:58,696 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:35495
2025-09-03 10:52:58,696 - distributed.worker - INFO -          dashboard at:          10.6.105.20:40309
2025-09-03 10:52:58,696 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,696 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,696 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,696 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,696 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-3x2sz7cg
2025-09-03 10:52:58,696 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,704 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:41101
2025-09-03 10:52:58,705 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:41101
2025-09-03 10:52:58,705 - distributed.worker - INFO -          dashboard at:          10.6.105.20:40231
2025-09-03 10:52:58,705 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,705 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,705 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,705 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,705 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_45od7xx
2025-09-03 10:52:58,705 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,705 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:35043
2025-09-03 10:52:58,705 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:35043
2025-09-03 10:52:58,705 - distributed.worker - INFO -          dashboard at:          10.6.105.20:34335
2025-09-03 10:52:58,705 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,705 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,705 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,705 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,705 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-3y6303bk
2025-09-03 10:52:58,705 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,707 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:40103
2025-09-03 10:52:58,707 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:44463
2025-09-03 10:52:58,707 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:40103
2025-09-03 10:52:58,707 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:44463
2025-09-03 10:52:58,707 - distributed.worker - INFO -          dashboard at:          10.6.105.20:43907
2025-09-03 10:52:58,707 - distributed.worker - INFO -          dashboard at:          10.6.105.20:44485
2025-09-03 10:52:58,707 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,707 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,707 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,707 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,707 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,707 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,707 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,707 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,707 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-bkd_0p61
2025-09-03 10:52:58,707 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-vkp7gsg5
2025-09-03 10:52:58,707 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,707 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,717 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:41403
2025-09-03 10:52:58,717 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:41403
2025-09-03 10:52:58,717 - distributed.worker - INFO -          dashboard at:          10.6.105.20:41281
2025-09-03 10:52:58,717 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,717 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,717 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,717 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,717 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_r_7h4z6
2025-09-03 10:52:58,717 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,742 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:36797
2025-09-03 10:52:58,742 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:36797
2025-09-03 10:52:58,742 - distributed.worker - INFO -          dashboard at:          10.6.105.20:33757
2025-09-03 10:52:58,742 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,742 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,742 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,742 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,742 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-anf4vse6
2025-09-03 10:52:58,742 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,742 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:42911
2025-09-03 10:52:58,743 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:42911
2025-09-03 10:52:58,743 - distributed.worker - INFO -          dashboard at:          10.6.105.20:39663
2025-09-03 10:52:58,743 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,743 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,743 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,743 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,743 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-hyhexi5n
2025-09-03 10:52:58,743 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,747 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:46297
2025-09-03 10:52:58,747 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:46297
2025-09-03 10:52:58,747 - distributed.worker - INFO -          dashboard at:          10.6.105.20:39497
2025-09-03 10:52:58,747 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,747 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,747 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,747 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,747 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-bmhy976i
2025-09-03 10:52:58,747 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,750 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:45919
2025-09-03 10:52:58,750 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:45919
2025-09-03 10:52:58,750 - distributed.worker - INFO -          dashboard at:          10.6.105.20:35255
2025-09-03 10:52:58,750 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,750 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,750 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,750 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,750 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-uswl4s85
2025-09-03 10:52:58,750 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,756 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:42553
2025-09-03 10:52:58,756 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:42553
2025-09-03 10:52:58,757 - distributed.worker - INFO -          dashboard at:          10.6.105.20:45555
2025-09-03 10:52:58,757 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,757 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,757 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,757 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,757 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-oo18s96g
2025-09-03 10:52:58,757 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,757 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:38091
2025-09-03 10:52:58,757 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:38091
2025-09-03 10:52:58,757 - distributed.worker - INFO -          dashboard at:          10.6.105.20:34913
2025-09-03 10:52:58,757 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,757 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,757 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,757 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,757 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-b2q7_1pq
2025-09-03 10:52:58,757 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,765 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:42109
2025-09-03 10:52:58,765 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:42109
2025-09-03 10:52:58,765 - distributed.worker - INFO -          dashboard at:          10.6.105.20:40153
2025-09-03 10:52:58,765 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,765 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,765 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,765 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,765 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-mgf6bxzl
2025-09-03 10:52:58,765 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,766 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:39851
2025-09-03 10:52:58,766 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:39851
2025-09-03 10:52:58,766 - distributed.worker - INFO -          dashboard at:          10.6.105.20:41337
2025-09-03 10:52:58,766 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,766 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,766 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,766 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,766 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-jpka_qox
2025-09-03 10:52:58,766 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,776 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:34457
2025-09-03 10:52:58,776 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:34457
2025-09-03 10:52:58,776 - distributed.worker - INFO -          dashboard at:          10.6.105.20:45549
2025-09-03 10:52:58,777 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,777 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,777 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,777 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,777 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-te9ffvn0
2025-09-03 10:52:58,777 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,788 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:39911
2025-09-03 10:52:58,788 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:39911
2025-09-03 10:52:58,788 - distributed.worker - INFO -          dashboard at:          10.6.105.20:44181
2025-09-03 10:52:58,788 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,788 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,788 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,788 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,788 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-rrpunkgp
2025-09-03 10:52:58,789 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:00,173 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:46811'
2025-09-03 10:53:00,178 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:38223'
2025-09-03 10:53:00,183 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:41845'
2025-09-03 10:53:00,187 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:34715'
2025-09-03 10:53:00,193 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:34157'
2025-09-03 10:53:00,199 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:35965'
2025-09-03 10:53:00,205 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:45285'
2025-09-03 10:53:00,209 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:39003'
2025-09-03 10:53:00,216 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:35649'
2025-09-03 10:53:00,220 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:33157'
2025-09-03 10:53:00,226 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:35233'
2025-09-03 10:53:00,231 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:39291'
2025-09-03 10:53:00,236 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:42493'
2025-09-03 10:53:00,243 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:41837'
2025-09-03 10:53:00,248 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:37351'
2025-09-03 10:53:00,254 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:42921'
2025-09-03 10:53:00,261 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:34565'
2025-09-03 10:53:00,267 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:43611'
2025-09-03 10:53:00,273 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:33773'
2025-09-03 10:53:00,279 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:34497'
2025-09-03 10:53:00,285 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:44575'
2025-09-03 10:53:00,292 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:33107'
2025-09-03 10:53:00,298 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:44633'
2025-09-03 10:53:00,304 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:42917'
2025-09-03 10:53:00,309 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:45443'
2025-09-03 10:53:00,313 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:36245'
2025-09-03 10:53:00,319 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.20:44207'
2025-09-03 10:53:01,144 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:42781
2025-09-03 10:53:01,144 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:42781
2025-09-03 10:53:01,144 - distributed.worker - INFO -          dashboard at:          10.6.105.20:38295
2025-09-03 10:53:01,144 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,144 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,144 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,144 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,144 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-zrmjwjl3
2025-09-03 10:53:01,144 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,148 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:43437
2025-09-03 10:53:01,148 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:43437
2025-09-03 10:53:01,148 - distributed.worker - INFO -          dashboard at:          10.6.105.20:46321
2025-09-03 10:53:01,148 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,148 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,148 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,148 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,148 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-snmj01x3
2025-09-03 10:53:01,148 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,155 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,156 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,156 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,158 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,166 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:35237
2025-09-03 10:53:01,166 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:35237
2025-09-03 10:53:01,166 - distributed.worker - INFO -          dashboard at:          10.6.105.20:33275
2025-09-03 10:53:01,166 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,166 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,166 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,167 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,167 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-akuja8hv
2025-09-03 10:53:01,167 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,172 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,173 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,175 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,188 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,190 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,190 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,191 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,205 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,206 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,206 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,208 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,259 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:36481
2025-09-03 10:53:01,260 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:36481
2025-09-03 10:53:01,260 - distributed.worker - INFO -          dashboard at:          10.6.105.20:37797
2025-09-03 10:53:01,260 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,260 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,260 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,260 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,260 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-4e9lp6qj
2025-09-03 10:53:01,260 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,295 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:43809
2025-09-03 10:53:01,295 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:43809
2025-09-03 10:53:01,295 - distributed.worker - INFO -          dashboard at:          10.6.105.20:43587
2025-09-03 10:53:01,295 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,295 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,295 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,295 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,295 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_pm3lslp
2025-09-03 10:53:01,295 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,303 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:35553
2025-09-03 10:53:01,303 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:41587
2025-09-03 10:53:01,303 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:35553
2025-09-03 10:53:01,303 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:41587
2025-09-03 10:53:01,303 - distributed.worker - INFO -          dashboard at:          10.6.105.20:43939
2025-09-03 10:53:01,303 - distributed.worker - INFO -          dashboard at:          10.6.105.20:33973
2025-09-03 10:53:01,303 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,303 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,303 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,303 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,303 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,303 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,303 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,303 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,303 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-1_jajc02
2025-09-03 10:53:01,303 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-xii_79nh
2025-09-03 10:53:01,304 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,304 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,305 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:39435
2025-09-03 10:53:01,305 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:39435
2025-09-03 10:53:01,305 - distributed.worker - INFO -          dashboard at:          10.6.105.20:44987
2025-09-03 10:53:01,305 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,305 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,305 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,305 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,305 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-19a2kef1
2025-09-03 10:53:01,305 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,305 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:32833
2025-09-03 10:53:01,305 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:32833
2025-09-03 10:53:01,305 - distributed.worker - INFO -          dashboard at:          10.6.105.20:45447
2025-09-03 10:53:01,305 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,305 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,305 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,305 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,305 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-z39orrui
2025-09-03 10:53:01,305 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,361 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:35585
2025-09-03 10:53:01,361 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:35585
2025-09-03 10:53:01,361 - distributed.worker - INFO -          dashboard at:          10.6.105.20:40105
2025-09-03 10:53:01,361 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,361 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,361 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,361 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,361 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-edbagzjq
2025-09-03 10:53:01,361 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,363 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:46473
2025-09-03 10:53:01,363 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:46473
2025-09-03 10:53:01,363 - distributed.worker - INFO -          dashboard at:          10.6.105.20:36463
2025-09-03 10:53:01,363 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,363 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,363 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,363 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,363 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-rpk_dbd2
2025-09-03 10:53:01,363 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,367 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:39051
2025-09-03 10:53:01,367 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:39051
2025-09-03 10:53:01,367 - distributed.worker - INFO -          dashboard at:          10.6.105.20:36781
2025-09-03 10:53:01,367 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,367 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,367 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,367 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,367 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-k7ti5pgf
2025-09-03 10:53:01,367 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,368 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:33871
2025-09-03 10:53:01,368 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:33871
2025-09-03 10:53:01,368 - distributed.worker - INFO -          dashboard at:          10.6.105.20:42117
2025-09-03 10:53:01,368 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,368 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,368 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,368 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,368 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-r6mo4za0
2025-09-03 10:53:01,368 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,368 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:46153
2025-09-03 10:53:01,368 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:46153
2025-09-03 10:53:01,369 - distributed.worker - INFO -          dashboard at:          10.6.105.20:35257
2025-09-03 10:53:01,369 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,369 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,369 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,369 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,369 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-s9bpo3od
2025-09-03 10:53:01,369 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,370 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:43173
2025-09-03 10:53:01,370 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:43173
2025-09-03 10:53:01,370 - distributed.worker - INFO -          dashboard at:          10.6.105.20:34297
2025-09-03 10:53:01,370 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,370 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,370 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,370 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,370 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-i8p8hik1
2025-09-03 10:53:01,370 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,372 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:38677
2025-09-03 10:53:01,372 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:38677
2025-09-03 10:53:01,372 - distributed.worker - INFO -          dashboard at:          10.6.105.20:34189
2025-09-03 10:53:01,372 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,372 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,372 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,372 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,372 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-oxbjf3km
2025-09-03 10:53:01,372 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,372 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:45613
2025-09-03 10:53:01,372 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:45613
2025-09-03 10:53:01,372 - distributed.worker - INFO -          dashboard at:          10.6.105.20:41793
2025-09-03 10:53:01,373 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,373 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,373 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,373 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,373 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-rqd2ind5
2025-09-03 10:53:01,373 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,375 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:40283
2025-09-03 10:53:01,375 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:40283
2025-09-03 10:53:01,375 - distributed.worker - INFO -          dashboard at:          10.6.105.20:37711
2025-09-03 10:53:01,375 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,375 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,375 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,375 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,375 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-wh7mrq5k
2025-09-03 10:53:01,375 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,378 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:46629
2025-09-03 10:53:01,378 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:46629
2025-09-03 10:53:01,378 - distributed.worker - INFO -          dashboard at:          10.6.105.20:38013
2025-09-03 10:53:01,378 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,378 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,378 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,378 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,378 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-zyydg9ec
2025-09-03 10:53:01,378 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,395 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:43165
2025-09-03 10:53:01,395 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:43165
2025-09-03 10:53:01,395 - distributed.worker - INFO -          dashboard at:          10.6.105.20:41025
2025-09-03 10:53:01,395 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,395 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,395 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,395 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,395 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-c5m9ucju
2025-09-03 10:53:01,395 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,396 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:35443
2025-09-03 10:53:01,396 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:35443
2025-09-03 10:53:01,396 - distributed.worker - INFO -          dashboard at:          10.6.105.20:37931
2025-09-03 10:53:01,396 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,396 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,396 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,396 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,396 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-zvnleakh
2025-09-03 10:53:01,396 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,396 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:36365
2025-09-03 10:53:01,396 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:36365
2025-09-03 10:53:01,396 - distributed.worker - INFO -          dashboard at:          10.6.105.20:33697
2025-09-03 10:53:01,396 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,396 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,396 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,396 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,396 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-inewsfeo
2025-09-03 10:53:01,396 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,399 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:37751
2025-09-03 10:53:01,399 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:37751
2025-09-03 10:53:01,399 - distributed.worker - INFO -          dashboard at:          10.6.105.20:43451
2025-09-03 10:53:01,399 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,399 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,399 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,399 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,399 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-t7977pwu
2025-09-03 10:53:01,399 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,402 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:36833
2025-09-03 10:53:01,403 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:36833
2025-09-03 10:53:01,403 - distributed.worker - INFO -          dashboard at:          10.6.105.20:43251
2025-09-03 10:53:01,403 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,403 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,403 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,403 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,403 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-c7tz17hp
2025-09-03 10:53:01,403 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,406 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:35723
2025-09-03 10:53:01,406 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:35723
2025-09-03 10:53:01,406 - distributed.worker - INFO -          dashboard at:          10.6.105.20:33379
2025-09-03 10:53:01,406 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,406 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,406 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,406 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,406 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-h9sbnwjh
2025-09-03 10:53:01,406 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,411 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:46261
2025-09-03 10:53:01,411 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:46261
2025-09-03 10:53:01,411 - distributed.worker - INFO -          dashboard at:          10.6.105.20:41001
2025-09-03 10:53:01,412 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,412 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,412 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,412 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,412 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-4z6fnthj
2025-09-03 10:53:01,412 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,422 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.20:35433
2025-09-03 10:53:01,422 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.20:35433
2025-09-03 10:53:01,422 - distributed.worker - INFO -          dashboard at:          10.6.105.20:42543
2025-09-03 10:53:01,422 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,422 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,422 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:01,422 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:01,422 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ib6awtjj
2025-09-03 10:53:01,423 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,692 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,693 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,693 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,695 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,709 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,710 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,710 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,711 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,726 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,727 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,727 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,729 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,742 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,743 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,744 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,745 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,759 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,760 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,761 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,762 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,927 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,928 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,928 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,930 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,943 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,944 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,945 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,946 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,412 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,413 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,413 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,415 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,428 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,429 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,430 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,432 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,662 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,663 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,665 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,679 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,680 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,680 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,682 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,796 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,798 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,798 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,800 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,813 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,814 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,814 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,817 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,828 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,829 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,829 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,830 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,845 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,847 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,847 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,849 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,862 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,863 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,863 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,865 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,879 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,880 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,880 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,882 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,895 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,897 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,897 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,899 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,911 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,913 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,913 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,915 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,929 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,930 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,930 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,932 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,945 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,946 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,946 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,948 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,961 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,962 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,963 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,964 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,978 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,980 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,980 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,981 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,995 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,996 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,996 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,998 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:04,311 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:04,312 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:04,313 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:04,314 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:04,553 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:04,554 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:04,554 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:04,555 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:05,098 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:05,099 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:05,099 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:05,100 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:05,115 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:05,116 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:05,116 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:05,117 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:05,132 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:05,133 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:05,133 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:05,134 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:05,148 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:05,150 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:05,150 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:05,151 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:05,165 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:05,166 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:05,166 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:05,168 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:07,939 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:07,940 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:07,940 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:07,942 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:09,500 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:09,502 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:09,502 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:09,503 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:09,517 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:09,519 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:09,519 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:09,520 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:09,534 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:09,535 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:09,535 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:09,537 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:09,551 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:09,552 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:09,552 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:09,553 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:09,567 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:09,569 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:09,569 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:09,570 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:09,584 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:09,586 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:09,586 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:09,587 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:09,601 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:09,602 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:09,603 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:09,604 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:09,618 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:09,619 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:09,619 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:09,621 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:11,997 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:11,998 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:11,998 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,000 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:15,277 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:15,278 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:15,278 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:15,279 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:15,293 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:15,295 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:15,295 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:15,296 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:15,311 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:15,312 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:15,312 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:15,313 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:15,328 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:15,329 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:15,329 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:15,331 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:15,345 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:15,346 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:15,346 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:15,348 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:15,362 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:15,363 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:15,364 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:15,365 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:15,379 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:15,380 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:15,380 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:15,382 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:15,396 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:15,398 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:15,398 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:15,399 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:15,413 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:15,415 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:15,415 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:15,416 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:15,431 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:15,432 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:15,432 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:15,433 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,101 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,102 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,102 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,104 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,119 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,121 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,121 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,123 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,137 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,138 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,139 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,140 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,154 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,155 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,156 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,157 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,172 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,173 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,175 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,190 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,191 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,191 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,193 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,207 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,208 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,209 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,210 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,225 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,226 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,226 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,228 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,243 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,244 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,244 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,246 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,260 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,262 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,262 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,264 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,278 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,280 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,280 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,282 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,296 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,297 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,297 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,299 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,313 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,314 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,316 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,331 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,333 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,333 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,335 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,348 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,350 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,350 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,351 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,366 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,367 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,367 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,369 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,383 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,385 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,385 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,387 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,401 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,403 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,403 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,404 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,418 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,420 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,420 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,422 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,437 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,438 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,439 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,440 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,454 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,456 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,456 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,458 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,472 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,474 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,474 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,476 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,491 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,492 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,493 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,494 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,508 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,510 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,510 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,512 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,525 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,526 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,528 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,543 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,544 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,545 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,546 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:20,561 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:20,563 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,563 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:20,565 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:25,121 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:25,123 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,123 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:25,124 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:25,139 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:25,141 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,141 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:25,143 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:25,160 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:25,164 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,164 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:25,168 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:25,175 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:25,176 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,177 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:25,178 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:25,193 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:25,195 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:25,197 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:25,211 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:25,213 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,213 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:25,215 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:25,230 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:25,232 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,232 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:25,233 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:25,248 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:25,249 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,250 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:25,251 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:25,269 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:25,272 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,273 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:25,278 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:25,396 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,396 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,402 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,405 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,745 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,858 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,937 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,951 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:28,594 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:28,628 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:28,644 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:32,164 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:32,472 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:32,474 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:32,474 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:32,475 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:32,677 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:32,679 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:32,679 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:32,681 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:32,699 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:32,714 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:32,750 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:32,752 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:32,752 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:32,754 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:32,931 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:32,949 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:33,900 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:33,902 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:33,902 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:33,904 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:33,918 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:33,919 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:33,920 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:33,921 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:33,935 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:33,936 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:33,936 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:33,938 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:33,952 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:33,954 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:33,954 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:33,956 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:34,983 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:34,999 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:35,317 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:35,558 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:37,972 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:37,973 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:37,973 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:37,975 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:38,026 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:38,027 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:38,027 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:38,029 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:40,505 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:40,523 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:40,540 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:40,556 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:40,574 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:40,658 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:40,660 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:40,660 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:40,662 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,403 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,405 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,405 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,407 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:51,233 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:51,249 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:51,266 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:51,284 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:56:38,444 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,446 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,458 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,463 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,688 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,690 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,738 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,739 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,758 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,759 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,144 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,151 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,292 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,293 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,303 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,310 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,401 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,407 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,638 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,644 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,191 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,192 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,429 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,431 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,451 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,456 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,482 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,483 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,791 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,793 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,048 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,051 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,321 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,322 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,392 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,398 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,615 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,616 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,649 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,651 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,682 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,684 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,936 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,941 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,234 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,240 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,408 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,411 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,612 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,614 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,618 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,619 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,623 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,626 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,679 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,681 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,327 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,333 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,419 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,421 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,453 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,454 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,696 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,699 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,781 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,783 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,179 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,184 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,376 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,378 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,465 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,476 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,581 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,587 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,715 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,720 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,205 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,215 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,274 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,278 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,586 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,588 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,783 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,791 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,342 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,344 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,662 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,669 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,914 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,920 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,941 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,942 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,110 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,116 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,131 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,137 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,245 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,251 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,478 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,484 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,085 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,086 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,288 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,289 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,321 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,322 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,347 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,348 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,665 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,670 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,671 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,675 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,687 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,689 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,977 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,982 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,820 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,823 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,060 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,065 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,139 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,144 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,166 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,168 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,243 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,245 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,611 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,613 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,060 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,061 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,063 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,065 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,071 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,073 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,208 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,210 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,276 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,280 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,753 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,754 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,920 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,921 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,071 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,073 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,587 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,592 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,639 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,645 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,651 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,652 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,668 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,674 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,675 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,685 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,693 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,694 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,183 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,191 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,431 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,433 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,648 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,655 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,168 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,170 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,803 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,810 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,091 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,097 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,314 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,319 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,352 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,358 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,519 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,520 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:56,813 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:56,818 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:57,191 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:57,194 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:57,401 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:57,402 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:57,414 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:57,416 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:57,761 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:57,764 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:58,608 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:58,611 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:58,804 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:58,810 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:59,046 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:59,051 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:59,487 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:59,489 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:00,161 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:00,166 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:00,559 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:00,564 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:01,318 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:01,323 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:02,144 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:02,151 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:02,584 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:02,591 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:03,031 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:03,033 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:04,433 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:04,435 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:04,921 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:04,927 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:09,992 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:09,994 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:09,999 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:09,999 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,001 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,001 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,005 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,007 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,010 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,011 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,012 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,013 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,012 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,016 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,017 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,019 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,071 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,073 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,333 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,335 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,345 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,346 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,347 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,348 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,351 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,354 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,354 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,354 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,356 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,356 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,357 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,357 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,358 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,360 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,362 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,362 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,364 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,366 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,491 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,493 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,517 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,520 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,528 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,530 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,530 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,532 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,541 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,543 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,545 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,547 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,618 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,620 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,626 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,633 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,658 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,658 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,660 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,661 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,736 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,738 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,739 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,741 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,751 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,754 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,754 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,756 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,785 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,787 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,880 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,882 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,888 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,890 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,896 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,898 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,920 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,922 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,931 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,933 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,121 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,123 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,123 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,124 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,182 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,184 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,269 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,271 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,271 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,273 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,317 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,320 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,372 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,374 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,390 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,392 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,392 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,395 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,459 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,461 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,476 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,478 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,484 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,490 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,489 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,489 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,491 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,493 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,494 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,495 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,518 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,518 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,520 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,520 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,526 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,528 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,531 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,533 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,545 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,553 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,565 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,568 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,569 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,569 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,583 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,585 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,615 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,616 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,618 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,622 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,644 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,645 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,645 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,652 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,681 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,683 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,689 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,694 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,753 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,756 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,774 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,776 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,797 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,799 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,806 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,808 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,886 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,894 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,922 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,924 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,986 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,989 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,997 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,002 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,017 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,019 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,103 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,105 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,111 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,113 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,159 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,162 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,168 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,170 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,290 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,291 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,333 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,335 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,382 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,384 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,387 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,389 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,423 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,429 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,455 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,463 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,470 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,472 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,511 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,512 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,525 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,526 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,533 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,534 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,535 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,536 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,615 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,617 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,749 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,751 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,931 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,933 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,078 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,080 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,270 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,272 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,274 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,276 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,501 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,506 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,634 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,636 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,689 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,692 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:14,490 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:14,492 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:15,010 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:15,013 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:15,119 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:15,121 - distributed.utils - INFO - Reload module qme_vars from .py file
