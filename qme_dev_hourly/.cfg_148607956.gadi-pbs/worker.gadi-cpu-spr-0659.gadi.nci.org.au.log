Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 10:52:43,530 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:37867'
2025-09-03 10:52:43,542 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:35559'
2025-09-03 10:52:43,548 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:33623'
2025-09-03 10:52:43,554 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:36875'
2025-09-03 10:52:43,620 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:34683'
2025-09-03 10:52:44,111 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:36179'
2025-09-03 10:52:44,114 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:39023'
2025-09-03 10:52:44,118 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:44815'
2025-09-03 10:52:44,125 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:41679'
2025-09-03 10:52:44,129 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:36281'
2025-09-03 10:52:44,133 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:34819'
2025-09-03 10:52:44,137 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:45873'
2025-09-03 10:52:44,142 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:45587'
2025-09-03 10:52:44,152 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:42177'
2025-09-03 10:52:44,157 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:44975'
2025-09-03 10:52:44,161 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:37317'
2025-09-03 10:52:44,165 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:34327'
2025-09-03 10:52:44,169 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:42609'
2025-09-03 10:52:44,173 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:41967'
2025-09-03 10:52:44,178 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:45405'
2025-09-03 10:52:44,182 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:41725'
2025-09-03 10:52:44,187 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:43445'
2025-09-03 10:52:44,191 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:40073'
2025-09-03 10:52:44,196 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:37657'
2025-09-03 10:52:44,201 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:37259'
2025-09-03 10:52:44,206 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:44093'
2025-09-03 10:52:44,210 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:35001'
2025-09-03 10:52:44,214 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:33147'
2025-09-03 10:52:44,219 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:36119'
2025-09-03 10:52:44,224 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:39527'
2025-09-03 10:52:44,229 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:38039'
2025-09-03 10:52:44,234 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:39541'
2025-09-03 10:52:44,239 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:46543'
2025-09-03 10:52:44,243 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:37535'
2025-09-03 10:52:44,246 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:38521'
2025-09-03 10:52:44,250 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:42845'
2025-09-03 10:52:44,255 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:38255'
2025-09-03 10:52:44,260 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:34865'
2025-09-03 10:52:44,264 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:37563'
2025-09-03 10:52:44,269 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:33533'
2025-09-03 10:52:44,274 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:38981'
2025-09-03 10:52:44,278 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:44027'
2025-09-03 10:52:44,367 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:44799
2025-09-03 10:52:44,367 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:35215
2025-09-03 10:52:44,367 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:44799
2025-09-03 10:52:44,367 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:35215
2025-09-03 10:52:44,367 - distributed.worker - INFO -          dashboard at:          10.6.105.11:41771
2025-09-03 10:52:44,367 - distributed.worker - INFO -          dashboard at:          10.6.105.11:41575
2025-09-03 10:52:44,367 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,367 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:36253
2025-09-03 10:52:44,367 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:43651
2025-09-03 10:52:44,367 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,367 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,367 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:36253
2025-09-03 10:52:44,367 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:44,367 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,367 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:43651
2025-09-03 10:52:44,367 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:44,367 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:44,367 - distributed.worker - INFO -          dashboard at:          10.6.105.11:38983
2025-09-03 10:52:44,367 - distributed.worker - INFO -          dashboard at:          10.6.105.11:35357
2025-09-03 10:52:44,367 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:44,367 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-mf_ewmgi
2025-09-03 10:52:44,367 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,367 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,367 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-1h0hdn42
2025-09-03 10:52:44,367 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,367 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,367 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,367 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:44,367 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,367 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:44,367 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:44,367 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:44,367 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-4tvtxhag
2025-09-03 10:52:44,367 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-f9kv7w50
2025-09-03 10:52:44,367 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,367 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,395 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:37787'
2025-09-03 10:52:44,400 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:41881'
2025-09-03 10:52:44,405 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:38385'
2025-09-03 10:52:44,410 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:41875'
2025-09-03 10:52:44,415 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:35399'
2025-09-03 10:52:44,418 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:41455'
2025-09-03 10:52:44,422 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:41463
2025-09-03 10:52:44,422 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:41463
2025-09-03 10:52:44,422 - distributed.worker - INFO -          dashboard at:          10.6.105.11:40931
2025-09-03 10:52:44,422 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,422 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,422 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:44,422 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:44,423 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-6to0rcnj
2025-09-03 10:52:44,423 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,423 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:43901'
2025-09-03 10:52:44,429 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:38083'
2025-09-03 10:52:44,434 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:33491'
2025-09-03 10:52:44,439 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:42075'
2025-09-03 10:52:44,442 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:40643'
2025-09-03 10:52:44,446 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:34023'
2025-09-03 10:52:44,450 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:32875'
2025-09-03 10:52:44,453 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:44497'
2025-09-03 10:52:44,458 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:41293'
2025-09-03 10:52:44,463 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:41637'
2025-09-03 10:52:44,468 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:42359'
2025-09-03 10:52:44,473 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:40489'
2025-09-03 10:52:44,477 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:43097'
2025-09-03 10:52:44,482 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:37887'
2025-09-03 10:52:44,486 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:33597'
2025-09-03 10:52:44,491 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:40197'
2025-09-03 10:52:44,496 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:39433'
2025-09-03 10:52:44,500 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:39839'
2025-09-03 10:52:44,505 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:43443'
2025-09-03 10:52:44,510 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:32947'
2025-09-03 10:52:44,515 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:44403'
2025-09-03 10:52:44,519 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:42931'
2025-09-03 10:52:44,523 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:44145'
2025-09-03 10:52:44,527 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:44923'
2025-09-03 10:52:44,530 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:39349'
2025-09-03 10:52:44,534 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:34957'
2025-09-03 10:52:44,539 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:32911'
2025-09-03 10:52:44,545 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:39393'
2025-09-03 10:52:44,548 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:43025'
2025-09-03 10:52:45,144 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:37359
2025-09-03 10:52:45,145 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:37359
2025-09-03 10:52:45,145 - distributed.worker - INFO -          dashboard at:          10.6.105.11:38687
2025-09-03 10:52:45,145 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,145 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,145 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,145 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,145 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_y2xregi
2025-09-03 10:52:45,145 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,187 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:36987
2025-09-03 10:52:45,187 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:36987
2025-09-03 10:52:45,187 - distributed.worker - INFO -          dashboard at:          10.6.105.11:34689
2025-09-03 10:52:45,187 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,187 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,187 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,187 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,187 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-qgz67jzt
2025-09-03 10:52:45,187 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,262 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:33877
2025-09-03 10:52:45,262 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:33877
2025-09-03 10:52:45,262 - distributed.worker - INFO -          dashboard at:          10.6.105.11:37467
2025-09-03 10:52:45,262 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,262 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,262 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,262 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,262 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ium3ovns
2025-09-03 10:52:45,262 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,389 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:33521
2025-09-03 10:52:45,389 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:33521
2025-09-03 10:52:45,389 - distributed.worker - INFO -          dashboard at:          10.6.105.11:42173
2025-09-03 10:52:45,389 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,389 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,389 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,389 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,389 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-851_o02c
2025-09-03 10:52:45,389 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,395 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:44355
2025-09-03 10:52:45,395 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:44355
2025-09-03 10:52:45,395 - distributed.worker - INFO -          dashboard at:          10.6.105.11:42371
2025-09-03 10:52:45,395 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,395 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,395 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,395 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,395 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-d67dcftq
2025-09-03 10:52:45,395 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,409 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:33665
2025-09-03 10:52:45,409 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:33665
2025-09-03 10:52:45,409 - distributed.worker - INFO -          dashboard at:          10.6.105.11:33327
2025-09-03 10:52:45,409 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,409 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,409 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,409 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,410 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-v24vnis3
2025-09-03 10:52:45,410 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,527 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:45779
2025-09-03 10:52:45,527 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:45779
2025-09-03 10:52:45,527 - distributed.worker - INFO -          dashboard at:          10.6.105.11:46841
2025-09-03 10:52:45,527 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,527 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,527 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,527 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-z1cfoyuc
2025-09-03 10:52:45,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,567 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:45649
2025-09-03 10:52:45,567 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:45649
2025-09-03 10:52:45,567 - distributed.worker - INFO -          dashboard at:          10.6.105.11:34559
2025-09-03 10:52:45,567 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,567 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,567 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,567 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,567 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-zq7b8zi8
2025-09-03 10:52:45,567 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,631 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:37959
2025-09-03 10:52:45,631 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:37959
2025-09-03 10:52:45,631 - distributed.worker - INFO -          dashboard at:          10.6.105.11:39175
2025-09-03 10:52:45,631 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,631 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,631 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,631 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,631 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-v3jr6znf
2025-09-03 10:52:45,631 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,644 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:45351
2025-09-03 10:52:45,644 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:45351
2025-09-03 10:52:45,644 - distributed.worker - INFO -          dashboard at:          10.6.105.11:45923
2025-09-03 10:52:45,644 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,644 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,644 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,644 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,644 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-1j0n_ya3
2025-09-03 10:52:45,644 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,679 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:42913
2025-09-03 10:52:45,679 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:42913
2025-09-03 10:52:45,679 - distributed.worker - INFO -          dashboard at:          10.6.105.11:40041
2025-09-03 10:52:45,679 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,679 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,679 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,679 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,679 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-0bxss915
2025-09-03 10:52:45,679 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,695 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:38483
2025-09-03 10:52:45,695 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:38483
2025-09-03 10:52:45,695 - distributed.worker - INFO -          dashboard at:          10.6.105.11:44593
2025-09-03 10:52:45,695 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,695 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,695 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,695 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,695 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-fyf8hmvt
2025-09-03 10:52:45,695 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,698 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:43867
2025-09-03 10:52:45,699 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:43867
2025-09-03 10:52:45,699 - distributed.worker - INFO -          dashboard at:          10.6.105.11:36677
2025-09-03 10:52:45,699 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,699 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,699 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,699 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,699 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-qlpuf2i5
2025-09-03 10:52:45,699 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,724 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:41543
2025-09-03 10:52:45,725 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:41543
2025-09-03 10:52:45,725 - distributed.worker - INFO -          dashboard at:          10.6.105.11:38225
2025-09-03 10:52:45,725 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,725 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,725 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,725 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,725 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-a4d9cey8
2025-09-03 10:52:45,725 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,728 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:46787
2025-09-03 10:52:45,728 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:46787
2025-09-03 10:52:45,728 - distributed.worker - INFO -          dashboard at:          10.6.105.11:36195
2025-09-03 10:52:45,728 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,728 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,728 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,728 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,728 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-a_n0oxvp
2025-09-03 10:52:45,728 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,794 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:44335
2025-09-03 10:52:45,795 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:44335
2025-09-03 10:52:45,795 - distributed.worker - INFO -          dashboard at:          10.6.105.11:38879
2025-09-03 10:52:45,795 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,795 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,795 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,795 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,795 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-yhrtavts
2025-09-03 10:52:45,795 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,825 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:37965
2025-09-03 10:52:45,826 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:37965
2025-09-03 10:52:45,826 - distributed.worker - INFO -          dashboard at:          10.6.105.11:39511
2025-09-03 10:52:45,826 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,826 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,826 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,826 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,826 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-r4z6cg69
2025-09-03 10:52:45,826 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,827 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:43737
2025-09-03 10:52:45,827 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:43737
2025-09-03 10:52:45,827 - distributed.worker - INFO -          dashboard at:          10.6.105.11:36171
2025-09-03 10:52:45,827 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,827 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,827 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,828 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,828 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-x27pcglk
2025-09-03 10:52:45,828 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,872 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:36793
2025-09-03 10:52:45,872 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:36793
2025-09-03 10:52:45,872 - distributed.worker - INFO -          dashboard at:          10.6.105.11:34693
2025-09-03 10:52:45,872 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,872 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,872 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,872 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,872 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-1xsbvg50
2025-09-03 10:52:45,872 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,876 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:40497
2025-09-03 10:52:45,876 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:40497
2025-09-03 10:52:45,876 - distributed.worker - INFO -          dashboard at:          10.6.105.11:41047
2025-09-03 10:52:45,876 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,876 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,876 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,876 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,876 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-q91ktr_r
2025-09-03 10:52:45,876 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,880 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:45455
2025-09-03 10:52:45,880 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:45455
2025-09-03 10:52:45,881 - distributed.worker - INFO -          dashboard at:          10.6.105.11:41761
2025-09-03 10:52:45,881 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,881 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,881 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,881 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,881 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-0was15o8
2025-09-03 10:52:45,881 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,106 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:44621
2025-09-03 10:52:46,106 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:44621
2025-09-03 10:52:46,106 - distributed.worker - INFO -          dashboard at:          10.6.105.11:37881
2025-09-03 10:52:46,106 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,106 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,106 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,106 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,106 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-s0lamz_y
2025-09-03 10:52:46,106 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,167 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:42665
2025-09-03 10:52:46,167 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:42665
2025-09-03 10:52:46,167 - distributed.worker - INFO -          dashboard at:          10.6.105.11:42079
2025-09-03 10:52:46,167 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,167 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,167 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,167 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,167 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9xqfj_1b
2025-09-03 10:52:46,167 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,175 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:46143
2025-09-03 10:52:46,175 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:46143
2025-09-03 10:52:46,175 - distributed.worker - INFO -          dashboard at:          10.6.105.11:43511
2025-09-03 10:52:46,175 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,175 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,175 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,175 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,175 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-bqxvfhiv
2025-09-03 10:52:46,175 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,198 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:39133
2025-09-03 10:52:46,198 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:39133
2025-09-03 10:52:46,198 - distributed.worker - INFO -          dashboard at:          10.6.105.11:38539
2025-09-03 10:52:46,198 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,198 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,198 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,198 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-5tz20jnq
2025-09-03 10:52:46,199 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,227 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:44465
2025-09-03 10:52:46,227 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:44465
2025-09-03 10:52:46,227 - distributed.worker - INFO -          dashboard at:          10.6.105.11:43997
2025-09-03 10:52:46,227 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,227 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,227 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,227 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,227 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-tsowq9nh
2025-09-03 10:52:46,227 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:33225
2025-09-03 10:52:46,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:33225
2025-09-03 10:52:46,236 - distributed.worker - INFO -          dashboard at:          10.6.105.11:36187
2025-09-03 10:52:46,236 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,237 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,237 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,237 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-1hexxemy
2025-09-03 10:52:46,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,244 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:40565
2025-09-03 10:52:46,244 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:40565
2025-09-03 10:52:46,244 - distributed.worker - INFO -          dashboard at:          10.6.105.11:41971
2025-09-03 10:52:46,244 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,244 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,244 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,244 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,244 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-7otfjke3
2025-09-03 10:52:46,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,254 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:38723
2025-09-03 10:52:46,255 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:38723
2025-09-03 10:52:46,255 - distributed.worker - INFO -          dashboard at:          10.6.105.11:34645
2025-09-03 10:52:46,255 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,255 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,255 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,255 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,255 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-oeogb5lw
2025-09-03 10:52:46,255 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,266 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:44549
2025-09-03 10:52:46,266 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:44549
2025-09-03 10:52:46,266 - distributed.worker - INFO -          dashboard at:          10.6.105.11:33963
2025-09-03 10:52:46,266 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,266 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,266 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,266 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,266 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_o9mcnfu
2025-09-03 10:52:46,266 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,276 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:36111
2025-09-03 10:52:46,276 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:36111
2025-09-03 10:52:46,276 - distributed.worker - INFO -          dashboard at:          10.6.105.11:34435
2025-09-03 10:52:46,276 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,276 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,276 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,276 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,276 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-umt28std
2025-09-03 10:52:46,276 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,290 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:41021
2025-09-03 10:52:46,290 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:41021
2025-09-03 10:52:46,290 - distributed.worker - INFO -          dashboard at:          10.6.105.11:39359
2025-09-03 10:52:46,290 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,290 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,290 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,290 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,290 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-f5zb6sp6
2025-09-03 10:52:46,290 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,296 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:34191
2025-09-03 10:52:46,296 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:34191
2025-09-03 10:52:46,296 - distributed.worker - INFO -          dashboard at:          10.6.105.11:41049
2025-09-03 10:52:46,296 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,296 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,296 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,296 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,296 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-38h0k9j1
2025-09-03 10:52:46,296 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,298 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:42929
2025-09-03 10:52:46,298 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:42929
2025-09-03 10:52:46,298 - distributed.worker - INFO -          dashboard at:          10.6.105.11:41871
2025-09-03 10:52:46,299 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,299 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,299 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,299 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,299 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-npbm9ome
2025-09-03 10:52:46,299 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,400 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:39849
2025-09-03 10:52:46,400 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:39849
2025-09-03 10:52:46,400 - distributed.worker - INFO -          dashboard at:          10.6.105.11:44983
2025-09-03 10:52:46,400 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,400 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,400 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,400 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,400 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-0oivafax
2025-09-03 10:52:46,400 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,402 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:33059
2025-09-03 10:52:46,402 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:33059
2025-09-03 10:52:46,402 - distributed.worker - INFO -          dashboard at:          10.6.105.11:33625
2025-09-03 10:52:46,402 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,402 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,402 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,402 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,402 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-7ojtt71q
2025-09-03 10:52:46,403 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,438 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:37135
2025-09-03 10:52:46,438 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:37135
2025-09-03 10:52:46,438 - distributed.worker - INFO -          dashboard at:          10.6.105.11:41209
2025-09-03 10:52:46,438 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,438 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,438 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,438 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,438 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-u9k1blf6
2025-09-03 10:52:46,438 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,493 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:41443
2025-09-03 10:52:46,493 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:41443
2025-09-03 10:52:46,493 - distributed.worker - INFO -          dashboard at:          10.6.105.11:46735
2025-09-03 10:52:46,493 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,493 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,493 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,493 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,493 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-8g5il0p4
2025-09-03 10:52:46,493 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,499 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:45603
2025-09-03 10:52:46,499 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:45603
2025-09-03 10:52:46,499 - distributed.worker - INFO -          dashboard at:          10.6.105.11:40443
2025-09-03 10:52:46,499 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,499 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,499 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,499 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,499 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-blbn67gy
2025-09-03 10:52:46,500 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,555 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:37971
2025-09-03 10:52:46,555 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:37971
2025-09-03 10:52:46,555 - distributed.worker - INFO -          dashboard at:          10.6.105.11:46111
2025-09-03 10:52:46,555 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,555 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,555 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,555 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,555 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-hny68dfg
2025-09-03 10:52:46,556 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,567 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:36891
2025-09-03 10:52:46,567 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:36891
2025-09-03 10:52:46,567 - distributed.worker - INFO -          dashboard at:          10.6.105.11:34003
2025-09-03 10:52:46,567 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,567 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,567 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,567 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,567 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-6h1m03p0
2025-09-03 10:52:46,567 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,569 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:42065
2025-09-03 10:52:46,569 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:42065
2025-09-03 10:52:46,569 - distributed.worker - INFO -          dashboard at:          10.6.105.11:45539
2025-09-03 10:52:46,569 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,569 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,569 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,569 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,569 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-r1xy5bei
2025-09-03 10:52:46,569 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,574 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:43669
2025-09-03 10:52:46,574 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:43669
2025-09-03 10:52:46,574 - distributed.worker - INFO -          dashboard at:          10.6.105.11:34845
2025-09-03 10:52:46,574 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,574 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,574 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,574 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,574 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-us1xnkji
2025-09-03 10:52:46,574 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,597 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:39481
2025-09-03 10:52:46,597 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:39481
2025-09-03 10:52:46,597 - distributed.worker - INFO -          dashboard at:          10.6.105.11:43889
2025-09-03 10:52:46,597 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,597 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,597 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,597 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,597 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-iln37f4i
2025-09-03 10:52:46,597 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,624 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:39321
2025-09-03 10:52:46,624 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:39321
2025-09-03 10:52:46,624 - distributed.worker - INFO -          dashboard at:          10.6.105.11:44021
2025-09-03 10:52:46,624 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,624 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,624 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,624 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,624 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-zo8mqhw6
2025-09-03 10:52:46,624 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,631 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:40327
2025-09-03 10:52:46,631 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:40327
2025-09-03 10:52:46,631 - distributed.worker - INFO -          dashboard at:          10.6.105.11:42247
2025-09-03 10:52:46,631 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,631 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,631 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,631 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,632 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-nehghsvn
2025-09-03 10:52:46,632 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,650 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:34371
2025-09-03 10:52:46,650 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:34371
2025-09-03 10:52:46,650 - distributed.worker - INFO -          dashboard at:          10.6.105.11:46385
2025-09-03 10:52:46,650 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,650 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,650 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,650 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,650 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-mpfyrvec
2025-09-03 10:52:46,650 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,666 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:36305
2025-09-03 10:52:46,666 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:36305
2025-09-03 10:52:46,666 - distributed.worker - INFO -          dashboard at:          10.6.105.11:34495
2025-09-03 10:52:46,666 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,666 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,666 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,666 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,666 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9d2doien
2025-09-03 10:52:46,666 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,675 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:42049
2025-09-03 10:52:46,675 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:42049
2025-09-03 10:52:46,675 - distributed.worker - INFO -          dashboard at:          10.6.105.11:42621
2025-09-03 10:52:46,675 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,675 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,675 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,675 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,675 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-hb4ce_0j
2025-09-03 10:52:46,675 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,848 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:45867
2025-09-03 10:52:46,848 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:45867
2025-09-03 10:52:46,848 - distributed.worker - INFO -          dashboard at:          10.6.105.11:36121
2025-09-03 10:52:46,848 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,848 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,848 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,848 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,848 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-abpa93l1
2025-09-03 10:52:46,848 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,851 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:36805
2025-09-03 10:52:46,851 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:36805
2025-09-03 10:52:46,851 - distributed.worker - INFO -          dashboard at:          10.6.105.11:41599
2025-09-03 10:52:46,851 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,851 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,852 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,852 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,852 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-6nh8zh0i
2025-09-03 10:52:46,852 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,865 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:35377
2025-09-03 10:52:46,865 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:44315
2025-09-03 10:52:46,866 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:35377
2025-09-03 10:52:46,866 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:44315
2025-09-03 10:52:46,866 - distributed.worker - INFO -          dashboard at:          10.6.105.11:42273
2025-09-03 10:52:46,866 - distributed.worker - INFO -          dashboard at:          10.6.105.11:38137
2025-09-03 10:52:46,866 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,866 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,866 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,866 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,866 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,866 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,866 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,866 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,866 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-qvf4ajvk
2025-09-03 10:52:46,866 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-to1efaqd
2025-09-03 10:52:46,866 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,866 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,872 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:40689
2025-09-03 10:52:46,872 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:40689
2025-09-03 10:52:46,872 - distributed.worker - INFO -          dashboard at:          10.6.105.11:42163
2025-09-03 10:52:46,872 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,872 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:45945
2025-09-03 10:52:46,872 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,872 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,872 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:45945
2025-09-03 10:52:46,872 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,872 - distributed.worker - INFO -          dashboard at:          10.6.105.11:44587
2025-09-03 10:52:46,872 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9hua8hwq
2025-09-03 10:52:46,872 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,872 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,872 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,872 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,872 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,872 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-w_uoi046
2025-09-03 10:52:46,872 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,882 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:37923
2025-09-03 10:52:46,882 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:37923
2025-09-03 10:52:46,882 - distributed.worker - INFO -          dashboard at:          10.6.105.11:33193
2025-09-03 10:52:46,882 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,882 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,882 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,882 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,882 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_yxkx8mc
2025-09-03 10:52:46,882 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,884 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:43729
2025-09-03 10:52:46,884 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:43729
2025-09-03 10:52:46,884 - distributed.worker - INFO -          dashboard at:          10.6.105.11:37187
2025-09-03 10:52:46,884 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,884 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,884 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,884 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,884 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-wf_wmp_k
2025-09-03 10:52:46,884 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,884 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:33405
2025-09-03 10:52:46,884 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:33405
2025-09-03 10:52:46,884 - distributed.worker - INFO -          dashboard at:          10.6.105.11:45477
2025-09-03 10:52:46,884 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,884 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,884 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,884 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,884 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-t8px66bp
2025-09-03 10:52:46,884 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,028 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:36439
2025-09-03 10:52:47,028 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:36439
2025-09-03 10:52:47,028 - distributed.worker - INFO -          dashboard at:          10.6.105.11:43247
2025-09-03 10:52:47,028 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,028 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,028 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:47,028 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:47,028 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-m2zfdqzu
2025-09-03 10:52:47,028 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,030 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:40805
2025-09-03 10:52:47,030 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:40805
2025-09-03 10:52:47,030 - distributed.worker - INFO -          dashboard at:          10.6.105.11:39657
2025-09-03 10:52:47,030 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,030 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,030 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:47,030 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:47,030 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-x13llt3l
2025-09-03 10:52:47,030 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,037 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:43909
2025-09-03 10:52:47,037 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:43909
2025-09-03 10:52:47,037 - distributed.worker - INFO -          dashboard at:          10.6.105.11:34853
2025-09-03 10:52:47,037 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,037 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,037 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:47,037 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:47,037 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-5ae6bhp2
2025-09-03 10:52:47,037 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,039 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:38001
2025-09-03 10:52:47,039 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:38001
2025-09-03 10:52:47,039 - distributed.worker - INFO -          dashboard at:          10.6.105.11:35919
2025-09-03 10:52:47,039 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,039 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,039 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:47,039 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:47,039 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-5b8s183y
2025-09-03 10:52:47,039 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,041 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:38079
2025-09-03 10:52:47,041 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:38079
2025-09-03 10:52:47,041 - distributed.worker - INFO -          dashboard at:          10.6.105.11:41667
2025-09-03 10:52:47,041 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,041 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,042 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:47,042 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:47,042 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-b4fxjog1
2025-09-03 10:52:47,042 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,046 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:34403
2025-09-03 10:52:47,046 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:34403
2025-09-03 10:52:47,046 - distributed.worker - INFO -          dashboard at:          10.6.105.11:37931
2025-09-03 10:52:47,046 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,047 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:47,047 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:47,047 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-2f44dejl
2025-09-03 10:52:47,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,049 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:38599
2025-09-03 10:52:47,049 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:38599
2025-09-03 10:52:47,049 - distributed.worker - INFO -          dashboard at:          10.6.105.11:41869
2025-09-03 10:52:47,049 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,049 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,049 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:47,049 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:47,049 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-qoru7xyt
2025-09-03 10:52:47,049 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,050 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:35029
2025-09-03 10:52:47,050 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:35029
2025-09-03 10:52:47,051 - distributed.worker - INFO -          dashboard at:          10.6.105.11:46363
2025-09-03 10:52:47,051 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,051 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,051 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:47,051 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:47,051 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-go899sov
2025-09-03 10:52:47,051 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,055 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:42203
2025-09-03 10:52:47,055 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:42203
2025-09-03 10:52:47,055 - distributed.worker - INFO -          dashboard at:          10.6.105.11:35213
2025-09-03 10:52:47,055 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,055 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,055 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:47,055 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:47,055 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-mt8bbc2o
2025-09-03 10:52:47,055 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,056 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:38073
2025-09-03 10:52:47,056 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:38073
2025-09-03 10:52:47,056 - distributed.worker - INFO -          dashboard at:          10.6.105.11:36533
2025-09-03 10:52:47,056 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:43797
2025-09-03 10:52:47,056 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,056 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,056 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:43797
2025-09-03 10:52:47,056 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:47,056 - distributed.worker - INFO -          dashboard at:          10.6.105.11:34633
2025-09-03 10:52:47,056 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:47,056 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,056 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-e4ge3oq1
2025-09-03 10:52:47,056 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,056 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:47,056 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,056 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:47,056 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-1b_uryn2
2025-09-03 10:52:47,056 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,057 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:38111
2025-09-03 10:52:47,057 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:38111
2025-09-03 10:52:47,057 - distributed.worker - INFO -          dashboard at:          10.6.105.11:36181
2025-09-03 10:52:47,057 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,057 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,057 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:47,057 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:47,057 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-eabhpr2t
2025-09-03 10:52:47,058 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,058 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:44509
2025-09-03 10:52:47,058 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:44509
2025-09-03 10:52:47,058 - distributed.worker - INFO -          dashboard at:          10.6.105.11:36339
2025-09-03 10:52:47,058 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,058 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,058 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:47,058 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:47,058 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-isfku305
2025-09-03 10:52:47,058 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,073 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:36069
2025-09-03 10:52:47,073 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:36069
2025-09-03 10:52:47,074 - distributed.worker - INFO -          dashboard at:          10.6.105.11:44521
2025-09-03 10:52:47,074 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,074 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,074 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:47,074 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:47,074 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-rx5m05ek
2025-09-03 10:52:47,074 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,314 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:33029'
2025-09-03 10:52:47,317 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:39689'
2025-09-03 10:52:47,339 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:33031'
2025-09-03 10:52:47,343 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:36619'
2025-09-03 10:52:47,347 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:40249'
2025-09-03 10:52:47,354 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:41607'
2025-09-03 10:52:47,358 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:42109'
2025-09-03 10:52:47,362 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:44693'
2025-09-03 10:52:47,366 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:39305'
2025-09-03 10:52:47,371 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:46457'
2025-09-03 10:52:47,377 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:46109'
2025-09-03 10:52:47,383 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:33103'
2025-09-03 10:52:47,781 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:47,782 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,782 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,784 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:47,950 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:47,952 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,952 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,954 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:47,966 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:47,968 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,968 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,969 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:47,982 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:47,983 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,983 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,984 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:48,151 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:42725
2025-09-03 10:52:48,151 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:42725
2025-09-03 10:52:48,151 - distributed.worker - INFO -          dashboard at:          10.6.105.11:41545
2025-09-03 10:52:48,151 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,151 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,151 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:48,151 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:48,151 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-m32hitvq
2025-09-03 10:52:48,151 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,156 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:33381
2025-09-03 10:52:48,156 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:33381
2025-09-03 10:52:48,156 - distributed.worker - INFO -          dashboard at:          10.6.105.11:42569
2025-09-03 10:52:48,156 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,156 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,156 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:48,156 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:48,156 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-7nmq1w5x
2025-09-03 10:52:48,156 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,159 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:35199
2025-09-03 10:52:48,159 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:35199
2025-09-03 10:52:48,159 - distributed.worker - INFO -          dashboard at:          10.6.105.11:41103
2025-09-03 10:52:48,159 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,159 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,159 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:48,159 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:48,159 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-c27kucuz
2025-09-03 10:52:48,159 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,173 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:45697
2025-09-03 10:52:48,174 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:45697
2025-09-03 10:52:48,174 - distributed.worker - INFO -          dashboard at:          10.6.105.11:43401
2025-09-03 10:52:48,174 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,174 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,174 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:48,174 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:48,174 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_wf9tqt1
2025-09-03 10:52:48,174 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,185 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:43231
2025-09-03 10:52:48,185 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:43231
2025-09-03 10:52:48,185 - distributed.worker - INFO -          dashboard at:          10.6.105.11:38381
2025-09-03 10:52:48,185 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,185 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,185 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:48,185 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:48,185 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-b1sk6ov3
2025-09-03 10:52:48,185 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,190 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:37987
2025-09-03 10:52:48,190 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:37987
2025-09-03 10:52:48,190 - distributed.worker - INFO -          dashboard at:          10.6.105.11:37267
2025-09-03 10:52:48,190 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,190 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,190 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:48,190 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:48,190 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-235hb9bd
2025-09-03 10:52:48,190 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,193 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:38945
2025-09-03 10:52:48,193 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:38945
2025-09-03 10:52:48,193 - distributed.worker - INFO -          dashboard at:          10.6.105.11:41097
2025-09-03 10:52:48,193 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,193 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,193 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:48,193 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:48,193 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-6k7esall
2025-09-03 10:52:48,193 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,199 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:44171
2025-09-03 10:52:48,199 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:44171
2025-09-03 10:52:48,199 - distributed.worker - INFO -          dashboard at:          10.6.105.11:36083
2025-09-03 10:52:48,199 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,199 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,199 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:48,199 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:48,199 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_65djugs
2025-09-03 10:52:48,199 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,200 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:33779
2025-09-03 10:52:48,200 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:33779
2025-09-03 10:52:48,200 - distributed.worker - INFO -          dashboard at:          10.6.105.11:41849
2025-09-03 10:52:48,200 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,200 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,200 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:48,201 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:48,201 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-6qvs84ig
2025-09-03 10:52:48,201 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,202 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:48,203 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,203 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,205 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:34073
2025-09-03 10:52:48,205 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:48,205 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:41735
2025-09-03 10:52:48,205 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:34073
2025-09-03 10:52:48,205 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:41735
2025-09-03 10:52:48,205 - distributed.worker - INFO -          dashboard at:          10.6.105.11:37287
2025-09-03 10:52:48,205 - distributed.worker - INFO -          dashboard at:          10.6.105.11:40649
2025-09-03 10:52:48,205 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,205 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,205 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,205 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,205 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:34663
2025-09-03 10:52:48,205 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:48,205 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:48,205 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:48,205 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:34663
2025-09-03 10:52:48,205 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:48,205 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-xytgkt8o
2025-09-03 10:52:48,205 - distributed.worker - INFO -          dashboard at:          10.6.105.11:41213
2025-09-03 10:52:48,205 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ozjxsdug
2025-09-03 10:52:48,205 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,205 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,205 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,205 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,205 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:48,205 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:48,205 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-3i3tw6yz
2025-09-03 10:52:48,205 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,218 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:48,219 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,219 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,221 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:48,233 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:48,235 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,235 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,236 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:48,248 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:48,249 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,250 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,251 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:48,263 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:48,265 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,265 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,266 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:48,279 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:48,280 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,280 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,281 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:48,831 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:48,832 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,832 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,834 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:48,847 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:48,848 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,848 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,849 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,108 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,109 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,110 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,111 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,124 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,125 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,126 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,127 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,139 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,140 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,140 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,142 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,154 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,156 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,156 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,158 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,169 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,171 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,171 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,172 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,184 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:44039'
2025-09-03 10:52:49,189 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:38151'
2025-09-03 10:52:49,192 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:39167'
2025-09-03 10:52:49,198 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:43933'
2025-09-03 10:52:49,203 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:38713'
2025-09-03 10:52:49,209 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:42411'
2025-09-03 10:52:49,216 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:34775'
2025-09-03 10:52:49,222 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:37523'
2025-09-03 10:52:49,228 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:40667'
2025-09-03 10:52:49,495 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,496 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,496 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,498 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,510 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,511 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,512 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,513 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,526 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,527 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,528 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,541 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,542 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,542 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,544 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,556 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,558 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,558 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,559 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,572 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,573 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,573 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,575 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,587 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,588 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,589 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,590 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,603 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,604 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,604 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,606 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,618 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,620 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,620 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,621 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,718 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,719 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,720 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,721 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,734 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,735 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,736 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,737 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,750 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,751 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,751 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,752 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,765 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,767 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,767 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,768 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,781 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,782 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,782 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,784 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,796 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,798 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,798 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,799 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,812 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,813 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,813 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,815 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,828 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,829 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,829 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,831 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,843 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,844 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,844 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,846 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,859 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,860 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,860 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,862 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,875 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,876 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,876 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,877 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,891 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,892 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,892 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,893 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,906 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,907 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,907 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,909 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,922 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,923 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,923 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,925 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,937 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,938 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,939 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,940 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,953 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,954 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,954 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,956 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,968 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,969 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,970 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,971 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,982 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:36941
2025-09-03 10:52:49,982 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:36941
2025-09-03 10:52:49,982 - distributed.worker - INFO -          dashboard at:          10.6.105.11:45979
2025-09-03 10:52:49,982 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,982 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,982 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:49,982 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:49,982 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-0hlhr9cv
2025-09-03 10:52:49,982 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,984 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,985 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,985 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,987 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,995 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:41157
2025-09-03 10:52:49,995 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:41157
2025-09-03 10:52:49,995 - distributed.worker - INFO -          dashboard at:          10.6.105.11:38143
2025-09-03 10:52:49,995 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,995 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,995 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:49,995 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:49,995 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-o6kicx0r
2025-09-03 10:52:49,995 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,996 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:34881
2025-09-03 10:52:49,996 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:34881
2025-09-03 10:52:49,996 - distributed.worker - INFO -          dashboard at:          10.6.105.11:46531
2025-09-03 10:52:49,996 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,996 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,996 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:49,996 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:49,996 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-wf_p7s2x
2025-09-03 10:52:49,997 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,000 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:37883
2025-09-03 10:52:50,000 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:37883
2025-09-03 10:52:50,000 - distributed.worker - INFO -          dashboard at:          10.6.105.11:46361
2025-09-03 10:52:50,000 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,000 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,001 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:50,001 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:50,001 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-2n3nsep2
2025-09-03 10:52:50,001 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,030 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:36105
2025-09-03 10:52:50,030 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:36105
2025-09-03 10:52:50,030 - distributed.worker - INFO -          dashboard at:          10.6.105.11:44409
2025-09-03 10:52:50,030 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,031 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,031 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:50,031 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:50,031 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-0keg74hk
2025-09-03 10:52:50,031 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,032 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:46439
2025-09-03 10:52:50,032 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:46439
2025-09-03 10:52:50,032 - distributed.worker - INFO -          dashboard at:          10.6.105.11:45299
2025-09-03 10:52:50,032 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,032 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,032 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:50,032 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:50,032 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-51393ihx
2025-09-03 10:52:50,032 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,033 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:41955
2025-09-03 10:52:50,033 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:41955
2025-09-03 10:52:50,033 - distributed.worker - INFO -          dashboard at:          10.6.105.11:36491
2025-09-03 10:52:50,033 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,033 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,033 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:50,033 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:50,033 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-rp450mqk
2025-09-03 10:52:50,033 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,043 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:33699
2025-09-03 10:52:50,043 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:33699
2025-09-03 10:52:50,043 - distributed.worker - INFO -          dashboard at:          10.6.105.11:36881
2025-09-03 10:52:50,043 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,043 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,043 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:50,043 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:50,043 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-zpylua9f
2025-09-03 10:52:50,043 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,048 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,049 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,050 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,051 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,061 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:44061
2025-09-03 10:52:50,061 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:44061
2025-09-03 10:52:50,061 - distributed.worker - INFO -          dashboard at:          10.6.105.11:34375
2025-09-03 10:52:50,061 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,061 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,061 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:50,061 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:50,061 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-usm0y7io
2025-09-03 10:52:50,061 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,319 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,321 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,321 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,322 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,336 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,337 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,337 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,339 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,368 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,369 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,370 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,372 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,383 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,385 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,385 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,387 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,399 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,400 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,400 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,402 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,414 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,416 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,416 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,418 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,430 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,432 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,432 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,434 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,495 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,496 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,496 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,498 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,513 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,517 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,517 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,523 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,526 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,528 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,528 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,530 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,542 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,543 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,543 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,545 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,558 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,560 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,560 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,562 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,575 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,576 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,576 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,578 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,591 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,592 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,592 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,594 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:51,713 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:40975'
2025-09-03 10:52:51,717 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:39623'
2025-09-03 10:52:51,720 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:36117'
2025-09-03 10:52:51,725 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:32793'
2025-09-03 10:52:51,730 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:41983'
2025-09-03 10:52:51,736 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.11:36495'
2025-09-03 10:52:51,860 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:51,861 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,861 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,862 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:52,510 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:46195
2025-09-03 10:52:52,510 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:46195
2025-09-03 10:52:52,510 - distributed.worker - INFO -          dashboard at:          10.6.105.11:34821
2025-09-03 10:52:52,510 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,511 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,511 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,511 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,511 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-91sef71a
2025-09-03 10:52:52,511 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,518 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:44735
2025-09-03 10:52:52,518 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:44735
2025-09-03 10:52:52,518 - distributed.worker - INFO -          dashboard at:          10.6.105.11:45659
2025-09-03 10:52:52,518 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,518 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,518 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,518 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,518 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ofxmc0dy
2025-09-03 10:52:52,518 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,523 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:36183
2025-09-03 10:52:52,524 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:36183
2025-09-03 10:52:52,524 - distributed.worker - INFO -          dashboard at:          10.6.105.11:46813
2025-09-03 10:52:52,524 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,524 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,524 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,524 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-jznqt3xk
2025-09-03 10:52:52,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,527 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:42401
2025-09-03 10:52:52,527 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:42401
2025-09-03 10:52:52,527 - distributed.worker - INFO -          dashboard at:          10.6.105.11:36799
2025-09-03 10:52:52,527 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,527 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,527 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,527 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-cut6lvvu
2025-09-03 10:52:52,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,527 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:35381
2025-09-03 10:52:52,527 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:35381
2025-09-03 10:52:52,527 - distributed.worker - INFO -          dashboard at:          10.6.105.11:43605
2025-09-03 10:52:52,527 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,527 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,527 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,527 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-i6dzfv3x
2025-09-03 10:52:52,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,555 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.11:35097
2025-09-03 10:52:52,556 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.11:35097
2025-09-03 10:52:52,556 - distributed.worker - INFO -          dashboard at:          10.6.105.11:45443
2025-09-03 10:52:52,556 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,556 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,556 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,556 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,556 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-i_svm91q
2025-09-03 10:52:52,556 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,747 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:52,749 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,749 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,750 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,097 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,098 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,098 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,100 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,113 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,114 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,115 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,116 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,130 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,131 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,131 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,133 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,383 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,384 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,385 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,386 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,400 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,402 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,402 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,404 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,416 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,418 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,418 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,420 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,433 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,434 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,434 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,436 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,449 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,450 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,451 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,452 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,465 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,466 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,467 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,469 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,481 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,483 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,483 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,485 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,497 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,498 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,499 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,501 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,513 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,514 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,514 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,516 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,902 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,904 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,904 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,905 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,918 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,919 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,919 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,921 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:02,691 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:02,692 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:02,692 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:02,694 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:02,707 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:02,709 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:02,709 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:02,711 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:02,724 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:02,725 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:02,725 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:02,727 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:02,741 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:02,742 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:02,742 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:02,744 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:02,757 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:02,758 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:02,758 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:02,760 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:02,774 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:02,776 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:02,776 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:02,777 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:15,529 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:15,568 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:15,632 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:15,645 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:15,680 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:15,696 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:15,700 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:15,726 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:15,729 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:15,874 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:15,878 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:15,882 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:17,051 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:17,052 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:19,224 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:19,225 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:19,226 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:19,227 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:19,662 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:19,664 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:19,664 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:19,666 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:19,983 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:19,996 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:19,997 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,001 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,032 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,034 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,034 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,044 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,063 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:20,850 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:20,865 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:20,881 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:21,501 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:21,523 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:21,532 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:21,548 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:21,565 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:21,580 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:21,596 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:22,190 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:22,192 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:22,194 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:22,226 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:22,228 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,228 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:22,230 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:22,298 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:22,300 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,300 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:22,302 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:22,316 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:22,317 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,318 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:22,319 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:22,868 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:23,427 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,428 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,428 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,429 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,499 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,500 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,500 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,502 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,679 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,680 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,680 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,682 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,751 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,752 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,752 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,754 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,769 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,770 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,770 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,772 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:23,911 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:23,912 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,912 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:23,914 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:24,000 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:24,002 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:24,002 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:24,004 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:24,762 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:24,763 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:24,763 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:24,765 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:24,780 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:24,782 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:24,782 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:24,784 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:25,101 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:25,103 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,103 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:25,104 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:25,390 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:25,408 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:25,423 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:25,438 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:25,457 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:25,471 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:25,487 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:25,504 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:25,517 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:25,909 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:25,925 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:40,622 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:40,623 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:40,623 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:40,625 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:40,640 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:40,641 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:40,641 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:40,643 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:50,134 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:50,135 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:50,136 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:50,145 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:50,164 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:50,318 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:50,320 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:50,320 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:50,322 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:50,349 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:50,350 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:50,351 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:50,352 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:50,374 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:50,375 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:50,375 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:50,377 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:50,397 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:50,399 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:50,399 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:50,401 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:50,419 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:50,420 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:50,420 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:50,422 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:56:38,091 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,097 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,160 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,171 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,446 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,448 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,768 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,774 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,592 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,594 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,648 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,650 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,681 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,686 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,921 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,923 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,924 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,930 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,932 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,937 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,960 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,965 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,983 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,988 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,168 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,175 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,175 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,181 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,440 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,442 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,448 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,449 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,537 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,539 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,664 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,670 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,753 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,754 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,086 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,087 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,250 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,253 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,346 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,348 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,444 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,446 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,498 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,503 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,979 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,986 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,030 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,036 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,050 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,052 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,196 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,203 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,239 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,240 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,304 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,307 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,830 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,839 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,148 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,150 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,494 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,497 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,709 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,713 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,872 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,873 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,301 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,304 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,315 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,316 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,510 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,512 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,512 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,517 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,572 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,575 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,656 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,661 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,696 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,701 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,934 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,936 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,951 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,956 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,199 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,205 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,384 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,386 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,475 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,476 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,023 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,029 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,029 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,030 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,458 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,459 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,758 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,764 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,784 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,790 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,094 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,099 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,416 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,418 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,964 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,970 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,007 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,015 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,028 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,033 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,850 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,856 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,213 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,218 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,288 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,293 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,320 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,326 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,360 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,362 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,989 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,991 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,059 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,065 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,357 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,362 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,487 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,489 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,895 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,901 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,055 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,057 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,081 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,087 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,118 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,120 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,350 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,351 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,622 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,624 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,793 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,798 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,854 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,859 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,111 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,112 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,128 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,129 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,200 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,205 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,206 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,208 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,313 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,318 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,623 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,625 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,958 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,963 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,074 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,080 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,216 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,218 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,975 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,980 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,058 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,060 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,313 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,314 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,388 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,390 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,458 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,464 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,478 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,487 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,567 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,574 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,816 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,821 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:56,131 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:56,133 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:56,418 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:56,420 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:57,114 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:57,116 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:57,250 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:57,256 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:57,510 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:57,516 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:59,190 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:59,191 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:59,805 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:59,807 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:00,525 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:00,527 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:01,479 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:01,485 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:02,272 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:02,273 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:03,865 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:03,868 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:04,158 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:04,165 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:04,901 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:04,906 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:09,989 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:09,990 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:09,991 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:09,992 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:09,995 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:09,995 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:09,997 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:09,998 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:09,999 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,000 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,005 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,006 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,007 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,007 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,008 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,009 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,014 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,016 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,017 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,019 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,028 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,033 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,328 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,330 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,337 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,339 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,338 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,340 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,345 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,350 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,361 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,363 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,367 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,371 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,386 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,388 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,496 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,498 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,499 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,504 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,517 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,519 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,529 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,531 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,531 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,533 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,543 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,545 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,626 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,628 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,628 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,630 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,636 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,638 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,639 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,642 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,644 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,641 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,660 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,662 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,737 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,742 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,756 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,758 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,886 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,888 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,893 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,895 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,912 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,914 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,915 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,914 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,915 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,916 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,917 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,918 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,930 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,932 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,932 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,934 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,934 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,936 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,103 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,105 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,107 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,109 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,111 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,116 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,166 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,168 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,262 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,264 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,276 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,278 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,278 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,280 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,281 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,281 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,282 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,283 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,295 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,296 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,297 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,298 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,312 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,314 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,375 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,377 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,378 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,380 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,408 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,410 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,463 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,465 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,485 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,487 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,492 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,493 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,504 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,509 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,539 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,545 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,563 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,565 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,580 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,583 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,597 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,599 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,618 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,620 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,643 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,645 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,679 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,681 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,691 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,693 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,750 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,756 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,762 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,764 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,977 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,979 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,991 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,993 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,059 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,061 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,111 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,113 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,327 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,329 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,362 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,367 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,431 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,433 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,476 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,478 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,498 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,500 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,512 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,514 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,521 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,522 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,553 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,555 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,565 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,568 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,612 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,614 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,622 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,624 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,623 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,626 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,673 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,675 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,757 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,759 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,763 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,765 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,841 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,846 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,896 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,898 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,990 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,992 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,068 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,070 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,072 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,074 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,238 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,240 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,375 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,377 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,389 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,397 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,408 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,413 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,526 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,529 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:14,060 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:14,064 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:14,091 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:14,092 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:14,157 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:14,159 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:14,203 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:14,205 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:14,475 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:14,477 - distributed.utils - INFO - Reload module qme_vars from .py file
