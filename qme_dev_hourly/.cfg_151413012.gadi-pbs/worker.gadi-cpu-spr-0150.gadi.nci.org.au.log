Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-10-02 09:24:20,292 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:37491'
2025-10-02 09:24:20,301 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:32787'
2025-10-02 09:24:20,306 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:36179'
2025-10-02 09:24:20,310 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:35315'
2025-10-02 09:24:20,314 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:44105'
2025-10-02 09:24:20,318 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:36787'
2025-10-02 09:24:20,323 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:39417'
2025-10-02 09:24:20,326 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:44411'
2025-10-02 09:24:20,330 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:41491'
2025-10-02 09:24:20,335 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:39583'
2025-10-02 09:24:20,339 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:45819'
2025-10-02 09:24:20,343 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:35747'
2025-10-02 09:24:20,347 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:40353'
2025-10-02 09:24:20,351 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:44065'
2025-10-02 09:24:20,355 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:42249'
2025-10-02 09:24:20,359 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:41963'
2025-10-02 09:24:20,363 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:40873'
2025-10-02 09:24:20,367 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:33237'
2025-10-02 09:24:20,371 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:44115'
2025-10-02 09:24:20,483 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:45489'
2025-10-02 09:24:20,487 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:44611'
2025-10-02 09:24:20,492 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:41171'
2025-10-02 09:24:20,495 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:33553'
2025-10-02 09:24:20,498 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:41693'
2025-10-02 09:24:20,503 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:34071'
2025-10-02 09:24:20,508 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:46483'
2025-10-02 09:24:20,512 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:37407'
2025-10-02 09:24:20,516 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:38343'
2025-10-02 09:24:20,519 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:45113'
2025-10-02 09:24:20,524 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:46345'
2025-10-02 09:24:20,529 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:44741'
2025-10-02 09:24:20,532 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:33301'
2025-10-02 09:24:20,537 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:45139'
2025-10-02 09:24:20,541 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:37791'
2025-10-02 09:24:20,547 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:46773'
2025-10-02 09:24:20,551 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:41205'
2025-10-02 09:24:20,556 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:40955'
2025-10-02 09:24:20,560 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:42575'
2025-10-02 09:24:20,564 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:32901'
2025-10-02 09:24:20,569 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:42095'
2025-10-02 09:24:20,573 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:44417'
2025-10-02 09:24:20,577 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:41101'
2025-10-02 09:24:20,581 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:45047'
2025-10-02 09:24:20,587 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:39397'
2025-10-02 09:24:20,592 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:35989'
2025-10-02 09:24:20,596 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:40351'
2025-10-02 09:24:20,601 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:37389'
2025-10-02 09:24:20,604 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:38647'
2025-10-02 09:24:20,609 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:35193'
2025-10-02 09:24:20,614 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:33643'
2025-10-02 09:24:20,618 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:39533'
2025-10-02 09:24:20,624 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.6:46795'
2025-10-02 09:24:21,514 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:36023
2025-10-02 09:24:21,514 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:43059
2025-10-02 09:24:21,514 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:38417
2025-10-02 09:24:21,514 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:44793
2025-10-02 09:24:21,514 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:44045
2025-10-02 09:24:21,514 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:41965
2025-10-02 09:24:21,514 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:36023
2025-10-02 09:24:21,514 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:43059
2025-10-02 09:24:21,514 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:38417
2025-10-02 09:24:21,514 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:44793
2025-10-02 09:24:21,514 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:44045
2025-10-02 09:24:21,514 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:41965
2025-10-02 09:24:21,514 - distributed.worker - INFO -          dashboard at:            10.6.83.6:46751
2025-10-02 09:24:21,514 - distributed.worker - INFO -          dashboard at:            10.6.83.6:46823
2025-10-02 09:24:21,514 - distributed.worker - INFO -          dashboard at:            10.6.83.6:36581
2025-10-02 09:24:21,514 - distributed.worker - INFO -          dashboard at:            10.6.83.6:33747
2025-10-02 09:24:21,514 - distributed.worker - INFO -          dashboard at:            10.6.83.6:37033
2025-10-02 09:24:21,514 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,514 - distributed.worker - INFO -          dashboard at:            10.6.83.6:43253
2025-10-02 09:24:21,514 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,514 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,514 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,514 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,514 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,514 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,514 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,514 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,514 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,514 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,514 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,514 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,514 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,514 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,514 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,514 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,514 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,514 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,514 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,515 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,515 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,515 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,515 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-2nkupguu
2025-10-02 09:24:21,515 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,515 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-fgwf1fwy
2025-10-02 09:24:21,515 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ttdcx3no
2025-10-02 09:24:21,515 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-97ckwbql
2025-10-02 09:24:21,515 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-up0yukfe
2025-10-02 09:24:21,515 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,515 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ld92_wfp
2025-10-02 09:24:21,515 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,515 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,515 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,515 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,515 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,526 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:46089
2025-10-02 09:24:21,526 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:46089
2025-10-02 09:24:21,526 - distributed.worker - INFO -          dashboard at:            10.6.83.6:44391
2025-10-02 09:24:21,526 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,526 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,526 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,526 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,526 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-g61q3mmk
2025-10-02 09:24:21,526 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,535 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,535 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,536 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,536 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,538 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,539 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,539 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,541 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,541 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,542 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,542 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,543 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,543 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,543 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,544 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,545 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,545 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,545 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,546 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,547 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,547 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,548 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,548 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,550 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,550 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,551 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,551 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,552 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,587 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:41367
2025-10-02 09:24:21,587 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:41367
2025-10-02 09:24:21,587 - distributed.worker - INFO -          dashboard at:            10.6.83.6:36387
2025-10-02 09:24:21,587 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,587 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,587 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,587 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,587 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-mxvzcs2j
2025-10-02 09:24:21,587 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,589 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:33257
2025-10-02 09:24:21,589 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:33257
2025-10-02 09:24:21,589 - distributed.worker - INFO -          dashboard at:            10.6.83.6:44569
2025-10-02 09:24:21,589 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,589 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,589 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:33947
2025-10-02 09:24:21,589 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,590 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,590 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:33947
2025-10-02 09:24:21,590 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-c1fuwf75
2025-10-02 09:24:21,590 - distributed.worker - INFO -          dashboard at:            10.6.83.6:36613
2025-10-02 09:24:21,590 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,590 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,590 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,590 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,590 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,590 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-zqsl78ip
2025-10-02 09:24:21,590 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,600 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:44189
2025-10-02 09:24:21,601 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:44189
2025-10-02 09:24:21,601 - distributed.worker - INFO -          dashboard at:            10.6.83.6:33061
2025-10-02 09:24:21,601 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,601 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,601 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,601 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,601 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-a_12qctu
2025-10-02 09:24:21,601 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:33545
2025-10-02 09:24:21,601 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,601 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:33545
2025-10-02 09:24:21,601 - distributed.worker - INFO -          dashboard at:            10.6.83.6:39973
2025-10-02 09:24:21,601 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,601 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,601 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,601 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,601 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-0ua2msmk
2025-10-02 09:24:21,601 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,604 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:46821
2025-10-02 09:24:21,604 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:46821
2025-10-02 09:24:21,604 - distributed.worker - INFO -          dashboard at:            10.6.83.6:46811
2025-10-02 09:24:21,604 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,604 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,604 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,604 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,604 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-waw5ocwh
2025-10-02 09:24:21,604 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,610 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:37557
2025-10-02 09:24:21,610 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:41819
2025-10-02 09:24:21,610 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:37557
2025-10-02 09:24:21,610 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,610 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:41819
2025-10-02 09:24:21,610 - distributed.worker - INFO -          dashboard at:            10.6.83.6:35441
2025-10-02 09:24:21,610 - distributed.worker - INFO -          dashboard at:            10.6.83.6:45761
2025-10-02 09:24:21,610 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,610 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,610 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,610 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,610 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,610 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,610 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,610 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,610 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ji9ery22
2025-10-02 09:24:21,610 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-c2dgow9_
2025-10-02 09:24:21,610 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,610 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,611 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,611 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,612 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,612 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,613 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,613 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,613 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:42443
2025-10-02 09:24:21,613 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:42443
2025-10-02 09:24:21,613 - distributed.worker - INFO -          dashboard at:            10.6.83.6:38965
2025-10-02 09:24:21,613 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,613 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,613 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,614 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,614 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-35wvcdkj
2025-10-02 09:24:21,614 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,614 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,615 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,615 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,615 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,617 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,622 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,622 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,623 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,624 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,624 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,625 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,625 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,626 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,626 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,626 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,627 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,628 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,631 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,632 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,632 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,634 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,634 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:40047
2025-10-02 09:24:21,634 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:40047
2025-10-02 09:24:21,634 - distributed.worker - INFO -          dashboard at:            10.6.83.6:36321
2025-10-02 09:24:21,634 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,634 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,634 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,634 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,634 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-4evkpddo
2025-10-02 09:24:21,634 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,634 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,635 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,635 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,636 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,637 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,637 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,637 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,638 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,648 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:36287
2025-10-02 09:24:21,648 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:36287
2025-10-02 09:24:21,648 - distributed.worker - INFO -          dashboard at:            10.6.83.6:41343
2025-10-02 09:24:21,648 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,648 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,648 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,649 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,649 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ssfv1nlh
2025-10-02 09:24:21,649 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,652 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:40315
2025-10-02 09:24:21,652 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:40315
2025-10-02 09:24:21,652 - distributed.worker - INFO -          dashboard at:            10.6.83.6:34553
2025-10-02 09:24:21,652 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,652 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,652 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,652 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,652 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-b8xb1a3p
2025-10-02 09:24:21,652 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,655 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,656 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,656 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,658 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,673 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:36709
2025-10-02 09:24:21,673 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:36709
2025-10-02 09:24:21,673 - distributed.worker - INFO -          dashboard at:            10.6.83.6:43955
2025-10-02 09:24:21,673 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,673 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,673 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,673 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,673 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ywgbu672
2025-10-02 09:24:21,673 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,707 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:46425
2025-10-02 09:24:21,707 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:46425
2025-10-02 09:24:21,707 - distributed.worker - INFO -          dashboard at:            10.6.83.6:33429
2025-10-02 09:24:21,707 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,707 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,707 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,707 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,707 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-4_iocy85
2025-10-02 09:24:21,707 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,719 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:39679
2025-10-02 09:24:21,719 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:39679
2025-10-02 09:24:21,720 - distributed.worker - INFO -          dashboard at:            10.6.83.6:42067
2025-10-02 09:24:21,720 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,720 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,720 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,720 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,720 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-jbtodbfa
2025-10-02 09:24:21,720 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,731 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:45115
2025-10-02 09:24:21,731 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:45115
2025-10-02 09:24:21,731 - distributed.worker - INFO -          dashboard at:            10.6.83.6:37845
2025-10-02 09:24:21,731 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,731 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,731 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,731 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,731 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-q8teycbl
2025-10-02 09:24:21,731 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,758 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,759 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,759 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,760 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,760 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,761 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,761 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,762 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,766 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,767 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,768 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,768 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,768 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,768 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,769 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,769 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,770 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,770 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,770 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,771 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,772 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:33665
2025-10-02 09:24:21,772 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:33665
2025-10-02 09:24:21,772 - distributed.worker - INFO -          dashboard at:            10.6.83.6:41471
2025-10-02 09:24:21,772 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,772 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,772 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,772 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,772 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ygqivqif
2025-10-02 09:24:21,772 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,779 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,780 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,780 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,781 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,791 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,791 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,791 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,792 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,832 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:46571
2025-10-02 09:24:21,832 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:46571
2025-10-02 09:24:21,832 - distributed.worker - INFO -          dashboard at:            10.6.83.6:35991
2025-10-02 09:24:21,832 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,832 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,832 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,832 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,832 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-zq432p9i
2025-10-02 09:24:21,832 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,854 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,855 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,855 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,856 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,889 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:45383
2025-10-02 09:24:21,889 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:45383
2025-10-02 09:24:21,889 - distributed.worker - INFO -          dashboard at:            10.6.83.6:33371
2025-10-02 09:24:21,889 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,889 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,889 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,889 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,889 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-2nid1x9p
2025-10-02 09:24:21,889 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,892 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:37093
2025-10-02 09:24:21,892 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:37093
2025-10-02 09:24:21,892 - distributed.worker - INFO -          dashboard at:            10.6.83.6:39691
2025-10-02 09:24:21,892 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,892 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,892 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:21,892 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:21,892 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-jxpoh60e
2025-10-02 09:24:21,892 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,910 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,911 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,911 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,912 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:21,915 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:21,916 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:21,916 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:21,918 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,043 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:35981
2025-10-02 09:24:22,043 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:35981
2025-10-02 09:24:22,043 - distributed.worker - INFO -          dashboard at:            10.6.83.6:39819
2025-10-02 09:24:22,043 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,043 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,043 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,043 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,043 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ghrsywcg
2025-10-02 09:24:22,043 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,044 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:37997
2025-10-02 09:24:22,044 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:37997
2025-10-02 09:24:22,044 - distributed.worker - INFO -          dashboard at:            10.6.83.6:44487
2025-10-02 09:24:22,044 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,044 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,044 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,044 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,044 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-3iq29ndf
2025-10-02 09:24:22,044 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,047 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:33649
2025-10-02 09:24:22,047 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:33649
2025-10-02 09:24:22,047 - distributed.worker - INFO -          dashboard at:            10.6.83.6:33917
2025-10-02 09:24:22,047 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,047 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,047 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,047 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,047 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ny_74p05
2025-10-02 09:24:22,047 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,049 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:45783
2025-10-02 09:24:22,049 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:45783
2025-10-02 09:24:22,049 - distributed.worker - INFO -          dashboard at:            10.6.83.6:45571
2025-10-02 09:24:22,050 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,050 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,050 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,050 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,050 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-yrm6mm2g
2025-10-02 09:24:22,050 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:35623
2025-10-02 09:24:22,050 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,050 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:35623
2025-10-02 09:24:22,050 - distributed.worker - INFO -          dashboard at:            10.6.83.6:43453
2025-10-02 09:24:22,050 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,050 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,050 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,050 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,050 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-nj_044y0
2025-10-02 09:24:22,050 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,050 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:41567
2025-10-02 09:24:22,050 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:41567
2025-10-02 09:24:22,050 - distributed.worker - INFO -          dashboard at:            10.6.83.6:40575
2025-10-02 09:24:22,050 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,050 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,050 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,050 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,050 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-lb4p41m4
2025-10-02 09:24:22,050 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,051 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:38105
2025-10-02 09:24:22,052 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:38105
2025-10-02 09:24:22,052 - distributed.worker - INFO -          dashboard at:            10.6.83.6:35851
2025-10-02 09:24:22,052 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,052 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,052 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,052 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,052 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-i8er5g1o
2025-10-02 09:24:22,052 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,053 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:44951
2025-10-02 09:24:22,053 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:44951
2025-10-02 09:24:22,053 - distributed.worker - INFO -          dashboard at:            10.6.83.6:45307
2025-10-02 09:24:22,053 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,053 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,053 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,054 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,054 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-uh6vpy34
2025-10-02 09:24:22,054 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,054 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:46563
2025-10-02 09:24:22,054 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:46563
2025-10-02 09:24:22,054 - distributed.worker - INFO -          dashboard at:            10.6.83.6:41987
2025-10-02 09:24:22,054 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,054 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,054 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,054 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,054 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-k9dv6978
2025-10-02 09:24:22,054 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,055 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:45541
2025-10-02 09:24:22,055 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:45541
2025-10-02 09:24:22,055 - distributed.worker - INFO -          dashboard at:            10.6.83.6:39429
2025-10-02 09:24:22,055 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,055 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,055 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,055 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,055 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-osigxzdd
2025-10-02 09:24:22,055 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,061 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:40477
2025-10-02 09:24:22,061 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:40477
2025-10-02 09:24:22,061 - distributed.worker - INFO -          dashboard at:            10.6.83.6:38033
2025-10-02 09:24:22,061 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,061 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,061 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,061 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,061 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-jlhx4ed0
2025-10-02 09:24:22,061 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,061 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:35639
2025-10-02 09:24:22,062 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:35639
2025-10-02 09:24:22,062 - distributed.worker - INFO -          dashboard at:            10.6.83.6:35519
2025-10-02 09:24:22,062 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,062 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,062 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,062 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,062 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-nk7g8sx2
2025-10-02 09:24:22,062 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,063 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,063 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:34389
2025-10-02 09:24:22,063 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,063 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:34389
2025-10-02 09:24:22,063 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,063 - distributed.worker - INFO -          dashboard at:            10.6.83.6:39941
2025-10-02 09:24:22,063 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,063 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,063 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,063 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,064 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-el4uv1ms
2025-10-02 09:24:22,064 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,064 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,065 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:42237
2025-10-02 09:24:22,065 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:42237
2025-10-02 09:24:22,065 - distributed.worker - INFO -          dashboard at:            10.6.83.6:45309
2025-10-02 09:24:22,065 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,065 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,065 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,065 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,065 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-mjgrllee
2025-10-02 09:24:22,065 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,065 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:39179
2025-10-02 09:24:22,065 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:39179
2025-10-02 09:24:22,065 - distributed.worker - INFO -          dashboard at:            10.6.83.6:46697
2025-10-02 09:24:22,066 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,066 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,066 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,066 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,066 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-t4fgzec0
2025-10-02 09:24:22,066 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,066 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:38437
2025-10-02 09:24:22,066 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:38437
2025-10-02 09:24:22,066 - distributed.worker - INFO -          dashboard at:            10.6.83.6:43999
2025-10-02 09:24:22,066 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,066 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,066 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,066 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,066 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-xij_oyu_
2025-10-02 09:24:22,066 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,067 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,068 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,068 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,069 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,069 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,070 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,070 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,071 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,071 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,072 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:38503
2025-10-02 09:24:22,072 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:38503
2025-10-02 09:24:22,072 - distributed.worker - INFO -          dashboard at:            10.6.83.6:42445
2025-10-02 09:24:22,072 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,072 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,072 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,072 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,072 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-4111722a
2025-10-02 09:24:22,072 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,072 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,072 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,073 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:46049
2025-10-02 09:24:22,073 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:46049
2025-10-02 09:24:22,074 - distributed.worker - INFO -          dashboard at:            10.6.83.6:33737
2025-10-02 09:24:22,074 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:34595
2025-10-02 09:24:22,074 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,074 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,074 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:34595
2025-10-02 09:24:22,074 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,074 - distributed.worker - INFO -          dashboard at:            10.6.83.6:36967
2025-10-02 09:24:22,074 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,074 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,074 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-yfg4cel0
2025-10-02 09:24:22,074 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,074 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,074 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,074 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,074 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,074 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-y9ftabwc
2025-10-02 09:24:22,074 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,074 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,074 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,075 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,075 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,075 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,076 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,076 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,076 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,077 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,077 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,077 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:34175
2025-10-02 09:24:22,077 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,077 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:34175
2025-10-02 09:24:22,077 - distributed.worker - INFO -          dashboard at:            10.6.83.6:36505
2025-10-02 09:24:22,077 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,077 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,077 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,077 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,077 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,077 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-0p08bpkw
2025-10-02 09:24:22,077 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,078 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,078 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,078 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,078 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,078 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,078 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,079 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,079 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,079 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,079 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,079 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,080 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,080 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,081 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,081 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,083 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,083 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,084 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:45805
2025-10-02 09:24:22,084 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:45805
2025-10-02 09:24:22,084 - distributed.worker - INFO -          dashboard at:            10.6.83.6:45143
2025-10-02 09:24:22,084 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,084 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,084 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,084 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,084 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,084 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,084 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-qi7c97mq
2025-10-02 09:24:22,084 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,085 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,086 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,086 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:40993
2025-10-02 09:24:22,086 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:40993
2025-10-02 09:24:22,086 - distributed.worker - INFO -          dashboard at:            10.6.83.6:41087
2025-10-02 09:24:22,086 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,086 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,086 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,086 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,086 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,086 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,086 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-74ptmqlc
2025-10-02 09:24:22,086 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,087 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,087 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,087 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,087 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,088 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,089 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,090 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,090 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,090 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,091 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:44283
2025-10-02 09:24:22,091 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:44283
2025-10-02 09:24:22,091 - distributed.worker - INFO -          dashboard at:            10.6.83.6:42649
2025-10-02 09:24:22,091 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,091 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,091 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,091 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,091 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-bca9hd9y
2025-10-02 09:24:22,091 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,091 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,091 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,091 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,092 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,092 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,092 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,093 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,094 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,095 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,095 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:34419
2025-10-02 09:24:22,095 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:34419
2025-10-02 09:24:22,095 - distributed.worker - INFO -          dashboard at:            10.6.83.6:45513
2025-10-02 09:24:22,095 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,095 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,095 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,095 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,095 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-nlfspvhb
2025-10-02 09:24:22,095 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,096 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,096 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,096 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,096 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,096 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,097 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,097 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,098 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.6:34367
2025-10-02 09:24:22,098 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.6:34367
2025-10-02 09:24:22,098 - distributed.worker - INFO -          dashboard at:            10.6.83.6:40939
2025-10-02 09:24:22,098 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,098 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,098 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:22,098 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:22,098 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-mc5s_97p
2025-10-02 09:24:22,098 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,098 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,099 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,099 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,099 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,100 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,100 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,100 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,101 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,101 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,101 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,101 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,102 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,110 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,110 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,110 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,111 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,111 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,111 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,112 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,113 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:22,113 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:22,114 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:22,114 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:22,115 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:56,421 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,421 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,421 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,421 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,421 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,421 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,422 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,422 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,422 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,422 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,422 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,422 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,423 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,423 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,423 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,423 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,423 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,423 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,423 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,423 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,423 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,423 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,424 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,424 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,424 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,424 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,424 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,424 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,424 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,424 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,424 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,424 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,424 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,424 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,424 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,425 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,425 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,425 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,425 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,425 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,425 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,425 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,425 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,425 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,425 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,425 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,426 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,426 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,426 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,426 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,426 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,426 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,426 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,426 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,426 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,426 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,426 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,426 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,426 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,426 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,426 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,424 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,425 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,427 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,427 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,427 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,425 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,425 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,427 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,425 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,427 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,428 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,428 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,428 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,428 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,428 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,429 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,429 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,429 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,429 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,430 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,428 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,430 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,430 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,428 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,430 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,428 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,429 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,430 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,430 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,430 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,436 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,437 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,437 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,437 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,438 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,438 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,439 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,439 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,439 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,440 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,440 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,440 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,443 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:59,177 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,177 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,177 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,177 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,177 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,177 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,178 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,178 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,178 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,178 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,178 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,178 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,178 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,178 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,179 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,178 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,179 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,179 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,179 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,179 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,179 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,179 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,179 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,179 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,180 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,180 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,180 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,180 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,180 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,180 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,181 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,181 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,180 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,180 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,181 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,181 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,181 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,181 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,181 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,181 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,181 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,181 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,181 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,181 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,181 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,181 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,181 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,181 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,181 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,181 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,182 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,181 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,181 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,182 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,181 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,182 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,182 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,182 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,182 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,182 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,182 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,182 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,182 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,182 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,182 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,180 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,182 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,183 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,183 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,182 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,182 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,183 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,183 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,183 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,183 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,182 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,183 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,183 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,183 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,183 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,183 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,183 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,183 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,184 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,184 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,184 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,184 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,184 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,184 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,184 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,184 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,185 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,185 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,186 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,186 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,186 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,186 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,186 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,186 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,186 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,186 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,186 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,186 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,195 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,585 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,585 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,586 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,586 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,586 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,586 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,586 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,586 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,586 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,586 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,587 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,587 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,587 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,587 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,587 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,587 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,587 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,587 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,588 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,588 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,588 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,588 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,588 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,588 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,589 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,589 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,589 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,589 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,589 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,589 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,589 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,589 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,589 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,589 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,589 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,590 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,590 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,590 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,590 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,590 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,590 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,590 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,590 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,590 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,590 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,590 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,590 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,591 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,590 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,590 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,591 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,591 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,591 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,591 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,591 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,591 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,591 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,591 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,591 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,591 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,591 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,591 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,591 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,592 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,592 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,592 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,592 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,592 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,592 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,592 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,592 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,592 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,592 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,592 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,593 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,593 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,593 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,594 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,594 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,594 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,594 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,594 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,594 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,594 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,595 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,595 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,595 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,595 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,595 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,595 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,595 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,595 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,595 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,595 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,595 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,595 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,596 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,596 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,596 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,596 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,596 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,596 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,596 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,596 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:25:00,012 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,012 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,012 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,012 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,012 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,012 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,012 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,012 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,013 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,013 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,013 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,013 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,013 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,013 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,013 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,013 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,014 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,014 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,014 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,014 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,014 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,014 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,014 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,014 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,015 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,015 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,015 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,015 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,015 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,015 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,015 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,015 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,015 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,015 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,015 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,015 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,015 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,015 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,015 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,015 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,015 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,015 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,015 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,016 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,016 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,016 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,016 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,016 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,016 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,016 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,016 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,016 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,016 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,016 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,016 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,016 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,016 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,016 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,016 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,016 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,016 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,016 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,016 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,017 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,017 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,017 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,017 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,017 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,017 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,017 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,017 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,017 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,017 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,017 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,017 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,017 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,017 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,017 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,017 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,018 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,018 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,018 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,018 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,018 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,019 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,019 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,019 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,019 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,019 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,019 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,019 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,019 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,020 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,020 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,020 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,020 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,020 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,021 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,021 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,021 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,021 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,021 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,021 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,021 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:22,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:22,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,665 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,822 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,081 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,092 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,093 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,093 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,097 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,097 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,098 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,098 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,140 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,176 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 19:25:43,481 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,480 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59450 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,481 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,480 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59444 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,593 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:35623. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,593 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:36709. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,481 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,594 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:44189. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,480 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59358 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,594 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:45383. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,511 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,594 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:38417. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,480 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,594 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:33649. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,595 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:46821. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,480 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59382 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,481 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59328 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,595 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:34389. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,595 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:36287. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,481 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,481 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,596 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:41367. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,596 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:45783. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,480 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59398 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,596 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:35639. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,597 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:41819. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,480 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59468 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,482 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,597 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:33257. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,481 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,597 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,497 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,597 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,598 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:42443. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,598 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:43059. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,596 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,480 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59456 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,598 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:44951. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,598 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:38437. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,598 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:34175. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,521 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,598 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:45541. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,526 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,527 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,599 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:37557. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,596 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,528 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,522 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,599 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:38503. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,528 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,538 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,600 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:44045. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,538 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,595 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,543 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,566 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,601 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:34595. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,596 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59258 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,597 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,603 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:39179. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,600 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,603 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:41567. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,604 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:44283. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,604 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:33947. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,604 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:46089. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,604 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:33545. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,599 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59266 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,605 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:41965. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,605 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:40315. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,605 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:36023. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,606 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:44793. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,606 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:45115. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,606 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:40047. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,606 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:37093. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,601 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,602 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59280 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,602 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59310 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,607 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:34367. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,607 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:44115'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,603 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59294 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,601 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59252 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,609 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,610 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,610 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,610 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,610 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,598 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,613 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:42249'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,614 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:44105'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,614 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,614 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:38343'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,615 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,615 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:41491'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,615 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,615 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,615 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:44411'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,615 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,615 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,615 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,615 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,615 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,615 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,616 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,616 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,616 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,616 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,616 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,616 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,616 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,616 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,616 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,616 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,616 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,616 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,616 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,617 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,617 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:46425. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,617 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,622 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,622 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:40353'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,622 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,623 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:37791'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,623 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:40873'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,623 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,623 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:39397'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,623 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,624 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,624 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:45819'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,624 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,624 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,624 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,624 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:37389'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,624 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,624 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,624 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,624 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,624 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,624 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,624 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:36179'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,624 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,624 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,624 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,624 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,625 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:44741'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,625 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,625 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,625 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,625 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,625 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,625 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:35193'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,625 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,625 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,625 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,625 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,625 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:41963'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,625 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,625 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,625 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,625 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,625 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,625 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:46345'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,625 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,625 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,625 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,626 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:44065'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,626 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,626 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,626 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:33643'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,626 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,626 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,626 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,626 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,626 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:45139'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,626 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,626 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,626 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,626 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,626 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,626 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,626 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:39533'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,626 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,626 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,627 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,627 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,627 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,627 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,627 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,627 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,627 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,627 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,627 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,627 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,627 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,627 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,627 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,627 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,627 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,627 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,627 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,627 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,627 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,628 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,628 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,628 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,628 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,628 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,628 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,628 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,628 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,628 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,628 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,629 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,629 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,629 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,629 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:32901'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,629 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:42575'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,629 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:45113'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,630 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:44417'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,630 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,630 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,630 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,630 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,630 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,630 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,630 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,631 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,631 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,631 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,631 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,631 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,631 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,631 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,631 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,631 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,631 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,631 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,631 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,631 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,631 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,632 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,633 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,634 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,635 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,636 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,636 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,636 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,637 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,638 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,638 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,639 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:35989'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,639 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,640 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,640 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,640 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,640 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,640 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,641 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,641 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,641 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,642 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,643 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,644 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,644 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,652 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,710 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:46483'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,476 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:60548 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,715 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,723 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:33665. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,741 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:45489'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,745 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 13, 2, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:43,751 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,752 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 13, 8, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:43,756 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,762 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,767 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,772 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,778 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,784 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,790 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,494 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:57488 remote=tcp://10.6.83.1:8728>: Stream is closed
2025-10-02 19:25:43,794 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:41693'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,795 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,796 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:39583'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,800 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,500 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59422 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,492 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59484 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,810 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 0, 7, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:43,809 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,817 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:34419. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,541 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59270 remote=tcp://10.6.83.1:8728>: Stream is closed
2025-10-02 19:25:43,825 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,827 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,834 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:46571. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,836 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:40993. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,521 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59342 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,851 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,859 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:38105. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,858 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,863 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:44611'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,505 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59408 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,867 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:40477. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,537 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59366 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,889 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,891 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,893 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,897 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:42237. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,900 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:37997. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,901 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:45805. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,554 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59436 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,911 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,920 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:46049. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,649 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59320 remote=tcp://10.6.83.1:8728>: Stream is closed
2025-10-02 19:25:43,935 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 16, 8, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:43,935 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,936 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,942 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,947 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,952 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,652 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.6:59234 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:44,052 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:44,060 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:46563. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,067 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:41101'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,069 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 6, 12, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,072 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:37491'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,073 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 1, 0, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,075 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,079 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,080 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,085 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,085 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,090 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,091 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,092 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:36787'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,095 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,096 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,100 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,114 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:33237'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,118 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:35315'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,130 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 14, 14, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,136 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,141 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,147 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,152 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,154 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,157 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,158 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:33553'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,159 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,165 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,170 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,170 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 3, 14, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,175 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,176 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,182 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:35747'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,182 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,188 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,180 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:44,191 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:39417'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,193 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,196 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:35981. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,197 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 1, 19, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,198 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,206 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 14, 13, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,210 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:32787'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,227 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,230 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 6, 18, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,233 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,237 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,237 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,238 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,238 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 11, 1, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,243 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,243 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,244 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,248 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,248 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,250 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,253 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,255 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,260 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,265 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,267 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,272 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,277 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,282 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,288 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 8, 10, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,303 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,309 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,314 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,319 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,324 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,335 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:42095'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,337 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:46773'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,351 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:45047'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,374 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 14, 3, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,375 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:40955'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,379 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 15, 8, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,401 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,404 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 1, 10, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,406 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,406 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,411 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,411 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,417 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,417 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,422 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,422 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,424 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 3, 12, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,427 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 2, 15, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,427 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,430 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,433 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,436 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,439 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,441 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,444 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,446 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,449 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,452 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,454 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,463 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:46795'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,465 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:37407'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,476 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,481 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,487 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,492 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,497 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,499 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:41205'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,516 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:40351'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,533 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 11, 0, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,539 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,545 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,546 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:38647'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,550 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,556 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,561 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,568 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 16, 18, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,570 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:44,578 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.6:39679. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,584 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 9, 12, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,597 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 11, 21, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,609 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,614 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,619 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,625 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,630 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,644 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 3, 21, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,705 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:34071'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,918 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,923 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,929 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,934 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,939 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,953 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,958 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,959 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,964 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,964 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,967 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,968 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,968 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,969 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,974 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:45,060 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 1, 20, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:45,087 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:45,092 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:45,097 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:45,102 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:45,108 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:45,391 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:33301'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:45,626 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,627 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,630 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,631 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,633 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,633 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,634 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,636 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,636 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,638 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,640 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,640 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,640 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,640 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,641 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,642 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,642 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,644 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,645 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,647 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,648 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,649 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,656 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,840 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 9, 19, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:45,883 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:45,889 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:45,894 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:45,899 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:45,905 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:45,943 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:40353'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,945 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:46345'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,948 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:40353' closed.
2025-10-02 19:25:45,948 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:46345' closed.
2025-10-02 19:25:45,961 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:42575'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,962 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:44741'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,963 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:42575' closed.
2025-10-02 19:25:45,964 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:44741' closed.
2025-10-02 19:25:46,071 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:38343'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,074 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:38343' closed.
2025-10-02 19:25:46,193 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:44417'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,194 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:44417' closed.
2025-10-02 19:25:46,203 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:35193'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,204 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:35193' closed.
2025-10-02 19:25:46,215 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:44115'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,216 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:44115' closed.
2025-10-02 19:25:46,222 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:44411'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,223 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:44411' closed.
2025-10-02 19:25:46,247 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:33643'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,248 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:33643' closed.
2025-10-02 19:25:46,250 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:40873'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,251 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:40873' closed.
2025-10-02 19:25:46,253 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:37791'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,254 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:37791' closed.
2025-10-02 19:25:46,256 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:41491'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,257 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:41491' closed.
2025-10-02 19:25:46,274 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:39533'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,275 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:39533' closed.
2025-10-02 19:25:46,304 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:35989'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,305 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:35989' closed.
2025-10-02 19:25:46,317 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:32901'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,318 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:44065'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,329 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:32901' closed.
2025-10-02 19:25:46,329 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:44065' closed.
2025-10-02 19:25:46,329 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:37389'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,330 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:37389' closed.
2025-10-02 19:25:46,363 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:42249'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,364 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:42249' closed.
2025-10-02 19:25:46,370 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:45819'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,371 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:44105'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,374 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:41963'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,374 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:45139'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,376 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:45819' closed.
2025-10-02 19:25:46,376 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:44105' closed.
2025-10-02 19:25:46,376 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:36179'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,377 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:41963' closed.
2025-10-02 19:25:46,378 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:45139' closed.
2025-10-02 19:25:46,378 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:36179' closed.
2025-10-02 19:25:46,394 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:45113'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,395 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:45113' closed.
2025-10-02 19:25:46,444 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:39397'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,445 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:39397' closed.
2025-10-02 19:25:47,560 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.6:41171'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:48,210 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 9, 21, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:48,218 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:48,224 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:48,229 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:48,234 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:48,240 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:50,512 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:52,516 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:53,021 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.6:45047'. Reason: nanny-close-gracefully
2025-10-02 19:25:53,022 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.6:45047' closed.
