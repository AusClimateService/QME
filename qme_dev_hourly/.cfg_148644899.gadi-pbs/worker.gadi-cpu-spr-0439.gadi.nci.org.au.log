Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 16:27:03,605 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:38015'
2025-09-03 16:27:03,615 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:40703'
2025-09-03 16:27:03,619 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:39533'
2025-09-03 16:27:03,623 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:33379'
2025-09-03 16:27:03,627 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:42887'
2025-09-03 16:27:03,631 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:45641'
2025-09-03 16:27:03,635 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:35523'
2025-09-03 16:27:03,639 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:40565'
2025-09-03 16:27:03,643 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:33575'
2025-09-03 16:27:03,650 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:42527'
2025-09-03 16:27:03,654 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:38461'
2025-09-03 16:27:03,659 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:33599'
2025-09-03 16:27:03,663 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:36259'
2025-09-03 16:27:03,667 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:33183'
2025-09-03 16:27:03,671 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:44641'
2025-09-03 16:27:03,675 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:46815'
2025-09-03 16:27:03,679 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:43615'
2025-09-03 16:27:03,683 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:34703'
2025-09-03 16:27:03,687 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:41729'
2025-09-03 16:27:03,691 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:41409'
2025-09-03 16:27:03,867 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:34017'
2025-09-03 16:27:03,873 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:40459'
2025-09-03 16:27:03,877 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:40333'
2025-09-03 16:27:03,881 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:35613'
2025-09-03 16:27:03,886 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:39229'
2025-09-03 16:27:03,890 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:40487'
2025-09-03 16:27:03,894 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:42877'
2025-09-03 16:27:03,898 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:33865'
2025-09-03 16:27:03,903 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:46343'
2025-09-03 16:27:03,908 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:36413'
2025-09-03 16:27:03,913 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:33547'
2025-09-03 16:27:03,918 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:34713'
2025-09-03 16:27:03,922 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:39619'
2025-09-03 16:27:03,927 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:43777'
2025-09-03 16:27:03,932 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:36809'
2025-09-03 16:27:03,936 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:37717'
2025-09-03 16:27:03,940 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:39847'
2025-09-03 16:27:03,945 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:37291'
2025-09-03 16:27:03,948 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:34223'
2025-09-03 16:27:03,952 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:37311'
2025-09-03 16:27:03,958 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:37105'
2025-09-03 16:27:03,961 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:40409'
2025-09-03 16:27:03,966 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:40315'
2025-09-03 16:27:03,970 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:38397'
2025-09-03 16:27:03,974 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:38881'
2025-09-03 16:27:03,978 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:35103'
2025-09-03 16:27:03,982 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:36601'
2025-09-03 16:27:03,987 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:45689'
2025-09-03 16:27:03,991 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:45787'
2025-09-03 16:27:03,996 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:44781'
2025-09-03 16:27:03,999 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:42825'
2025-09-03 16:27:04,005 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.7:38625'
2025-09-03 16:27:04,835 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:42621
2025-09-03 16:27:04,835 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:45181
2025-09-03 16:27:04,835 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:46335
2025-09-03 16:27:04,835 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:42621
2025-09-03 16:27:04,835 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:40629
2025-09-03 16:27:04,835 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:43831
2025-09-03 16:27:04,835 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:45181
2025-09-03 16:27:04,835 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:46335
2025-09-03 16:27:04,835 - distributed.worker - INFO -          dashboard at:           10.6.102.7:33651
2025-09-03 16:27:04,835 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:40629
2025-09-03 16:27:04,835 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:43831
2025-09-03 16:27:04,835 - distributed.worker - INFO -          dashboard at:           10.6.102.7:41291
2025-09-03 16:27:04,835 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,835 - distributed.worker - INFO -          dashboard at:           10.6.102.7:40843
2025-09-03 16:27:04,835 - distributed.worker - INFO -          dashboard at:           10.6.102.7:44329
2025-09-03 16:27:04,835 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,835 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,835 - distributed.worker - INFO -          dashboard at:           10.6.102.7:34221
2025-09-03 16:27:04,835 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,835 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,835 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:43757
2025-09-03 16:27:04,835 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:04,835 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,835 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:42511
2025-09-03 16:27:04,835 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,835 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,835 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,835 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:42893
2025-09-03 16:27:04,835 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:41699
2025-09-03 16:27:04,835 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:38069
2025-09-03 16:27:04,835 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:45297
2025-09-03 16:27:04,835 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:35297
2025-09-03 16:27:04,835 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:38665
2025-09-03 16:27:04,835 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:04,835 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:04,835 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:43757
2025-09-03 16:27:04,835 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:04,835 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:45577
2025-09-03 16:27:04,835 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:42511
2025-09-03 16:27:04,835 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,835 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:43909
2025-09-03 16:27:04,836 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:04,835 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:41979
2025-09-03 16:27:04,835 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:36979
2025-09-03 16:27:04,836 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:42893
2025-09-03 16:27:04,836 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:41699
2025-09-03 16:27:04,836 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:38069
2025-09-03 16:27:04,836 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:45297
2025-09-03 16:27:04,836 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:35297
2025-09-03 16:27:04,836 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:38665
2025-09-03 16:27:04,836 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:04,836 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-t432dqz_
2025-09-03 16:27:04,836 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:04,836 - distributed.worker - INFO -          dashboard at:           10.6.102.7:37433
2025-09-03 16:27:04,836 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:38109
2025-09-03 16:27:04,836 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:04,836 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:45577
2025-09-03 16:27:04,836 - distributed.worker - INFO -          dashboard at:           10.6.102.7:38713
2025-09-03 16:27:04,836 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:04,836 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:43909
2025-09-03 16:27:04,836 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:41979
2025-09-03 16:27:04,836 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:36979
2025-09-03 16:27:04,836 - distributed.worker - INFO -          dashboard at:           10.6.102.7:42055
2025-09-03 16:27:04,836 - distributed.worker - INFO -          dashboard at:           10.6.102.7:37867
2025-09-03 16:27:04,836 - distributed.worker - INFO -          dashboard at:           10.6.102.7:42671
2025-09-03 16:27:04,836 - distributed.worker - INFO -          dashboard at:           10.6.102.7:37681
2025-09-03 16:27:04,836 - distributed.worker - INFO -          dashboard at:           10.6.102.7:43631
2025-09-03 16:27:04,836 - distributed.worker - INFO -          dashboard at:           10.6.102.7:34655
2025-09-03 16:27:04,836 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-6x3lb_fc
2025-09-03 16:27:04,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,836 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,836 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-dz265m75
2025-09-03 16:27:04,836 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:04,836 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:38109
2025-09-03 16:27:04,836 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,836 - distributed.worker - INFO -          dashboard at:           10.6.102.7:34739
2025-09-03 16:27:04,836 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-i9v81bgq
2025-09-03 16:27:04,836 - distributed.worker - INFO -          dashboard at:           10.6.102.7:36753
2025-09-03 16:27:04,836 - distributed.worker - INFO -          dashboard at:           10.6.102.7:33175
2025-09-03 16:27:04,836 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,836 - distributed.worker - INFO -          dashboard at:           10.6.102.7:45709
2025-09-03 16:27:04,836 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,836 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,836 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,836 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,836 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,836 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-b6cqr_49
2025-09-03 16:27:04,836 - distributed.worker - INFO -          dashboard at:           10.6.102.7:42803
2025-09-03 16:27:04,836 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,836 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,836 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,836 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,836 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:04,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,836 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:04,836 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:04,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,836 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,836 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:04,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,836 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:04,836 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:04,836 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:04,836 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:04,836 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:04,836 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:04,836 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:04,836 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:04,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,836 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:04,836 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:04,836 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:04,836 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:04,836 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:04,836 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:04,836 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:04,836 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-ifhijbtl
2025-09-03 16:27:04,836 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:04,836 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:04,836 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:04,836 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-z3_9hloj
2025-09-03 16:27:04,836 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-mrjepi3z
2025-09-03 16:27:04,836 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:04,836 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:04,836 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:04,836 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-y2gx8dkc
2025-09-03 16:27:04,836 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-09tmiqpa
2025-09-03 16:27:04,836 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-r7d9ac0a
2025-09-03 16:27:04,836 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-g2gl1e7g
2025-09-03 16:27:04,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,836 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-h5oakfyk
2025-09-03 16:27:04,836 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:04,836 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-tgexzpc_
2025-09-03 16:27:04,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,836 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-hxkvqrsp
2025-09-03 16:27:04,836 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-b45f4lpv
2025-09-03 16:27:04,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,836 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-nlwy27vc
2025-09-03 16:27:04,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,837 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,837 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,837 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,837 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,837 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-rv1u8h1n
2025-09-03 16:27:04,837 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,837 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,837 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,837 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,837 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,852 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:04,853 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,853 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,853 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:04,854 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:04,855 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,855 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,855 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:04,857 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:04,857 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,857 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,858 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:04,859 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:04,859 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,859 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,860 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:04,863 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:04,863 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:04,864 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,864 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,864 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,864 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,864 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:04,865 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:04,865 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:04,866 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,867 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:04,867 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,867 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:04,868 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,868 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,868 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,868 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,868 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:04,868 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:04,869 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:04,871 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:04,872 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:04,872 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,872 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,873 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,873 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,874 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:04,874 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:04,874 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:04,875 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,875 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,875 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:04,876 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:04,876 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,876 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,876 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:04,877 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,877 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:04,878 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:04,878 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:04,878 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,878 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,879 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:04,879 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,879 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,879 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:04,880 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:04,880 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:04,881 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,881 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,881 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:04,881 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:04,881 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:04,882 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:04,883 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,060 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:39317
2025-09-03 16:27:05,060 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:39317
2025-09-03 16:27:05,060 - distributed.worker - INFO -          dashboard at:           10.6.102.7:34549
2025-09-03 16:27:05,060 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,060 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,060 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,060 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,060 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-phlcnz_a
2025-09-03 16:27:05,060 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,065 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:42251
2025-09-03 16:27:05,065 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:42251
2025-09-03 16:27:05,065 - distributed.worker - INFO -          dashboard at:           10.6.102.7:42277
2025-09-03 16:27:05,065 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,065 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,066 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,066 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,066 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-u5rbgw6w
2025-09-03 16:27:05,066 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,077 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,077 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,077 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,078 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,084 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,085 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,086 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,087 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,140 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:38615
2025-09-03 16:27:05,140 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:38615
2025-09-03 16:27:05,140 - distributed.worker - INFO -          dashboard at:           10.6.102.7:40597
2025-09-03 16:27:05,140 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,140 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,140 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,140 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,140 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-js8a6e1b
2025-09-03 16:27:05,140 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,141 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:40343
2025-09-03 16:27:05,141 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:40343
2025-09-03 16:27:05,141 - distributed.worker - INFO -          dashboard at:           10.6.102.7:46689
2025-09-03 16:27:05,141 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,141 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,141 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,141 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,141 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-wz6opi9d
2025-09-03 16:27:05,141 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,163 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,164 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,165 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,165 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,166 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,166 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,166 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,168 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,192 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:45741
2025-09-03 16:27:05,192 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:45741
2025-09-03 16:27:05,192 - distributed.worker - INFO -          dashboard at:           10.6.102.7:33071
2025-09-03 16:27:05,192 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,192 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,192 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,192 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-najecf4r
2025-09-03 16:27:05,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,212 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:40615
2025-09-03 16:27:05,212 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:40615
2025-09-03 16:27:05,212 - distributed.worker - INFO -          dashboard at:           10.6.102.7:42687
2025-09-03 16:27:05,212 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,212 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,212 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,212 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,212 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-6pjh1q_y
2025-09-03 16:27:05,212 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,215 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,216 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,216 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,217 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,237 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,238 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,238 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,239 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,251 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:37425
2025-09-03 16:27:05,251 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:37425
2025-09-03 16:27:05,251 - distributed.worker - INFO -          dashboard at:           10.6.102.7:34707
2025-09-03 16:27:05,251 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,251 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,251 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,251 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,251 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-1faeovbi
2025-09-03 16:27:05,251 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,272 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,273 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,273 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,274 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,304 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:40093
2025-09-03 16:27:05,304 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:40093
2025-09-03 16:27:05,304 - distributed.worker - INFO -          dashboard at:           10.6.102.7:43145
2025-09-03 16:27:05,304 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,304 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,304 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,304 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,304 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-_ma2dsm0
2025-09-03 16:27:05,304 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,308 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:38823
2025-09-03 16:27:05,308 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:38823
2025-09-03 16:27:05,308 - distributed.worker - INFO -          dashboard at:           10.6.102.7:37637
2025-09-03 16:27:05,308 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,308 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,308 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,308 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-u2pf1i71
2025-09-03 16:27:05,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,311 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:36599
2025-09-03 16:27:05,311 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:36599
2025-09-03 16:27:05,311 - distributed.worker - INFO -          dashboard at:           10.6.102.7:42101
2025-09-03 16:27:05,311 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,311 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,311 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,311 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,311 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-85pt1rnv
2025-09-03 16:27:05,312 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,318 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,318 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,318 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,319 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,324 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,324 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,324 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,325 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,329 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,330 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,330 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,331 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,338 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:42341
2025-09-03 16:27:05,338 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:42341
2025-09-03 16:27:05,338 - distributed.worker - INFO -          dashboard at:           10.6.102.7:42989
2025-09-03 16:27:05,338 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,338 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,338 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,338 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,338 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-j3z9hubc
2025-09-03 16:27:05,338 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,344 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:40465
2025-09-03 16:27:05,344 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:40465
2025-09-03 16:27:05,344 - distributed.worker - INFO -          dashboard at:           10.6.102.7:39029
2025-09-03 16:27:05,344 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,344 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,344 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,344 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,344 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-5conl9q8
2025-09-03 16:27:05,344 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,360 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,361 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,361 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,362 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,366 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,367 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,368 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,369 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,519 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:40765
2025-09-03 16:27:05,519 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:40765
2025-09-03 16:27:05,519 - distributed.worker - INFO -          dashboard at:           10.6.102.7:39699
2025-09-03 16:27:05,519 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,519 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,519 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,519 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,519 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-7mzj7iy_
2025-09-03 16:27:05,519 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,524 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:37409
2025-09-03 16:27:05,524 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:37409
2025-09-03 16:27:05,524 - distributed.worker - INFO -          dashboard at:           10.6.102.7:44769
2025-09-03 16:27:05,524 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,524 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,524 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-zpbqdi2f
2025-09-03 16:27:05,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,524 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:44249
2025-09-03 16:27:05,524 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:44249
2025-09-03 16:27:05,524 - distributed.worker - INFO -          dashboard at:           10.6.102.7:34473
2025-09-03 16:27:05,524 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:35663
2025-09-03 16:27:05,524 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,524 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:35663
2025-09-03 16:27:05,524 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,524 - distributed.worker - INFO -          dashboard at:           10.6.102.7:43085
2025-09-03 16:27:05,524 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-_86hmblh
2025-09-03 16:27:05,524 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,524 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,524 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-7_9v8pqj
2025-09-03 16:27:05,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,530 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:35817
2025-09-03 16:27:05,530 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:35817
2025-09-03 16:27:05,530 - distributed.worker - INFO -          dashboard at:           10.6.102.7:37597
2025-09-03 16:27:05,530 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,530 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,530 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,530 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,530 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-b0ckdo26
2025-09-03 16:27:05,530 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,532 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,532 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,532 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,533 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,536 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:41411
2025-09-03 16:27:05,536 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:41411
2025-09-03 16:27:05,536 - distributed.worker - INFO -          dashboard at:           10.6.102.7:45581
2025-09-03 16:27:05,536 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,536 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,536 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,536 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,536 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-mr_afsx9
2025-09-03 16:27:05,536 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,537 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:41879
2025-09-03 16:27:05,537 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:41879
2025-09-03 16:27:05,537 - distributed.worker - INFO -          dashboard at:           10.6.102.7:34127
2025-09-03 16:27:05,537 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,537 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,537 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,537 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,537 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-evoke2a_
2025-09-03 16:27:05,537 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,545 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:38189
2025-09-03 16:27:05,545 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:38189
2025-09-03 16:27:05,545 - distributed.worker - INFO -          dashboard at:           10.6.102.7:35955
2025-09-03 16:27:05,545 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,545 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,545 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,545 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,545 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-y2oo5yj0
2025-09-03 16:27:05,545 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,546 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,546 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:42469
2025-09-03 16:27:05,546 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:42469
2025-09-03 16:27:05,546 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:37309
2025-09-03 16:27:05,546 - distributed.worker - INFO -          dashboard at:           10.6.102.7:44507
2025-09-03 16:27:05,546 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,546 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:37309
2025-09-03 16:27:05,546 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,546 - distributed.worker - INFO -          dashboard at:           10.6.102.7:43843
2025-09-03 16:27:05,546 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,546 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,546 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,546 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,546 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-brab_pwi
2025-09-03 16:27:05,546 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,546 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,546 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,546 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-sl0s8v0y
2025-09-03 16:27:05,546 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,547 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,547 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,549 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,549 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,550 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:46019
2025-09-03 16:27:05,550 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,550 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:46019
2025-09-03 16:27:05,550 - distributed.worker - INFO -          dashboard at:           10.6.102.7:40623
2025-09-03 16:27:05,550 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,550 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,550 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,550 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,550 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,550 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-_4nl16ow
2025-09-03 16:27:05,550 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,551 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,552 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:44085
2025-09-03 16:27:05,552 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:44085
2025-09-03 16:27:05,552 - distributed.worker - INFO -          dashboard at:           10.6.102.7:40125
2025-09-03 16:27:05,552 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,552 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,552 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,552 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,552 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,552 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-dybj3qqk
2025-09-03 16:27:05,552 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,552 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,552 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,553 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,554 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:45669
2025-09-03 16:27:05,554 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:45669
2025-09-03 16:27:05,554 - distributed.worker - INFO -          dashboard at:           10.6.102.7:40817
2025-09-03 16:27:05,554 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,554 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,554 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,554 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,554 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-75q3vkm3
2025-09-03 16:27:05,554 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,555 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:44571
2025-09-03 16:27:05,555 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:44571
2025-09-03 16:27:05,555 - distributed.worker - INFO -          dashboard at:           10.6.102.7:43743
2025-09-03 16:27:05,555 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,555 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,555 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,555 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,555 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-2f90e38r
2025-09-03 16:27:05,555 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,555 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:38165
2025-09-03 16:27:05,555 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:38165
2025-09-03 16:27:05,555 - distributed.worker - INFO -          dashboard at:           10.6.102.7:44573
2025-09-03 16:27:05,555 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,555 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,555 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,555 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,555 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-pj3v_uz1
2025-09-03 16:27:05,555 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,556 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,557 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,557 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,558 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:32897
2025-09-03 16:27:05,558 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:32897
2025-09-03 16:27:05,558 - distributed.worker - INFO -          dashboard at:           10.6.102.7:38025
2025-09-03 16:27:05,558 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,558 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,558 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,558 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,558 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-fn2_7oya
2025-09-03 16:27:05,558 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,558 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,559 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:35253
2025-09-03 16:27:05,559 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:35253
2025-09-03 16:27:05,559 - distributed.worker - INFO -          dashboard at:           10.6.102.7:45551
2025-09-03 16:27:05,559 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,559 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,560 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,560 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,560 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-36a_93x9
2025-09-03 16:27:05,560 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,560 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,561 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,561 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,561 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,564 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,565 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,565 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,565 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:35615
2025-09-03 16:27:05,565 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:35615
2025-09-03 16:27:05,565 - distributed.worker - INFO -          dashboard at:           10.6.102.7:32965
2025-09-03 16:27:05,565 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,565 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,565 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,565 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,565 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-hr5kck42
2025-09-03 16:27:05,566 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,566 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,571 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,572 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,572 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,572 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,573 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,573 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,574 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,574 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,574 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,574 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,574 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,575 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,575 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,575 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,575 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,576 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:41799
2025-09-03 16:27:05,576 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:41799
2025-09-03 16:27:05,576 - distributed.worker - INFO -          dashboard at:           10.6.102.7:35237
2025-09-03 16:27:05,576 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,576 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,576 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,576 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:36125
2025-09-03 16:27:05,576 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,576 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,576 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:36125
2025-09-03 16:27:05,576 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-fbv1ou8q
2025-09-03 16:27:05,576 - distributed.worker - INFO -          dashboard at:           10.6.102.7:32889
2025-09-03 16:27:05,576 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,576 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,576 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,576 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,576 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,576 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-p7en83pt
2025-09-03 16:27:05,576 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,576 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,577 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,577 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,578 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,579 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,579 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,579 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,580 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,580 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,581 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,581 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,581 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,582 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,582 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,582 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,582 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,582 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,582 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,583 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,584 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,584 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:40035
2025-09-03 16:27:05,585 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:40035
2025-09-03 16:27:05,585 - distributed.worker - INFO -          dashboard at:           10.6.102.7:45095
2025-09-03 16:27:05,585 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,585 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,585 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,585 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,585 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-i43urr88
2025-09-03 16:27:05,585 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,585 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,586 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,586 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,586 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,587 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,587 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,588 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,588 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,594 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,594 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,594 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,595 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,595 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.7:44061
2025-09-03 16:27:05,595 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.7:44061
2025-09-03 16:27:05,595 - distributed.worker - INFO -          dashboard at:           10.6.102.7:43663
2025-09-03 16:27:05,595 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,595 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,595 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:05,595 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:05,595 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-fpjrs9ar
2025-09-03 16:27:05,596 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,601 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,602 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,602 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,602 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,603 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,603 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,604 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,604 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:05,605 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:05,605 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:05,605 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:05,605 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:35,708 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,708 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,708 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,709 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,709 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,709 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,709 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,709 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,709 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,709 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,710 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,710 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,710 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,710 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,710 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,710 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,710 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,710 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,710 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,710 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,710 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,710 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,711 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,711 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,711 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,711 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,711 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,711 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,711 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,712 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,712 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,712 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,712 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,712 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,712 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,712 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,712 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,712 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,712 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,712 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,712 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,712 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,712 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,713 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,713 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,713 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,713 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,713 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,713 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,713 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,713 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,713 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,713 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,713 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,713 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,713 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,713 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,712 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,714 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,714 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,714 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,714 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,714 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,712 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,712 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,714 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,715 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,715 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,715 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,715 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,716 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,716 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,716 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,716 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,713 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,716 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,716 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,714 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,716 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,716 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,715 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,715 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,715 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,716 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,715 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,716 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,716 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,717 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,718 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,722 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,722 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,723 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,724 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,725 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,725 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,726 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,727 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,727 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,727 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,727 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,728 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,728 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,729 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,730 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:38,385 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,385 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,385 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,385 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,385 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,385 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,385 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,386 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,386 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,386 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,386 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,386 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,386 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,386 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,386 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,386 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,386 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,387 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,387 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,387 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,387 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,387 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,387 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,387 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,387 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,387 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,388 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,388 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,388 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,388 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,388 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,389 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,388 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,389 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,389 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,389 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,388 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,389 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,389 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,389 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,388 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,389 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,389 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,389 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,389 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,389 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,389 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,389 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,389 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,389 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,389 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,390 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,390 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,389 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,387 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,390 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,389 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,389 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,390 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,390 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,390 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,390 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,390 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,390 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,390 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,390 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,390 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,390 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,390 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,391 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,390 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,390 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,391 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,390 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,390 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,392 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,391 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,390 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,388 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,391 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,391 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,392 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,392 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,392 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,392 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,392 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,393 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,393 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,390 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,393 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,393 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,393 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,393 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,394 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,391 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,394 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,394 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,394 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,394 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,394 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,394 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,395 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,395 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,396 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,784 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,784 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,784 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,784 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,784 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,784 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,784 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,784 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,784 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,785 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,785 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,785 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,785 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,785 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,785 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,785 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,785 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,786 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,786 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,786 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,786 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,786 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,786 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,786 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,787 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,787 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,787 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,787 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,787 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,787 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,787 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,787 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,787 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,787 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,788 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,788 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,788 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,788 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,788 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,788 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,788 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,788 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,788 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,788 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,788 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,788 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,788 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,788 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,789 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,789 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,789 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,789 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,789 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,789 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,790 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,789 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,789 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,789 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,789 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,789 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,790 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,790 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,790 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,790 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,790 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,790 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,790 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,791 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,791 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,791 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,791 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,792 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,792 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,792 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,792 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,792 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,792 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,792 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,792 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,792 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,792 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,792 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,792 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,792 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,792 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,793 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,793 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,793 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,793 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,793 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,793 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,793 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,793 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,793 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,793 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,793 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,794 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,794 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,794 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,794 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,794 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,794 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,794 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,794 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:39,226 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,226 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,226 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,226 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,226 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,226 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,226 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,227 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,227 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,227 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,227 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,227 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,227 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,227 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,227 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,227 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,227 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,228 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,228 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,228 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,228 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,228 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,228 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,228 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,228 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,228 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,228 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,229 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,229 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,229 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,229 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,229 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,229 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,229 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,229 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,229 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,229 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,229 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,229 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,229 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,229 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,229 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,229 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,229 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,229 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,229 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,230 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,230 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,230 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,230 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,230 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,230 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,230 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,230 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,230 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,230 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,230 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,230 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,230 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,230 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,230 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,231 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,231 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,231 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,231 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,231 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,231 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,231 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,231 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,231 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,231 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,231 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,231 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,231 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,231 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,231 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,231 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,231 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,232 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,232 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,232 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,232 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,232 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,232 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,233 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,232 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,233 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,232 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,233 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,233 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,233 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,233 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,233 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,233 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,233 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,234 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,234 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,234 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,234 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,234 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,235 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,235 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,235 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,235 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:30:59,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:45,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:46,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:47,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:47,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:47,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:48,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:48,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:49,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:50,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:50,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:50,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:52,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:52,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:52,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:52,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:53,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:53,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:53,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:53,384 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:54,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:54,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:54,675 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:55,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:55,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:55,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:57,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:57,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:57,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:57,038 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:58,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:58,858 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:00,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:01,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:04,466 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:06,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:08,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:09,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:10,556 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:11,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:11,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:12,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:12,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:12,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:12,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:12,929 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:12,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:13,373 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:13,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:13,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:13,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:13,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:13,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:13,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:14,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:14,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:15,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:15,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:15,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:15,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:15,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:15,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:16,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:16,821 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:17,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:17,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:17,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:18,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:19,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:19,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:19,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:19,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:19,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:20,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:20,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:30,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:30,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:31,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:32,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:32,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:32,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:33,479 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:34,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:37,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:37,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:37,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:37,698 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:37,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:38,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:39,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:40,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:41,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:42,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:42,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:42,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:42,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:43,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:43,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:43,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:43,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:45,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:45,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:46,675 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:46,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:47,187 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:47,623 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:51,548 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:02,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:02,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:02,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:02,667 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:02,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:02,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:02,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:03,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:03,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:04,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:04,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:04,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:04,665 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:04,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:04,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:04,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,445 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,532 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,400 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:07,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:08,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:09,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:09,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:09,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:09,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:17,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:18,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:18,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:18,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:18,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:18,873 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:18,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:19,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:19,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:19,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:19,318 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:19,318 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:19,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:19,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:20,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:20,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:20,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:20,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:20,313 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:21,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:21,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:21,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:21,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:21,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:21,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:23,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:24,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:24,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:24,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:25,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:25,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:25,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:25,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:25,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:26,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:27,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:27,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:27,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:28,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:28,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:28,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:29,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:31,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:56,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:04,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:07,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:07,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:07,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:07,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:07,751 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:08,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:08,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:08,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:08,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:09,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:09,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:10,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:10,691 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:10,698 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:12,067 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:12,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:12,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:12,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:12,469 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:13,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:13,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:13,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:13,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:13,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:13,770 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:13,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:13,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:13,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:13,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:14,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:14,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:14,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:14,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:16,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:18,238 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:18,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:18,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:23,096 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:24,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:30,479 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:31,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:31,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:31,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:31,441 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:31,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:31,548 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:32,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:32,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:32,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:32,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:32,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:32,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:33,203 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:33,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:33,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:33,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:33,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,321 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,381 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,527 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:35,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:35,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:35,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:35,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:37,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:37,190 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:37,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:37,649 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:37,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:37,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:37,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:38,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:39,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:40,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:42,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:42,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:44,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:44,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:46,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:48,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:50,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:50,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:50,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:50,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:50,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:50,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:51,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:51,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:51,255 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:51,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:51,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:51,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:51,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:51,943 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:52,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:52,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:52,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:52,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:53,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:53,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:53,308 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:53,309 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:53,345 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:53,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:54,165 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:54,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:54,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:54,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:54,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:54,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:57,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:57,254 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:57,254 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:57,255 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:58,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:58,444 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:00,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:00,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:00,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:00,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:00,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:10,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:10,574 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:10,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:10,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:10,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:10,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:11,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:11,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:12,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:12,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:12,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:12,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:12,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:13,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:13,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:13,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:13,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:14,396 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:14,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:14,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:15,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:15,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:15,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:15,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:15,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:15,678 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:15,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:15,925 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:16,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:16,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:16,650 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:16,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:16,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:17,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:17,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:17,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:17,770 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:17,770 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:17,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:17,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:17,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:18,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:18,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:18,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:19,865 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:20,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:20,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:22,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:22,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:30,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:30,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:30,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:30,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:31,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:31,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:31,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:31,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:31,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:31,608 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:31,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:31,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:31,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:31,738 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:31,963 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:31,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:33,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:33,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:33,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:33,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:34,234 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,957 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:38,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:38,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:39,128 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:39,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:40,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:40,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:40,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:40,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:42,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:42,135 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:42,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:43,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:49,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:50,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,568 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:53,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:53,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:53,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:53,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:53,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:53,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:54,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:54,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:54,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,574 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:57,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:58,527 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:59,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:59,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:59,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:00,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:00,678 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:00,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:02,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:02,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:14,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:14,359 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:14,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:14,532 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:14,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:14,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:14,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:15,255 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:15,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:15,342 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:15,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:15,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:16,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:16,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:16,135 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:16,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:16,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:16,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:16,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:16,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:17,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:17,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:18,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:18,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:18,308 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:18,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:18,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:22,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:23,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:23,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:23,860 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:24,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:24,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:25,014 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:25,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:25,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:26,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:27,380 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:27,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:32,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:32,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:33,771 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:34,805 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:35,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:35,484 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:35,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:35,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:35,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:35,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:35,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:35,675 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:35,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:36,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:36,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:36,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:37,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:37,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:37,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:37,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:37,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:38,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:38,844 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:39,098 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:39,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:39,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:40,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:40,479 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:40,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:40,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:41,532 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:41,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:42,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:42,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:45,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:46,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:49,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:49,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:49,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:57,453 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:57,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:57,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:59,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:59,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:59,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:59,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:59,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:00,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:00,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:01,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:01,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:01,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:01,626 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:02,741 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:02,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:03,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:03,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:05,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:05,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:05,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:06,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:06,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:06,459 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:06,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:06,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:06,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:08,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:08,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:08,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:10,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:10,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:11,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:12,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:14,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:15,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:16,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:17,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:28,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:28,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:29,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:29,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:29,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:29,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:30,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:30,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:31,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:31,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:31,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:31,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:31,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:33,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:33,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:33,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:33,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:33,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:35,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:35,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:35,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:36,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:36,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:36,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:39,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:40,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:43,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:43,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:46,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:47,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:49,228 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:49,307 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:50,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:50,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:51,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:52,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:52,244 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:52,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:52,794 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:52,796 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:52,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:52,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:52,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:53,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:53,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:53,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:53,691 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:53,692 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:53,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:53,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:53,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:53,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:53,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:54,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:54,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:54,838 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:55,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:55,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:55,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:55,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:56,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:56,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:56,479 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:56,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:56,556 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:57,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:57,859 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:58,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:58,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:00,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:03,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:03,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:03,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:03,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:03,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:03,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:04,310 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:04,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:04,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:04,359 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:04,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:04,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:04,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:04,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:05,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:05,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:06,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:06,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:07,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:07,847 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:07,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:07,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:08,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:09,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:09,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:09,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:09,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:09,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:10,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:10,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:10,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:10,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:10,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:11,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:11,823 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:11,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:12,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:12,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:12,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:12,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:13,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:14,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:14,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:15,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:21,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:35,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:35,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:41,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:41,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:43,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:52,756 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:53,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:53,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:53,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:53,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:53,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:53,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:53,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:54,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,101 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:56,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:56,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:57,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:57,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:57,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:57,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:57,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:57,308 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:57,309 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,543 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:59,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:59,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:59,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:59,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:59,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:59,756 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:59,796 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:59,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:05,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:28,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:28,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:30,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:30,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:30,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:30,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:30,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:31,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:31,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:31,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:31,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:32,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:32,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,337 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,556 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:34,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:34,130 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:34,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:34,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:34,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:35,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:35,307 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:35,309 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:35,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:35,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:35,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:35,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:35,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:35,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:36,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:36,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:36,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:36,719 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:36,721 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:37,035 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:37,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:38,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:38,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:38,483 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:38,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:38,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:39,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:40,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:41,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:41,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:41,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:41,612 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:47,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:51,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:52,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,617 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:56,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:56,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:56,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:56,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:56,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:56,963 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:56,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:56,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,178 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:58,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:58,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:58,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:58,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:58,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:58,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:59,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:10,400 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:10,850 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:10,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:13,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:13,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:13,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:13,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:13,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:13,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:14,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:14,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:14,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,098 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:16,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:16,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:16,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,328 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:18,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:18,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:18,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:18,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:18,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:18,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:18,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:19,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:19,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:19,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:20,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:20,612 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:21,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:21,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:21,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:22,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:24,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:26,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:26,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:26,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:26,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:26,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:26,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:27,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:27,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:27,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:27,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:27,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:27,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:27,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:27,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:27,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,895 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:30,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:30,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:31,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:46,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:47,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:47,458 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:47,458 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:48,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:48,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:48,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:48,608 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:48,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:48,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:48,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:48,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:49,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:49,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:49,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:49,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:49,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:49,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:49,850 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:50,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:50,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:50,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:51,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:51,165 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:51,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:51,770 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:55,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:56,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:56,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:56,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:56,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:58,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:00,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:01,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:01,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:01,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:03,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:03,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:04,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:04,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:04,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:04,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:05,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:05,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:05,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:05,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:05,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:05,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:05,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:05,603 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:05,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:06,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:07,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:07,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:07,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:07,908 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:08,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:08,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:08,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:09,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:20,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:21,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:21,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:21,373 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:21,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:21,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:21,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:21,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:21,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:28,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:31,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:31,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:31,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:31,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:34,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:34,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:35,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,718 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:38,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:38,395 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:38,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:38,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,248 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,430 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,865 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:41,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:43,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:44,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:45,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:46,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:47,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:48,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:01,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:01,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:01,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:02,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:02,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:02,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:02,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:02,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:02,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:02,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:02,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:02,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:02,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:03,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:03,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:03,922 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:04,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:04,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:04,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:04,396 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:04,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:04,678 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:04,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:05,431 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:05,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:05,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:05,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:05,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:05,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:05,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:05,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:06,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:06,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:09,678 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:11,239 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:12,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:13,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:13,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:13,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:13,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,794 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:15,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:15,771 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:15,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:16,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:16,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,275 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,384 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:19,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:20,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:20,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:20,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:20,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:23,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:28,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:29,985 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:31,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:31,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:32,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:32,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:32,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:34,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:34,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:34,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:34,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,831 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:44571. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,831 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:38165. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,831 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:43757. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,831 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:38069. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,831 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:46335. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,831 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,831 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,831 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,832 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:40035. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,832 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:35253. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,832 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:42251. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,832 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:40765. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,832 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:40465. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,832 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:36979. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,832 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:44085. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,831 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,833 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:45669. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,831 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,833 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:43831. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,831 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,831 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,831 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,833 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:42511. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,831 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,833 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:41979. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,833 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:45297. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,831 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,833 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:45181. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,833 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:35297. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,833 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:35817. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,833 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:39317. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,833 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:36599. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,833 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:42341. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,829 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.7:59910 remote=tcp://10.6.102.1:8786>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-03 16:42:34,834 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:41411. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,834 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:38189. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,834 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:40093. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,834 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:32897. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,834 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:43909. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,836 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,836 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:42893. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,842 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:33547'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,844 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2857, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,845 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2862, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,845 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,845 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,845 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,846 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,846 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,849 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:42825'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,850 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:42527'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,850 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:40703'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,850 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 8214, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,851 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:38461'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,851 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2203, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,851 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:40315'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,851 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,851 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1518, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,852 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:37105'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,852 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,852 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.35:46777, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,852 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2981, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,852 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2709, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,852 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.4:38153, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,852 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,852 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.2:38501, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,852 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,852 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:39229'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,852 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2325, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,852 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.22:39565, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,852 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,852 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1230, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,852 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1417, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,852 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,852 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:46343'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,852 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.9:36513, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,853 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,853 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,853 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3037, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,853 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,853 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,853 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:39847'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,853 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,853 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1822, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,853 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,853 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,853 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,853 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,853 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1821, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,853 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,853 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,853 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,853 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,853 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,853 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,853 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,853 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2218, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,853 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2193, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,854 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,854 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1692, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,854 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,854 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2062, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,854 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,854 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,854 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2949, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,854 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,854 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,854 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3315, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,854 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,854 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,860 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:35523'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,860 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:34223'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,860 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:35103'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,861 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:33575'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,861 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:44641'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,861 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:43615'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,861 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:36259'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,861 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3028, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,862 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1423, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,862 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3235, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,862 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:36413'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,862 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1421, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,862 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3225, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,862 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:41729'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,862 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2716, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,862 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3226, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,862 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:39619'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,862 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.10:37795, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,862 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1629, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,862 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2296, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,863 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.35:46777, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,863 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:34713'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,863 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1700, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,863 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2306, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,863 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.6:34561, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,863 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:43777'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,863 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.7:41799, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,863 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 8567, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,863 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:36809'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,863 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2156, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,863 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3656, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,863 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:34017'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,864 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:33183'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,864 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.34:35903, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,864 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2948, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,864 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:40409'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,864 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1169, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,864 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1456, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,864 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1463, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,864 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.23:44917, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,864 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:33379'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,864 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1458, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,864 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.4:35269, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,865 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,865 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,865 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,865 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.6:35109, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,865 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:33599'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,865 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1613, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,865 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,865 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,865 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,865 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,865 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3547, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,865 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.16:38355, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,865 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,865 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,865 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,865 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,865 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,865 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,865 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2867, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,865 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,865 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3549, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,865 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,865 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.5:40763, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,865 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,865 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,865 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,865 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,865 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,866 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,866 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,866 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,866 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,866 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2068, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,866 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.23:38869, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,866 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2728, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,866 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.7:41799, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,867 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,867 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,867 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,867 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,867 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,867 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,867 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,867 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,867 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,867 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,867 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,867 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,868 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,868 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,868 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,868 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,868 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,868 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,868 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3725, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,868 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,868 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,868 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,868 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1167, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,868 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,869 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,869 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,869 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,869 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:39533'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,869 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,869 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.8:46353, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,870 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 5506, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,870 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.4:43251, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,870 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 8306, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,870 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.15:44845, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,870 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,870 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1937, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,870 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1935, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,872 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,872 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,872 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,872 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,872 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,873 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,873 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,873 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,873 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,884 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14e9a3f09b90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:34,886 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14b1000b0290>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:34,898 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:35,001 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:35,001 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:35,002 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:41699. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,001 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:35,002 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:37309. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,002 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:44061. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,013 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:35,014 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:35,014 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:35,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:35,020 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59550 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:35,021 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.7:44061 -> tcp://10.6.102.8:33299
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.7:44061 remote=tcp://10.6.102.8:55458>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-03 16:42:35,022 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59776 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:35,024 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:46815'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,024 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:35,025 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:35,025 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:35,025 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:35,025 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:35,025 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:45689'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,026 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:35,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:35,027 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:35,027 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:35,027 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:35,027 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:35,032 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59884 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:35,031 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x151ed7422cd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:35,034 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:38397'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,034 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1457cd310a90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:35,039 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:35,040 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:35,040 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:35,040 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:35,040 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:35,040 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:35,043 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:35,045 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14d476f60fd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:35,053 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:35,087 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:35,257 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.7:44085 -> tcp://10.6.102.24:35261
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.7:44085 remote=tcp://10.6.102.24:40116>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-03 16:42:35,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:35,452 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:35,454 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:40343. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,458 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:35,463 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.102.7:37309
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1016, in send_recv
    await comm.write(msg, serializers=serializers, on_error="raise")
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.102.7:37552 remote=tcp://10.6.102.7:37309>: Stream is closed
2025-09-03 16:42:35,467 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:35,467 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:40615. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:35,477 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59654 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59654 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:35,477 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59642 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:35,481 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:42877'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,482 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.20:43319, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:35,482 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.1:40895, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:35,483 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:35,483 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:40487'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,483 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:35,483 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:35,483 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:35,483 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:35,484 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:35,484 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:35,484 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:35,484 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:35,484 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:35,489 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14fd07ec6f50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:35,493 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x145fc19a2e10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:35,498 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:35,503 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:35,568 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:35,570 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:35663. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:35,593 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59728 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:35,597 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:41409'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,598 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:35,598 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:35,598 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:35,598 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:35,598 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:35,605 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:35,614 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,049 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,050 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:40629. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,057 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,058 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:46019. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,069 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59788 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59788 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:36,073 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:33865'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,071 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59518 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:36,074 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,074 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,074 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,074 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,074 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,076 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:42887'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,076 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,077 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,077 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,077 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,077 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,081 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14ac886a28d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,084 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1496780d0d50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,090 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,094 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,478 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,479 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,481 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,506 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,506 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,509 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:37409. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,509 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:35615. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,512 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,531 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59706 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:36,534 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59846 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:36,536 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:40333'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,538 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:44781'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,539 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,539 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,539 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,539 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,539 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,543 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,543 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,543 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,543 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,543 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,546 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x150c2d275910>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,548 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15449fd39110>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,555 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,556 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,656 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,658 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:36125. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,659 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,681 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59866 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:36,685 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:37717'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,686 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,686 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,686 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,686 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,686 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,691 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x149d4f014510>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,698 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,845 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.7:44085 -> tcp://10.6.102.25:39963
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.7:44085 remote=tcp://10.6.102.25:46372>: Stream is closed
2025-09-03 16:42:36,901 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:36,922 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,006 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,008 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,017 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,018 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,019 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,021 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,021 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:38109. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,042 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,045 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,046 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59620 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:37,049 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:40565'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,050 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,050 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,050 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,051 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,051 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,055 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,055 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x154c95a5ded0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,061 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,090 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,163 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,164 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:37425. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,165 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,174 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59656 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59656 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:37,178 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:36601'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,179 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,179 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,179 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,179 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,179 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,183 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x147f22fce890>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,189 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,277 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,385 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,386 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:42621. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,388 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,389 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:38665. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,388 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,390 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:38823. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,393 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,400 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,401 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:41879. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,403 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,403 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:45741. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,401 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.102.14:45689
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.102.7:57764 remote=tcp://10.6.102.14:45689>: Stream is closed
2025-09-03 16:42:37,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,406 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.102.14:43905
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.102.7:35858 remote=tcp://10.6.102.14:43905>: Stream is closed
2025-09-03 16:42:37,408 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.102.14:33567
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.102.7:36726 remote=tcp://10.6.102.14:33567>: Stream is closed
2025-09-03 16:42:37,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,409 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.102.9:36513
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.102.7:45092 remote=tcp://10.6.102.9:36513>: Stream is closed
2025-09-03 16:42:37,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,411 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59752 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59752 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:37,412 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59648 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59648 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:37,414 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59478 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:37,414 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59570 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:37,413 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.102.3:36579
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 231, in read
    buffer = await read_bytes_rw(stream, buffer_nbytes)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 367, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.102.7:57026 remote=tcp://10.6.102.3:36579>: Stream is closed
2025-09-03 16:42:37,415 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:37291'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,415 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:35613'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,416 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,416 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,416 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:38015'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,416 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,416 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,416 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,417 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:34703'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,417 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.6:42895, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:37,417 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,417 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,417 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,417 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,417 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,419 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.9:33465, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:37,419 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,419 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,419 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,420 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,420 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,419 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59676 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:37,422 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,422 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:38881'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,422 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,422 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,422 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,422 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,422 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.14:36709, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:37,423 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,423 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,423 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,423 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,423 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,421 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1542165a1350>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,421 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14c92d33d250>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,424 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14da530d0690>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,426 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x152b32e21410>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,426 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1463a6ec2650>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,427 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,428 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,429 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,431 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,434 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,462 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,475 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:38397'. Reason: nanny-close-gracefully
2025-09-03 16:42:37,478 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:38397' closed.
2025-09-03 16:42:37,492 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:39619'. Reason: nanny-close-gracefully
2025-09-03 16:42:37,495 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:34713'. Reason: nanny-close-gracefully
2025-09-03 16:42:37,496 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:33379'. Reason: nanny-close-gracefully
2025-09-03 16:42:37,497 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:39619' closed.
2025-09-03 16:42:37,498 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:34713' closed.
2025-09-03 16:42:37,499 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:33379' closed.
2025-09-03 16:42:37,500 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,506 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,519 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,528 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,558 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:45689'. Reason: nanny-close-gracefully
2025-09-03 16:42:37,568 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:45689' closed.
2025-09-03 16:42:37,568 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:42527'. Reason: nanny-close-gracefully
2025-09-03 16:42:37,569 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:46815'. Reason: nanny-close-gracefully
2025-09-03 16:42:37,570 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:42527' closed.
2025-09-03 16:42:37,570 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:46815' closed.
2025-09-03 16:42:37,617 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,640 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,697 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,835 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,873 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:33547'. Reason: nanny-close-gracefully
2025-09-03 16:42:37,874 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:33547' closed.
2025-09-03 16:42:37,922 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,953 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,955 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:44249. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,957 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,976 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.102.14:34765
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.102.7:57034 remote=tcp://10.6.102.14:34765>: Stream is closed
2025-09-03 16:42:37,979 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,983 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59712 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:37,986 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:37311'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,987 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.24:43191, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:37,987 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,987 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,987 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,987 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,987 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,986 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,988 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:42469. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,990 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x153912c2d2d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,996 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,000 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:40487'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,001 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:40487' closed.
2025-09-03 16:42:38,005 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.7:42469 -> tcp://10.6.102.16:40087
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.7:42469 remote=tcp://10.6.102.16:41004>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-03 16:42:38,008 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:42877'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,009 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:42877' closed.
2025-09-03 16:42:38,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:38,016 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59768 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:38,019 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:45787'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,020 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:38,020 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:38,020 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:38,020 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:38,020 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:38,023 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1477a14d4ed0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:38,028 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,092 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,096 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,128 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:41409'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,128 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:38,129 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:38615. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,130 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:41409' closed.
2025-09-03 16:42:38,135 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:38,139 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59634 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59634 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:38,143 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:40459'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,147 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.8:34133, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:38,147 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.3:39291, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:38,147 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:38,147 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:38,148 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:38,148 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:38,148 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:38,151 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x154c05485150>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:38,156 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,352 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,376 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:38,376 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:45577. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:38,389 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59600 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:38,399 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:45641'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,400 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3122, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:38,400 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:38,400 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:38,400 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:38,400 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:38,400 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:38,403 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x152285fc4910>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:38,482 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,484 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,485 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,517 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,520 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:38,521 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.7:41799. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,529 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:33865'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,530 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:33865' closed.
2025-09-03 16:42:38,535 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:42887'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,536 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:42887' closed.
2025-09-03 16:42:38,536 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.7:41799 -> tcp://10.6.102.7:41979
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.7:41799 remote=tcp://10.6.102.7:59888>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-03 16:42:38,543 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:38,546 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.7:59854 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:38,549 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.7:38625'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,550 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:38,550 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:38,550 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:38,550 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:38,550 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:38,553 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14f302b7eb90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:38,558 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,558 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,559 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,593 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,662 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,701 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,748 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,897 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,925 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,002 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:39533'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,003 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:39533' closed.
2025-09-03 16:42:39,009 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,011 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,026 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,033 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,065 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,068 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:36413'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,069 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:36413' closed.
2025-09-03 16:42:39,071 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:43777'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,072 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:43777' closed.
2025-09-03 16:42:39,078 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:42825'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,079 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:42825' closed.
2025-09-03 16:42:39,085 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,106 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:44781'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,109 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:44781' closed.
2025-09-03 16:42:39,143 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:37717'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,145 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:37717' closed.
2025-09-03 16:42:39,160 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:40333'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,161 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:40333' closed.
2025-09-03 16:42:39,169 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,191 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,202 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:35103'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,204 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:35103' closed.
2025-09-03 16:42:39,280 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,396 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,430 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,430 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,431 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,434 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,437 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,438 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:39229'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,439 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:39229' closed.
2025-09-03 16:42:39,445 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:38461'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,446 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:38461' closed.
2025-09-03 16:42:39,474 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:36259'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,475 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:36259' closed.
2025-09-03 16:42:39,522 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,532 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,537 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:39847'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,538 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:39847' closed.
2025-09-03 16:42:39,564 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:40565'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,565 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:40565' closed.
2025-09-03 16:42:39,621 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:36601'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,622 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:36601' closed.
2025-09-03 16:42:39,643 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:33575'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,644 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,644 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:33575' closed.
2025-09-03 16:42:39,701 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,837 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:40315'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,838 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:40315' closed.
2025-09-03 16:42:39,839 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,883 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:35613'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,891 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:35613' closed.
2025-09-03 16:42:39,901 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:38881'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,902 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:38881' closed.
2025-09-03 16:42:39,910 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:33599'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,911 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:33599' closed.
2025-09-03 16:42:39,926 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,948 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:40703'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,950 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:40703' closed.
2025-09-03 16:42:39,961 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,996 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:35523'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,997 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:35523' closed.
2025-09-03 16:42:39,998 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,028 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:37291'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,029 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:37291' closed.
2025-09-03 16:42:40,031 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,065 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:38015'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,066 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:38015' closed.
2025-09-03 16:42:40,136 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:34703'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,137 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:34703' closed.
2025-09-03 16:42:40,144 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:46343'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,145 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:46343' closed.
2025-09-03 16:42:40,156 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:33183'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,157 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:33183' closed.
2025-09-03 16:42:40,159 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,351 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:41729'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,352 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:41729' closed.
2025-09-03 16:42:40,356 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,380 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:34017'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,382 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:34017' closed.
2025-09-03 16:42:40,492 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:45787'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,493 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:45787' closed.
2025-09-03 16:42:40,495 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:43615'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,496 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:43615' closed.
2025-09-03 16:42:40,514 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:37311'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,515 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:37311' closed.
2025-09-03 16:42:40,560 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,596 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,692 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:40459'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,694 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:40459' closed.
2025-09-03 16:42:40,752 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,796 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:34223'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,796 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:34223' closed.
2025-09-03 16:42:40,902 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,978 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:38625'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,979 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:38625' closed.
2025-09-03 16:42:41,019 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:45641'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,020 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:45641' closed.
2025-09-03 16:42:41,038 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,089 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,205 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:37105'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,206 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:37105' closed.
2025-09-03 16:42:41,365 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:44641'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,366 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:44641' closed.
2025-09-03 16:42:41,513 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:40409'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,513 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:40409' closed.
2025-09-03 16:42:41,538 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.7:36809'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,539 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.7:36809' closed.
2025-09-03 16:42:41,541 - distributed.dask_worker - INFO - End worker
