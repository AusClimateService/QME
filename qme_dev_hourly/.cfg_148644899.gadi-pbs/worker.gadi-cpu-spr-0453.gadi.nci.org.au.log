Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 16:27:16,539 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:42595'
2025-09-03 16:27:16,546 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:36219'
2025-09-03 16:27:16,551 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:45287'
2025-09-03 16:27:16,555 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:43453'
2025-09-03 16:27:16,559 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:41317'
2025-09-03 16:27:16,562 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:46207'
2025-09-03 16:27:16,569 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:46307'
2025-09-03 16:27:16,572 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:36195'
2025-09-03 16:27:16,576 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:33361'
2025-09-03 16:27:16,581 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:38641'
2025-09-03 16:27:16,585 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:42161'
2025-09-03 16:27:16,589 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:34241'
2025-09-03 16:27:16,593 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:37257'
2025-09-03 16:27:16,598 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:35863'
2025-09-03 16:27:16,602 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:33651'
2025-09-03 16:27:16,607 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:40367'
2025-09-03 16:27:16,611 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:44693'
2025-09-03 16:27:16,617 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:38329'
2025-09-03 16:27:16,621 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:45839'
2025-09-03 16:27:16,626 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:43557'
2025-09-03 16:27:16,705 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:38867'
2025-09-03 16:27:16,709 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:37125'
2025-09-03 16:27:16,714 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:43695'
2025-09-03 16:27:16,719 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:36777'
2025-09-03 16:27:16,723 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:46619'
2025-09-03 16:27:16,727 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:39457'
2025-09-03 16:27:16,732 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:46103'
2025-09-03 16:27:16,737 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:36003'
2025-09-03 16:27:16,741 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:43451'
2025-09-03 16:27:16,746 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:33689'
2025-09-03 16:27:16,750 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:39961'
2025-09-03 16:27:16,754 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:40397'
2025-09-03 16:27:16,758 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:37921'
2025-09-03 16:27:16,762 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:44677'
2025-09-03 16:27:16,767 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:35987'
2025-09-03 16:27:16,771 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:41405'
2025-09-03 16:27:16,776 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:34423'
2025-09-03 16:27:16,781 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:34743'
2025-09-03 16:27:16,785 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:46705'
2025-09-03 16:27:16,789 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:36103'
2025-09-03 16:27:16,794 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:44987'
2025-09-03 16:27:16,797 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:37155'
2025-09-03 16:27:16,800 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:33243'
2025-09-03 16:27:16,806 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:42113'
2025-09-03 16:27:16,812 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:36437'
2025-09-03 16:27:16,817 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:32969'
2025-09-03 16:27:16,821 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:36953'
2025-09-03 16:27:16,825 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:34451'
2025-09-03 16:27:16,829 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:37569'
2025-09-03 16:27:16,833 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:34175'
2025-09-03 16:27:16,838 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:35703'
2025-09-03 16:27:16,843 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.21:36755'
2025-09-03 16:27:17,868 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:43175
2025-09-03 16:27:17,868 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:43457
2025-09-03 16:27:17,868 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:37427
2025-09-03 16:27:17,868 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:34201
2025-09-03 16:27:17,868 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:45781
2025-09-03 16:27:17,868 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:43175
2025-09-03 16:27:17,868 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:46113
2025-09-03 16:27:17,868 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:42785
2025-09-03 16:27:17,868 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:34467
2025-09-03 16:27:17,868 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:35907
2025-09-03 16:27:17,868 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:43457
2025-09-03 16:27:17,868 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:39165
2025-09-03 16:27:17,868 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:35855
2025-09-03 16:27:17,868 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:40747
2025-09-03 16:27:17,868 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:44193
2025-09-03 16:27:17,868 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:33739
2025-09-03 16:27:17,868 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:37427
2025-09-03 16:27:17,868 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:34201
2025-09-03 16:27:17,868 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:45781
2025-09-03 16:27:17,868 - distributed.worker - INFO -          dashboard at:          10.6.102.21:36105
2025-09-03 16:27:17,868 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:46113
2025-09-03 16:27:17,868 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:42785
2025-09-03 16:27:17,868 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:34467
2025-09-03 16:27:17,868 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:35907
2025-09-03 16:27:17,868 - distributed.worker - INFO -          dashboard at:          10.6.102.21:40765
2025-09-03 16:27:17,868 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:39165
2025-09-03 16:27:17,868 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:35855
2025-09-03 16:27:17,868 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:40747
2025-09-03 16:27:17,868 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:44193
2025-09-03 16:27:17,868 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:33739
2025-09-03 16:27:17,868 - distributed.worker - INFO -          dashboard at:          10.6.102.21:36439
2025-09-03 16:27:17,868 - distributed.worker - INFO -          dashboard at:          10.6.102.21:46421
2025-09-03 16:27:17,868 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,868 - distributed.worker - INFO -          dashboard at:          10.6.102.21:45253
2025-09-03 16:27:17,868 - distributed.worker - INFO -          dashboard at:          10.6.102.21:36917
2025-09-03 16:27:17,868 - distributed.worker - INFO -          dashboard at:          10.6.102.21:35831
2025-09-03 16:27:17,868 - distributed.worker - INFO -          dashboard at:          10.6.102.21:38651
2025-09-03 16:27:17,868 - distributed.worker - INFO -          dashboard at:          10.6.102.21:40511
2025-09-03 16:27:17,868 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,868 - distributed.worker - INFO -          dashboard at:          10.6.102.21:40843
2025-09-03 16:27:17,868 - distributed.worker - INFO -          dashboard at:          10.6.102.21:41077
2025-09-03 16:27:17,868 - distributed.worker - INFO -          dashboard at:          10.6.102.21:36637
2025-09-03 16:27:17,868 - distributed.worker - INFO -          dashboard at:          10.6.102.21:35927
2025-09-03 16:27:17,868 - distributed.worker - INFO -          dashboard at:          10.6.102.21:34105
2025-09-03 16:27:17,868 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,868 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,868 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,868 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,868 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,868 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,868 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,868 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,868 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,868 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,868 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,868 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,868 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,868 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:17,868 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,868 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,868 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,868 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,868 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:36129
2025-09-03 16:27:17,869 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:17,869 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:17,869 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:17,869 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:17,869 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:17,869 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:17,869 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:17,869 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:17,869 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:17,869 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:17,869 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:17,869 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:17,869 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:36129
2025-09-03 16:27:17,869 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:17,869 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-mvj_yao5
2025-09-03 16:27:17,869 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:17,869 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:17,869 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:17,869 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:17,869 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:17,869 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:17,869 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:17,869 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:17,869 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-nkw76azy
2025-09-03 16:27:17,869 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-zq9iu1zw
2025-09-03 16:27:17,869 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:17,869 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:17,869 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:17,869 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:17,869 - distributed.worker - INFO -          dashboard at:          10.6.102.21:40135
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-f5vgbxut
2025-09-03 16:27:17,869 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-d1kdg4km
2025-09-03 16:27:17,869 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-6cxnyirv
2025-09-03 16:27:17,869 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-xa55fl70
2025-09-03 16:27:17,869 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-1m5_xl25
2025-09-03 16:27:17,869 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-wj_3s3vj
2025-09-03 16:27:17,869 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-xyg_qo0i
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-752x_alv
2025-09-03 16:27:17,869 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-mda4tpsw
2025-09-03 16:27:17,869 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-vyz0os_3
2025-09-03 16:27:17,869 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,869 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-lurplcg_
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,869 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:17,869 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:17,869 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-9h8o53ku
2025-09-03 16:27:17,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,870 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:39713
2025-09-03 16:27:17,870 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:39713
2025-09-03 16:27:17,870 - distributed.worker - INFO -          dashboard at:          10.6.102.21:36977
2025-09-03 16:27:17,870 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,870 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,870 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:17,870 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:17,870 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-oo1pts8q
2025-09-03 16:27:17,870 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,876 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:45987
2025-09-03 16:27:17,876 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:45987
2025-09-03 16:27:17,876 - distributed.worker - INFO -          dashboard at:          10.6.102.21:37335
2025-09-03 16:27:17,876 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,876 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,876 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:17,876 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:17,876 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-3h3o4akq
2025-09-03 16:27:17,876 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,881 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:42989
2025-09-03 16:27:17,881 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:42989
2025-09-03 16:27:17,882 - distributed.worker - INFO -          dashboard at:          10.6.102.21:45809
2025-09-03 16:27:17,882 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,882 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,882 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:17,882 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:17,882 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-nmyryj18
2025-09-03 16:27:17,882 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,886 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:17,887 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,887 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,888 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:17,892 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:17,892 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,892 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,893 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:17,896 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:17,897 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,898 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,899 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:17,899 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:17,900 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,900 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,901 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:17,903 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:17,904 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,904 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,906 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:17,906 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:17,908 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,908 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,909 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:17,909 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:17,910 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,910 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,911 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:17,911 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:17,912 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,912 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,913 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:17,914 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:17,914 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,914 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,916 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:17,916 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:17,917 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,917 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,918 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:17,919 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:17,919 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,919 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,919 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:17,920 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,920 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,921 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:17,921 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:17,922 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:17,922 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,923 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:17,924 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,924 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:17,924 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,925 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:17,926 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:17,926 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,927 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,927 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:17,928 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,928 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,928 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:17,928 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:17,929 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,930 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,930 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:17,931 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:17,931 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:17,932 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,932 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,934 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:17,990 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:46139
2025-09-03 16:27:17,990 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:46139
2025-09-03 16:27:17,990 - distributed.worker - INFO -          dashboard at:          10.6.102.21:34789
2025-09-03 16:27:17,990 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:17,990 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:17,991 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:17,991 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:17,991 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-t05fayd6
2025-09-03 16:27:17,991 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,006 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,007 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,007 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,008 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,023 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:41055
2025-09-03 16:27:18,023 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:41055
2025-09-03 16:27:18,023 - distributed.worker - INFO -          dashboard at:          10.6.102.21:44029
2025-09-03 16:27:18,023 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,023 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,023 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,023 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,023 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-imr0qg59
2025-09-03 16:27:18,023 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,029 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:38989
2025-09-03 16:27:18,029 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:38989
2025-09-03 16:27:18,029 - distributed.worker - INFO -          dashboard at:          10.6.102.21:42285
2025-09-03 16:27:18,029 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,029 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,029 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,029 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,029 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-_lpc4nkm
2025-09-03 16:27:18,029 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,049 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,050 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,050 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,052 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,052 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:35373
2025-09-03 16:27:18,052 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:35373
2025-09-03 16:27:18,052 - distributed.worker - INFO -          dashboard at:          10.6.102.21:45789
2025-09-03 16:27:18,052 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,052 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,052 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,052 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,052 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-ct83cdlh
2025-09-03 16:27:18,052 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,056 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,057 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,057 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,059 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,072 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:43357
2025-09-03 16:27:18,072 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:43357
2025-09-03 16:27:18,072 - distributed.worker - INFO -          dashboard at:          10.6.102.21:38387
2025-09-03 16:27:18,072 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,072 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,072 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,072 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,072 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-n0ig_w8a
2025-09-03 16:27:18,072 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,081 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,082 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,082 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,084 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,090 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:44251
2025-09-03 16:27:18,090 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:44251
2025-09-03 16:27:18,090 - distributed.worker - INFO -          dashboard at:          10.6.102.21:38967
2025-09-03 16:27:18,091 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,091 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,091 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,091 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,091 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-v39g1eu8
2025-09-03 16:27:18,091 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,096 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,097 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,097 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,099 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,116 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:44581
2025-09-03 16:27:18,116 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:44581
2025-09-03 16:27:18,116 - distributed.worker - INFO -          dashboard at:          10.6.102.21:33105
2025-09-03 16:27:18,116 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,116 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,116 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,116 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,116 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-tbvxycr7
2025-09-03 16:27:18,116 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,121 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:34771
2025-09-03 16:27:18,121 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:34771
2025-09-03 16:27:18,121 - distributed.worker - INFO -          dashboard at:          10.6.102.21:41371
2025-09-03 16:27:18,121 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,121 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,121 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,121 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,121 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-4x6nnvm3
2025-09-03 16:27:18,121 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,122 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,123 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,123 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,125 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,134 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:35017
2025-09-03 16:27:18,134 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:35017
2025-09-03 16:27:18,134 - distributed.worker - INFO -          dashboard at:          10.6.102.21:38685
2025-09-03 16:27:18,134 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,134 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,134 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,134 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,134 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-845b05bl
2025-09-03 16:27:18,134 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,143 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,144 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,144 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,145 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,163 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,164 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,164 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,165 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,168 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,169 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,169 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,171 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,373 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:45585
2025-09-03 16:27:18,373 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:45585
2025-09-03 16:27:18,373 - distributed.worker - INFO -          dashboard at:          10.6.102.21:35593
2025-09-03 16:27:18,373 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,373 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,373 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,373 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,373 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-lvxsrq6n
2025-09-03 16:27:18,373 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,376 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:41153
2025-09-03 16:27:18,376 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:41153
2025-09-03 16:27:18,376 - distributed.worker - INFO -          dashboard at:          10.6.102.21:37001
2025-09-03 16:27:18,376 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,376 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,376 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,376 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,376 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-54_h7o95
2025-09-03 16:27:18,376 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,378 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:43347
2025-09-03 16:27:18,378 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:43347
2025-09-03 16:27:18,378 - distributed.worker - INFO -          dashboard at:          10.6.102.21:32961
2025-09-03 16:27:18,378 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,378 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,378 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,378 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,378 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-o7szdq11
2025-09-03 16:27:18,378 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,380 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:34219
2025-09-03 16:27:18,380 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:34219
2025-09-03 16:27:18,380 - distributed.worker - INFO -          dashboard at:          10.6.102.21:32907
2025-09-03 16:27:18,380 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,380 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,380 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,380 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,380 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-_vyekqf0
2025-09-03 16:27:18,380 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,383 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:37077
2025-09-03 16:27:18,384 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:37077
2025-09-03 16:27:18,384 - distributed.worker - INFO -          dashboard at:          10.6.102.21:45313
2025-09-03 16:27:18,384 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,384 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,384 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,384 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,384 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-zxrvmwzx
2025-09-03 16:27:18,384 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,384 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:39213
2025-09-03 16:27:18,384 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:39213
2025-09-03 16:27:18,384 - distributed.worker - INFO -          dashboard at:          10.6.102.21:32933
2025-09-03 16:27:18,384 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,384 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,384 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,384 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,384 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-dr_97af7
2025-09-03 16:27:18,384 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,384 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:42151
2025-09-03 16:27:18,385 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:42151
2025-09-03 16:27:18,385 - distributed.worker - INFO -          dashboard at:          10.6.102.21:33945
2025-09-03 16:27:18,385 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,385 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,385 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,385 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,385 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-usyg5wth
2025-09-03 16:27:18,385 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,390 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:37419
2025-09-03 16:27:18,390 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:37419
2025-09-03 16:27:18,390 - distributed.worker - INFO -          dashboard at:          10.6.102.21:41353
2025-09-03 16:27:18,390 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,390 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,390 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,390 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,391 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-dr_wmt46
2025-09-03 16:27:18,391 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,391 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:40345
2025-09-03 16:27:18,391 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:40345
2025-09-03 16:27:18,391 - distributed.worker - INFO -          dashboard at:          10.6.102.21:33505
2025-09-03 16:27:18,391 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,391 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,391 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,391 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,391 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-vydjjhly
2025-09-03 16:27:18,391 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:33253
2025-09-03 16:27:18,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:33253
2025-09-03 16:27:18,392 - distributed.worker - INFO -          dashboard at:          10.6.102.21:39881
2025-09-03 16:27:18,392 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,392 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,392 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,392 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,392 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-gbkspv3t
2025-09-03 16:27:18,392 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,393 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:46391
2025-09-03 16:27:18,393 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:46391
2025-09-03 16:27:18,393 - distributed.worker - INFO -          dashboard at:          10.6.102.21:45187
2025-09-03 16:27:18,393 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,393 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,393 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,393 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-re074xj4
2025-09-03 16:27:18,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,395 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:34239
2025-09-03 16:27:18,395 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:34239
2025-09-03 16:27:18,395 - distributed.worker - INFO -          dashboard at:          10.6.102.21:46759
2025-09-03 16:27:18,395 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,395 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,395 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,395 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,395 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-mtdn5qik
2025-09-03 16:27:18,395 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,396 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:44981
2025-09-03 16:27:18,396 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:44981
2025-09-03 16:27:18,396 - distributed.worker - INFO -          dashboard at:          10.6.102.21:46191
2025-09-03 16:27:18,396 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,396 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,397 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,397 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,397 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-8jq_v313
2025-09-03 16:27:18,397 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,397 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:38595
2025-09-03 16:27:18,397 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:38595
2025-09-03 16:27:18,397 - distributed.worker - INFO -          dashboard at:          10.6.102.21:38391
2025-09-03 16:27:18,397 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,397 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,397 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,397 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,397 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-5jz11ux1
2025-09-03 16:27:18,397 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,399 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:41323
2025-09-03 16:27:18,399 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,399 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:41323
2025-09-03 16:27:18,399 - distributed.worker - INFO -          dashboard at:          10.6.102.21:43845
2025-09-03 16:27:18,399 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,399 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,399 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,399 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,399 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-j8lm3t5u
2025-09-03 16:27:18,399 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,400 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,400 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,400 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:33939
2025-09-03 16:27:18,400 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:33939
2025-09-03 16:27:18,400 - distributed.worker - INFO -          dashboard at:          10.6.102.21:43329
2025-09-03 16:27:18,400 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,400 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,400 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,400 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,400 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-eth58xqt
2025-09-03 16:27:18,400 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,401 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:44151
2025-09-03 16:27:18,401 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:44151
2025-09-03 16:27:18,401 - distributed.worker - INFO -          dashboard at:          10.6.102.21:38111
2025-09-03 16:27:18,401 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,401 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,401 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,401 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,401 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-v89hx_lc
2025-09-03 16:27:18,401 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,402 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,402 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:40391
2025-09-03 16:27:18,402 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:40391
2025-09-03 16:27:18,402 - distributed.worker - INFO -          dashboard at:          10.6.102.21:44187
2025-09-03 16:27:18,402 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,402 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,402 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,402 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,402 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-918mmgdo
2025-09-03 16:27:18,402 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,403 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:46725
2025-09-03 16:27:18,403 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:46725
2025-09-03 16:27:18,403 - distributed.worker - INFO -          dashboard at:          10.6.102.21:32899
2025-09-03 16:27:18,403 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,403 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,403 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,403 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,403 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-4888e4eh
2025-09-03 16:27:18,403 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,404 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:38463
2025-09-03 16:27:18,404 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:38463
2025-09-03 16:27:18,404 - distributed.worker - INFO -          dashboard at:          10.6.102.21:33801
2025-09-03 16:27:18,405 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,405 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,405 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,405 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,405 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-i8b2djl5
2025-09-03 16:27:18,405 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,409 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,410 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,410 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,411 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:37913
2025-09-03 16:27:18,411 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:36653
2025-09-03 16:27:18,411 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:37913
2025-09-03 16:27:18,411 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:36653
2025-09-03 16:27:18,411 - distributed.worker - INFO -          dashboard at:          10.6.102.21:36345
2025-09-03 16:27:18,411 - distributed.worker - INFO -          dashboard at:          10.6.102.21:38543
2025-09-03 16:27:18,411 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,411 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,411 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,411 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,411 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,411 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,411 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,411 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,411 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-5mf4k9nm
2025-09-03 16:27:18,411 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-avhur0n_
2025-09-03 16:27:18,411 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,411 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,412 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,413 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:34481
2025-09-03 16:27:18,413 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:34481
2025-09-03 16:27:18,413 - distributed.worker - INFO -          dashboard at:          10.6.102.21:37565
2025-09-03 16:27:18,413 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,413 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,413 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,413 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,413 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-a9p9rlrl
2025-09-03 16:27:18,413 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,413 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,414 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,415 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,416 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:43795
2025-09-03 16:27:18,416 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:43795
2025-09-03 16:27:18,416 - distributed.worker - INFO -          dashboard at:          10.6.102.21:39187
2025-09-03 16:27:18,416 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,416 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,416 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,416 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,416 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-06kiqoz4
2025-09-03 16:27:18,416 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,416 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,417 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.21:34147
2025-09-03 16:27:18,418 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.21:34147
2025-09-03 16:27:18,418 - distributed.worker - INFO -          dashboard at:          10.6.102.21:41539
2025-09-03 16:27:18,418 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,418 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,418 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:18,418 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:18,418 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-wj2iz2k2
2025-09-03 16:27:18,418 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,418 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,419 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,419 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,421 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,422 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,423 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,423 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,424 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,425 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,426 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,426 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,428 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,428 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,429 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,429 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,431 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,431 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,432 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,432 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,433 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,434 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,434 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,434 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,436 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,436 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,437 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,437 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,438 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,439 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,439 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,439 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,441 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,441 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,443 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,443 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,443 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,444 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,444 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,444 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,445 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,446 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,446 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,446 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,448 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,457 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,458 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,458 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,459 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,459 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,460 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,460 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,461 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,461 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,462 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,462 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,462 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,463 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,463 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,463 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,464 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,465 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,466 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,466 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,466 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,467 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,467 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,467 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,468 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,469 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,469 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,469 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,470 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,471 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,471 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,471 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,472 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,473 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,473 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,473 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,473 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,474 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,474 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,475 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,476 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:18,476 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:18,477 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:18,477 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:18,479 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:35,628 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,628 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,628 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,628 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,628 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,628 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,628 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,629 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,629 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,629 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,629 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,630 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,630 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,630 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,630 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,630 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,630 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,630 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,630 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,630 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,630 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,630 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,630 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,630 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,630 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,631 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,631 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,631 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,631 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,631 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,631 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,631 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,631 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,631 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,631 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,631 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,631 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,632 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,632 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,632 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,632 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,632 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,632 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,632 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,632 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,632 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,632 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,632 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,632 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,633 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,632 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,633 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,633 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,633 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,633 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,633 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,633 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,633 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,633 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,633 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,633 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,633 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,631 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,633 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,633 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,633 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,633 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,634 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,634 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,634 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,632 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,634 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,634 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,634 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,634 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,634 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,634 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,635 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,633 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,635 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,635 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,635 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,635 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,635 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,635 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,634 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,636 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,636 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,636 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,636 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,634 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,636 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,637 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,637 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,635 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,637 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,640 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,640 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,643 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,643 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,643 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,644 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,645 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,653 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:38,303 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,303 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,304 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,304 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,304 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,304 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,304 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,304 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,304 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,305 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,305 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,305 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,305 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,305 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,305 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,305 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,305 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,305 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,305 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,306 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,306 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,306 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,306 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,306 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,306 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,306 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,307 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,307 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,307 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,307 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,307 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,307 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,307 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,307 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,307 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,307 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,307 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,307 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,307 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,307 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,308 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,307 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,307 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,308 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,307 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,306 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,308 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,308 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,308 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,308 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,308 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,308 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,308 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,309 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,309 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,309 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,309 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,309 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,308 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,307 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,309 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,309 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,309 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,309 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,308 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,308 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,309 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,309 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,309 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,309 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,309 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,311 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,309 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,311 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,309 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,310 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,310 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,310 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,309 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,310 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,310 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,310 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,310 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,310 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,310 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,310 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,307 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,311 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,311 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,311 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,311 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,311 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,311 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,312 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,312 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,312 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,312 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,312 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,312 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,312 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,313 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,313 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,313 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,314 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,700 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,700 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,700 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,700 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,700 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,700 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,700 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,701 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,701 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,701 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,701 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,701 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,701 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,702 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,702 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,702 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,702 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,702 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,702 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,702 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,702 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,702 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,703 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,703 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,703 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,703 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,703 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,703 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,703 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,703 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,704 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,704 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,704 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,704 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,704 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,704 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,704 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,705 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,705 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,704 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,705 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,705 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,705 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,705 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,705 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,705 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,705 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,705 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,705 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,705 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,705 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,705 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,705 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,705 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,706 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,706 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,706 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,706 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,706 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,706 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,706 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,706 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,706 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,706 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,706 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,706 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,706 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,706 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,707 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,707 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,707 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,707 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,707 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,707 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,707 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,707 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,707 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,708 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,708 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,708 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,708 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,708 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,709 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,709 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,709 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,709 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,709 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,709 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,709 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,709 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,710 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,710 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,710 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,710 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,710 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,710 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,710 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,710 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,710 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,710 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,710 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,710 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,711 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,711 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:39,142 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,142 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,142 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,143 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,143 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,143 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,143 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,143 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,143 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,143 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,143 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,144 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,144 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,144 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,144 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,144 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,144 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,144 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,144 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,145 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,145 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,145 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,145 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,145 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,145 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,145 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,145 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,145 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,145 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,145 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,145 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,145 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,145 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,145 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,145 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,145 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,145 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,145 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,146 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,146 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,146 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,146 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,146 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,146 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,146 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,146 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,146 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,146 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,146 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,146 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,146 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,146 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,147 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,147 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,147 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,147 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,147 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,147 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,147 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,147 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,147 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,147 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,147 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,147 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,147 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,147 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,147 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,147 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,147 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,147 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,147 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,148 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,148 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,148 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,148 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,148 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,148 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,148 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,148 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,149 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,149 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,149 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,149 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,149 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,149 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,149 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,149 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,149 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,150 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,150 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,150 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,150 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,150 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,150 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,150 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,151 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,151 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,151 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,151 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,151 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,151 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,151 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,151 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,152 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:30:59,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,548 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,568 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:47,678 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:49,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:49,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:49,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:49,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:50,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:50,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:50,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:50,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:50,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,718 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,721 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:52,270 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:52,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:52,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:52,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:52,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:52,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:54,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:54,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:55,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:56,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:56,238 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:56,239 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:57,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:57,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:57,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:58,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:58,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:59,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:59,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:00,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:00,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:00,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:00,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:01,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:01,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:01,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:03,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:03,860 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:04,721 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:04,723 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:04,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:06,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:08,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:09,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:10,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:10,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:11,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:11,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:11,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:11,979 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:11,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:12,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:12,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:12,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:12,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:12,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:13,128 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:13,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:14,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:14,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:14,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:14,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:15,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:15,342 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:15,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:16,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:16,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:16,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:16,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:16,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:17,068 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:19,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:19,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:20,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:22,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:24,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:24,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:24,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:24,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:24,318 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:26,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:34,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:34,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:35,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:35,953 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:36,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:36,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:36,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:37,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:37,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:37,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:38,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:38,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:39,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:39,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:39,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:39,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:39,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:40,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:40,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:40,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:41,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:41,980 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:41,981 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:41,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:42,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:42,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:42,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:43,140 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:43,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:43,802 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:44,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:44,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:44,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:44,318 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:44,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:44,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:45,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:46,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:46,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:48,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:49,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:50,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:01,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:02,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:02,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:02,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:02,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:02,619 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:02,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:03,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:03,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:03,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:03,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:03,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:04,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:04,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:04,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:04,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:04,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:04,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:07,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:07,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:07,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:07,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:09,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:10,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:10,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:10,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:10,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:11,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:12,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:16,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:17,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:18,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:18,860 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:18,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:19,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:19,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:21,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:21,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:21,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:21,342 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:21,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,345 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:23,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:23,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:23,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:24,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:24,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:24,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:25,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:25,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:25,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:26,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:26,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:27,844 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:27,847 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:28,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:29,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:29,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:31,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:31,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:32,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:32,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:32,613 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:37,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:37,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:38,466 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:56,038 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:56,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:56,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:56,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:05,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:08,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:09,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:09,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:09,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:09,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:09,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:09,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:09,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:10,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:10,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:10,342 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:10,527 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:10,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:11,100 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:11,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:11,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:12,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:12,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:12,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:12,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:12,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:12,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:12,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:12,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:12,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:13,430 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:13,431 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:13,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:14,135 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:14,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:14,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:15,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:15,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:16,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:16,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:17,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:18,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:18,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:20,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:20,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:20,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:21,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:21,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:33,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:33,748 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:33,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:33,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:33,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,963 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:35,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:35,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:35,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:35,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:36,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:36,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:36,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:36,521 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:36,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:36,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:36,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:36,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:37,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:38,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:38,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:41,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:41,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:41,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:42,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:42,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:42,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:43,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:43,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:44,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:47,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:47,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:51,014 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:52,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:52,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:53,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:53,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:54,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:54,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:54,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:54,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:54,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,308 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:56,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:56,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:56,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:57,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:58,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:59,838 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:59,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:00,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:01,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:01,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:01,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:01,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:04,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:05,466 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:10,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:10,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:10,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:11,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:11,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:12,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:12,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:12,426 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:12,754 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:13,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:13,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:13,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:13,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:13,634 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:13,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:13,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:13,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:13,865 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:13,865 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:13,908 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:14,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:15,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:15,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:15,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:16,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:16,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:16,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:16,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:17,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:18,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:19,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:19,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:19,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:19,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:21,692 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:23,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:23,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:25,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:25,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:25,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:26,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:30,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:30,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:31,255 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:31,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:32,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:34,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:34,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:34,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:34,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:35,444 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:35,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:35,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:35,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:37,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:37,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:37,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:37,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:37,458 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:37,459 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:37,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:38,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:39,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:40,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:40,055 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:40,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:40,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:40,634 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:40,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:41,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:41,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:41,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:43,865 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:45,081 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:45,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:45,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:50,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:50,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:50,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:50,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:51,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:51,786 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,604 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:53,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:54,396 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:54,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:54,400 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:54,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:54,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:54,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,100 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,287 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:57,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:57,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:57,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:57,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:57,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:57,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:57,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:58,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:58,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:59,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:00,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:00,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:02,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:06,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:06,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:13,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:14,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:15,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:15,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:15,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:15,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:15,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:16,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:16,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:17,081 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:17,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:17,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:17,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:17,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:17,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:17,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:17,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:17,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:17,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:18,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:19,752 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:21,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:21,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:22,034 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:22,868 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:22,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:22,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:22,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:22,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:23,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:24,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:25,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:25,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:25,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:27,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:28,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:32,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:33,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:33,445 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:33,445 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:33,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:33,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:34,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:34,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:34,767 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:34,771 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:35,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:35,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:35,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:35,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:35,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:35,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:35,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:36,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:36,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:36,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:36,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:37,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:37,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:38,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:38,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:38,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:38,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:38,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:39,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:40,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:42,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:43,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:43,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:43,298 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:44,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:57,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:59,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:59,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:59,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:00,190 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:00,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:00,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:00,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:01,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:01,092 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:01,254 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:01,255 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:02,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:02,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:02,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:02,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:03,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:03,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:03,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:03,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:05,613 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:05,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:05,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:05,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:06,355 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:06,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:06,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:06,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:06,495 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:06,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:08,068 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:08,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:08,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:08,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:09,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:12,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:16,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:28,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:28,556 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:28,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:29,238 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:29,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:29,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:29,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:31,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:31,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:31,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:31,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:37,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:37,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:38,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:38,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:38,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:38,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:40,770 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:40,784 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:40,917 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:41,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:41,613 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:41,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:41,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:41,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:41,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:43,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:43,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:43,847 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:43,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:46,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:46,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:48,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:48,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:48,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:48,608 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:49,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:55,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:55,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:55,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:55,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:55,895 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:55,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:55,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:56,619 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:56,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:56,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:56,626 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:56,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:56,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:56,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:56,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:56,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:57,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:57,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:57,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:57,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:58,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:58,097 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:58,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:00,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:02,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:03,981 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:04,301 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:04,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:04,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:06,044 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:06,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:06,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:06,384 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:06,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:07,097 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:07,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:11,321 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:11,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:11,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:11,858 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:11,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:11,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:12,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:12,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:13,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:14,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:14,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:14,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:15,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:15,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:15,612 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:17,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:17,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:17,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:18,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:22,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:37,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:37,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:42,270 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:42,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:52,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:53,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:53,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:53,617 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:53,691 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:54,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:54,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:54,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:54,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,612 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:56,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:56,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:56,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:56,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:56,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:56,741 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:57,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:57,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:57,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:57,823 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:57,830 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:57,830 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,691 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:59,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:59,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:59,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:59,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:59,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:59,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:59,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:01,929 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:06,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:07,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:28,823 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:29,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:30,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:30,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:30,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:30,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:30,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:30,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:31,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:31,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:31,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:32,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:32,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,622 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,868 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:34,678 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:34,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:34,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:35,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:35,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:35,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:35,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:35,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:35,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:35,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:35,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:35,761 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:35,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:36,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:36,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:37,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:37,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:37,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:38,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:38,165 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:38,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:38,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:39,495 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:40,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:52,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:52,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:53,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:54,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:54,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:54,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:54,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:54,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:56,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:56,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:56,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:56,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:56,548 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:56,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:56,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:56,980 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:58,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:58,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:58,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:58,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:58,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:58,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:59,310 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:59,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:00,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:00,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:01,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:01,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:02,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:04,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:10,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:11,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:11,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:12,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:12,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:13,128 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:14,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:14,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:14,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:14,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:14,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:14,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:16,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:16,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:16,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:16,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:16,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,469 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:18,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:18,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:18,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:19,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:19,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:19,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:19,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:19,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:19,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:19,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:19,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:20,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:21,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:24,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:26,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:26,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:27,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:27,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:27,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:27,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,794 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:30,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:30,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:33,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:42,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:46,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:46,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:46,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:46,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:46,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:47,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:47,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:47,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:48,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:49,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:49,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:49,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:49,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:49,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:49,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:49,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:50,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:50,802 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:51,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:51,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:51,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:52,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:54,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:56,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:57,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:57,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:59,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:59,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:59,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:59,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:00,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:01,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:02,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:02,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:02,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:02,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:02,568 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:02,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:02,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:02,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:02,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:04,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:05,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:05,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:05,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:06,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:06,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:07,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:07,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:07,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:07,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:09,321 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:20,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:20,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:20,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:21,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:27,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:29,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:35,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:35,228 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,035 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,786 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:38,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:38,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:38,925 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,090 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,100 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,483 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:42,165 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:42,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:42,321 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:42,634 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:42,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:43,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:43,556 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:43,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:43,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:43,847 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:43,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:44,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:47,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:47,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:49,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:50,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:56,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:01,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:01,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:01,492 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:01,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:03,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:03,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:03,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:03,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:03,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:03,187 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:03,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:03,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:03,860 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:04,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:04,187 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:04,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:04,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:05,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:05,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:05,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:05,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:05,176 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:05,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:05,604 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:05,692 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:06,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:06,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:06,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:08,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:09,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:10,337 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:12,128 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:12,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:13,067 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:13,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:13,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:13,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:13,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:13,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:13,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,381 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:15,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:15,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,691 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,985 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,067 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:19,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:19,301 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:20,360 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:28,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:28,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:30,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:30,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:30,984 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:31,459 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:31,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:31,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:32,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:32,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:32,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:32,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:34,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:34,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,818 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:46391. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,818 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:37913. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,818 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:33939. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,818 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:40391. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,818 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:37077. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,818 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:45585. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,818 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:43347. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,818 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,818 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:45987. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,819 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:36129. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,819 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:35017. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,819 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:45781. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,818 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,819 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:43457. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,819 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:43795. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,820 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:46725. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,820 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:39213. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,820 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:38595. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,820 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:35373. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,817 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,820 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:39713. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,818 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,820 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:42151. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,820 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:43357. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,820 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:34239. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,820 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:34201. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,820 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:35907. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,818 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,819 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,821 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:39165. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,819 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,821 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:33739. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,821 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:44981. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,821 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:44581. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,821 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:42785. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,821 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:41055. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,822 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:38989. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,822 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:46139. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,833 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:34423'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,836 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:35703'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,836 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1547, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,837 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1551, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,837 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,837 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,838 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,838 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,838 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,844 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:33243'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,844 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:36103'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,845 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:34175'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,845 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1868, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,845 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:46619'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,845 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1871, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,845 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1485, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,845 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:36003'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,846 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1535, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,846 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,846 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:35863'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,846 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,846 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,846 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:37257'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,846 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,846 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,846 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,846 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1276, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,846 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,846 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:45287'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,846 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,846 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.24:46073, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,846 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,847 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3048, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,847 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:36755'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,847 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,847 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.20:44009, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,847 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3578, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,847 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2531, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,847 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3481, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,847 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:46207'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,847 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1161, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,847 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2098, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,847 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1063, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,847 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:37569'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,847 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1799, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,847 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,847 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1599, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,847 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1620, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,847 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:37155'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,847 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,848 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.5:40209, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,848 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,848 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,848 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,848 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1794, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,848 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.16:40797, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,848 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,848 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:43695'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,848 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,848 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,848 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,848 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1164, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,848 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,848 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,848 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,848 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,848 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:46705'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,848 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,848 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,848 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,848 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2025, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,848 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,848 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3303, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,848 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,848 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,848 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,848 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,849 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,848 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,848 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,848 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2692, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,849 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:34451'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,849 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3636, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,849 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,849 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,849 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,849 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,849 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,849 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3218, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,849 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,849 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3976, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,849 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,849 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,849 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,849 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,849 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,849 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,849 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2408, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,849 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,849 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,849 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,849 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,850 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,850 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,850 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2102, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,850 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,850 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,850 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,850 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,850 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.21:37419, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,850 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,850 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,850 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,850 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1772, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,850 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,850 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.14:34765, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,850 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,850 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,850 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1771, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,850 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,850 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1744, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,850 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,850 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3510, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,851 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,851 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3255, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,851 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,851 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,851 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,852 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,852 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,852 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,852 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,852 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,852 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,852 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,856 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,856 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:36219'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,857 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,857 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:46307'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,857 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,857 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:46103'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,857 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,858 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:34241'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,858 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:33689'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,858 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:40367'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,858 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1503, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,858 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:43557'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,858 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.5:33521, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,859 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1165, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,859 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:40397'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,859 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:43453'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,859 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:45839'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,859 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1147, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,859 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 4023, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,859 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:44693'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,859 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1156, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,860 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1983, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,860 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:38867'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,860 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:44677'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,860 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1960, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,860 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:43451'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,860 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2502, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,860 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1532, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,860 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1961, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,860 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1163, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,861 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.14:34765, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,861 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1950, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,861 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1439, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,861 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1568, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,861 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2337, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,862 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,862 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,862 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,862 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1875, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,862 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,862 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,862 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,862 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1878, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,862 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,862 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,862 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,862 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,862 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,862 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,865 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1176, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,865 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1451, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,870 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,893 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1639, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,893 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1634, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,895 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2026, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,895 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3297, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,896 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,896 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1450, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,897 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.18:33903, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,897 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,897 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,897 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,897 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,898 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,898 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,898 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,898 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,898 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,900 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2895, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,900 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3305, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,901 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,901 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,901 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,901 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,901 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,903 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,904 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,904 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,904 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,904 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,911 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,913 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:33253. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,912 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.21:39213 -> tcp://10.6.102.20:43509
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.21:39213 remote=tcp://10.6.102.20:54796>: Stream is closed
2025-09-03 16:42:34,923 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:34,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:34,934 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:34,935 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:58028 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:34,945 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:37125'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,946 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,946 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,946 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,946 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,946 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,953 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x148edf8050d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:34,962 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:35,038 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.21:45781 -> tcp://10.6.102.24:36995
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.21:45781 remote=tcp://10.6.102.24:40014>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-03 16:42:35,297 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:35,379 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:35,382 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:42989. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:35,392 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:57862 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:57862 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:35,398 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:42161'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,399 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:35,399 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:35,399 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:35,399 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:35,399 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:35,414 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:35,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:35,528 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:35,531 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:34771. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,568 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:57922 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:35,574 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:37921'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,575 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:35,576 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:35,576 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:35,576 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:35,576 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:35,585 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x145cc850cc50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:35,595 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:35,675 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:35,675 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:44151. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,680 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.21:44151 -> tcp://10.6.102.14:44361
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.21:44151 remote=tcp://10.6.102.14:39254>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-03 16:42:35,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:35,686 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:58102 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:35,688 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:41405'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,688 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 8222, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:35,689 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:35,689 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:35,689 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:35,689 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:35,689 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:35,693 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1520fff75090>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:35,698 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:35,721 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.21:44981 -> tcp://10.6.102.22:33863
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.21:44981 remote=tcp://10.6.102.22:43764>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-03 16:42:35,858 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:35,859 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:41153. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:35,869 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:57940 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:57940 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:35,873 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:34743'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,874 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:35,874 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:35,874 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:35,874 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:35,874 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:35,881 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x149bc16bc290>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:35,889 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,066 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,186 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,188 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:37427. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,197 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:57736 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:57736 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:36,202 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:36195'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,202 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,203 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,203 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,203 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,203 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,210 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14884d7f6250>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,217 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,227 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,228 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:40747. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,232 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:57820 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:57820 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:36,234 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:41317'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,235 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2405, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:36,235 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,235 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,235 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,235 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,235 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,239 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x151b4eb2c0d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,244 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,258 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,501 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,503 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:44251. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,514 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,516 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:35855. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,521 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,526 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:57916 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:36,527 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:57798 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:57798 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:36,530 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:39457'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,531 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.9:41413, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:36,531 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.35:33593, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:36,532 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3180, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:36,532 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,532 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:42595'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,532 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,532 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,532 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,532 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,532 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,533 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,533 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,533 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,533 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,539 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14ef12626990>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,540 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x149d4650ba10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,540 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,543 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:37419. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,547 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,557 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.21:37419 -> tcp://10.6.102.21:46725
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.21:37419 remote=tcp://10.6.102.21:42856>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-03 16:42:36,569 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:58008 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:36,569 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,571 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:38463. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,573 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:39961'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,574 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,574 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,574 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,574 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,574 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,581 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x150204665490>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,582 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:58140 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:58140 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:36,588 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,595 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:42113'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,595 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,596 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,596 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,596 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,596 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,602 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x153e7f069850>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,609 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,685 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,763 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,764 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:34467. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,773 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:57784 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:57784 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:36,777 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:33361'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,778 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2937, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:36,779 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.36:41805, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:36,779 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,779 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,779 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,779 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,779 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,785 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,921 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,921 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,924 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,927 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:36,938 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:36,965 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,118 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,117 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,119 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:36653. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,119 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:46113. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,118 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,120 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:40345. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,120 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,122 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:43175. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,132 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.21:46113 -> tcp://10.6.102.11:40795
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.21:46113 remote=tcp://10.6.102.11:35754>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-03 16:42:37,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,137 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.21:40345 -> tcp://10.6.102.19:42045
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.21:40345 remote=tcp://10.6.102.19:58162>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-03 16:42:37,137 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.21:36653 -> tcp://10.6.102.19:42045
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.21:36653 remote=tcp://10.6.102.19:40212>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-03 16:42:37,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,142 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.21:40345 -> tcp://10.6.102.25:33423
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.21:40345 remote=tcp://10.6.102.25:39906>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-03 16:42:37,143 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:57722 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:37,144 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:57706 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:37,146 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:33651'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,146 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:58152 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:37,147 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,147 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,147 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,147 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,147 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,148 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:38329'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,149 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:36437'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,150 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,150 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,150 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,150 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,150 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,151 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:58020 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:37,152 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,152 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,152 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,152 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,152 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,153 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:32969'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,152 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14651f192a10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,154 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,154 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,154 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,154 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,154 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,154 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x152d0f9c5490>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,157 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,157 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14e31784f610>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,159 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1552cd713dd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,161 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,163 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,164 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,170 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,172 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:34481. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,181 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:58172 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:58172 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:37,188 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:36953'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,189 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3520, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:37,189 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,189 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,189 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,189 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,189 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,194 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x151b4cf91e90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,197 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,198 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:34147. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,207 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:58176 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:58176 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:37,211 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:36777'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,211 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.23:37829, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:37,212 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,212 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,212 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,212 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,212 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,216 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x146228d78750>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,222 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,240 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,301 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,418 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,425 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,468 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:37125'. Reason: nanny-close-gracefully
2025-09-03 16:42:37,470 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:37125' closed.
2025-09-03 16:42:37,484 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:43557'. Reason: nanny-close-gracefully
2025-09-03 16:42:37,486 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:44693'. Reason: nanny-close-gracefully
2025-09-03 16:42:37,486 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:43557' closed.
2025-09-03 16:42:37,486 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:44693' closed.
2025-09-03 16:42:37,557 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,598 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,701 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,742 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:40397'. Reason: nanny-close-gracefully
2025-09-03 16:42:37,744 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:40397' closed.
2025-09-03 16:42:37,878 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:42161'. Reason: nanny-close-gracefully
2025-09-03 16:42:37,879 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:42161' closed.
2025-09-03 16:42:37,891 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,021 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.21:34201 -> tcp://10.6.102.8:46811
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.21:34201 remote=tcp://10.6.102.8:40454>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-03 16:42:38,055 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:37921'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,056 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:37921' closed.
2025-09-03 16:42:38,069 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,090 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:38,093 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:41323. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,094 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,094 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:38,098 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:34219. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,106 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:41405'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,108 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:41405' closed.
2025-09-03 16:42:38,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:38,114 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,118 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:58078 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:38,122 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:35987'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,123 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:38,123 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:38,123 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:38,123 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:38,123 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:38,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:38,127 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15112b02d2d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:38,130 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:57962 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:38,132 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,137 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:44987'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,138 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:38,138 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:38,138 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:38,138 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:38,138 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:38,142 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x152a735b5e50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:38,149 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,219 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,247 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,262 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,326 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:34743'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,327 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:34743' closed.
2025-09-03 16:42:38,436 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,517 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:43695'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,517 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:43695' closed.
2025-09-03 16:42:38,550 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,591 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,612 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,651 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:41317'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,653 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:41317' closed.
2025-09-03 16:42:38,656 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:36195'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,656 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:36195' closed.
2025-09-03 16:42:38,688 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,692 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,733 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:36103'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,734 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:36103' closed.
2025-09-03 16:42:38,846 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,900 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,925 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,926 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,927 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,985 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,024 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:39961'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,027 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:39961' closed.
2025-09-03 16:42:39,059 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:42595'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,060 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:42595' closed.
2025-09-03 16:42:39,111 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:36003'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,112 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:36003' closed.
2025-09-03 16:42:39,129 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:42113'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,132 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:42113' closed.
2025-09-03 16:42:39,136 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,160 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,163 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,166 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,215 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,224 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,244 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,310 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,324 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,348 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:46619'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,350 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:46619' closed.
2025-09-03 16:42:39,363 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:34451'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,363 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:34451' closed.
2025-09-03 16:42:39,387 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:33361'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,388 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:33361' closed.
2025-09-03 16:42:39,389 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,405 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,407 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,429 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,448 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,470 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:39,473 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.21:44193. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:39,483 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,490 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.21:44193 -> tcp://10.6.102.15:40847
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.21:44193 remote=tcp://10.6.102.15:46706>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-03 16:42:39,494 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,494 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,498 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:39,502 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.21:57828 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:39,507 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.21:38641'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:39,508 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:39,508 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:39,508 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:39,508 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:39,508 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:39,510 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14a768f78b10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:39,514 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,538 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,561 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,595 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,610 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:38329'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,611 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:38329' closed.
2025-09-03 16:42:39,614 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,628 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:33651'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,629 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:33651' closed.
2025-09-03 16:42:39,685 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:32969'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,686 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:32969' closed.
2025-09-03 16:42:39,688 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:36777'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,689 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:36777' closed.
2025-09-03 16:42:39,708 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:36953'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,709 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:36953' closed.
2025-09-03 16:42:39,734 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:36437'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,734 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:36437' closed.
2025-09-03 16:42:39,890 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:39457'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,891 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:39457' closed.
2025-09-03 16:42:40,018 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:46207'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,019 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:46207' closed.
2025-09-03 16:42:40,097 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,118 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,135 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,440 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,573 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:37155'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,574 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:37155' closed.
2025-09-03 16:42:40,611 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:35987'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,612 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:35987' closed.
2025-09-03 16:42:40,695 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,796 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:44987'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,797 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:44987' closed.
2025-09-03 16:42:40,801 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:33689'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,801 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:33689' closed.
2025-09-03 16:42:40,850 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,881 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:46705'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,882 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:46705' closed.
2025-09-03 16:42:40,904 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,989 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,123 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:34175'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,125 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:34175' closed.
2025-09-03 16:42:41,140 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,218 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,275 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:34423'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,276 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:34423' closed.
2025-09-03 16:42:41,313 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,327 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,352 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:45839'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,353 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:45839' closed.
2025-09-03 16:42:41,393 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,409 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,410 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,444 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:34241'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,445 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:34241' closed.
2025-09-03 16:42:41,452 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,486 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,498 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,499 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,502 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,517 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,542 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,599 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,617 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,623 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:37257'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,624 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:37257' closed.
2025-09-03 16:42:41,689 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:33243'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,690 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:33243' closed.
2025-09-03 16:42:41,780 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:35863'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,781 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:35863' closed.
2025-09-03 16:42:41,828 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:35703'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,829 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:35703' closed.
2025-09-03 16:42:41,947 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:37569'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,947 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:37569' closed.
2025-09-03 16:42:41,972 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:46307'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,975 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:46307' closed.
2025-09-03 16:42:41,978 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:44677'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,979 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:44677' closed.
2025-09-03 16:42:41,984 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:36219'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,985 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:36219' closed.
2025-09-03 16:42:41,993 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:43453'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,994 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:43453' closed.
2025-09-03 16:42:42,042 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:40367'. Reason: nanny-close-gracefully
2025-09-03 16:42:42,043 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:40367' closed.
2025-09-03 16:42:42,075 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:38641'. Reason: nanny-close-gracefully
2025-09-03 16:42:42,076 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:38641' closed.
2025-09-03 16:42:42,126 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:45287'. Reason: nanny-close-gracefully
2025-09-03 16:42:42,127 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:46103'. Reason: nanny-close-gracefully
2025-09-03 16:42:42,128 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:45287' closed.
2025-09-03 16:42:42,129 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:46103' closed.
2025-09-03 16:42:42,136 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:38867'. Reason: nanny-close-gracefully
2025-09-03 16:42:42,137 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:38867' closed.
2025-09-03 16:42:42,266 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:43451'. Reason: nanny-close-gracefully
2025-09-03 16:42:42,267 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:43451' closed.
2025-09-03 16:42:42,288 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.21:36755'. Reason: nanny-close-gracefully
2025-09-03 16:42:42,289 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.21:36755' closed.
2025-09-03 16:42:42,291 - distributed.dask_worker - INFO - End worker
