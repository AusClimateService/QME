Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-05 09:11:36,441 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:43357'
2025-09-05 09:11:36,449 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:35233'
2025-09-05 09:11:36,453 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:37831'
2025-09-05 09:11:36,457 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:42429'
2025-09-05 09:11:36,461 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:34111'
2025-09-05 09:11:36,465 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:32943'
2025-09-05 09:11:36,469 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:38669'
2025-09-05 09:11:36,473 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:44199'
2025-09-05 09:11:36,477 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:41589'
2025-09-05 09:11:36,483 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:46495'
2025-09-05 09:11:36,488 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:43521'
2025-09-05 09:11:36,492 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:32795'
2025-09-05 09:11:36,497 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:39123'
2025-09-05 09:11:36,501 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:44897'
2025-09-05 09:11:36,505 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:37271'
2025-09-05 09:11:36,509 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:46653'
2025-09-05 09:11:36,513 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:37187'
2025-09-05 09:11:36,518 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:39187'
2025-09-05 09:11:36,522 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:42413'
2025-09-05 09:11:36,525 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:36057'
2025-09-05 09:11:36,528 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:39453'
2025-09-05 09:11:36,612 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:41833'
2025-09-05 09:11:36,617 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:44301'
2025-09-05 09:11:36,629 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:41223'
2025-09-05 09:11:36,632 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:32941'
2025-09-05 09:11:36,636 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:34027'
2025-09-05 09:11:36,641 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:39043'
2025-09-05 09:11:36,646 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:35623'
2025-09-05 09:11:36,651 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:37185'
2025-09-05 09:11:36,655 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:42691'
2025-09-05 09:11:36,660 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:44703'
2025-09-05 09:11:36,665 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:36329'
2025-09-05 09:11:36,669 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:44487'
2025-09-05 09:11:36,673 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:37489'
2025-09-05 09:11:36,677 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:40355'
2025-09-05 09:11:36,682 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:42785'
2025-09-05 09:11:36,686 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:43563'
2025-09-05 09:11:36,690 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:44671'
2025-09-05 09:11:36,694 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:37103'
2025-09-05 09:11:36,698 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:37455'
2025-09-05 09:11:36,703 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:35921'
2025-09-05 09:11:36,708 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:40255'
2025-09-05 09:11:36,712 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:43013'
2025-09-05 09:11:36,716 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:40995'
2025-09-05 09:11:36,721 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:41727'
2025-09-05 09:11:36,726 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:42269'
2025-09-05 09:11:36,730 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:41775'
2025-09-05 09:11:36,737 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:42829'
2025-09-05 09:11:36,740 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:46641'
2025-09-05 09:11:36,745 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:34805'
2025-09-05 09:11:36,750 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:44687'
2025-09-05 09:11:36,754 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.50:46235'
2025-09-05 09:11:37,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:33413
2025-09-05 09:11:37,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:40405
2025-09-05 09:11:37,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:37439
2025-09-05 09:11:37,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:33899
2025-09-05 09:11:37,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:38817
2025-09-05 09:11:37,672 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:33413
2025-09-05 09:11:37,672 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:40405
2025-09-05 09:11:37,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:38999
2025-09-05 09:11:37,672 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:37439
2025-09-05 09:11:37,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:45303
2025-09-05 09:11:37,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:34085
2025-09-05 09:11:37,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:42645
2025-09-05 09:11:37,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:37687
2025-09-05 09:11:37,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:42543
2025-09-05 09:11:37,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:40279
2025-09-05 09:11:37,673 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:33899
2025-09-05 09:11:37,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:39381
2025-09-05 09:11:37,673 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:38817
2025-09-05 09:11:37,673 - distributed.worker - INFO -          dashboard at:          10.6.101.50:35549
2025-09-05 09:11:37,673 - distributed.worker - INFO -          dashboard at:          10.6.101.50:44829
2025-09-05 09:11:37,673 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:38999
2025-09-05 09:11:37,673 - distributed.worker - INFO -          dashboard at:          10.6.101.50:40343
2025-09-05 09:11:37,673 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:45303
2025-09-05 09:11:37,673 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:34085
2025-09-05 09:11:37,673 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:42645
2025-09-05 09:11:37,673 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:37687
2025-09-05 09:11:37,673 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:42543
2025-09-05 09:11:37,673 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:40279
2025-09-05 09:11:37,673 - distributed.worker - INFO -          dashboard at:          10.6.101.50:41717
2025-09-05 09:11:37,673 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:39381
2025-09-05 09:11:37,673 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,673 - distributed.worker - INFO -          dashboard at:          10.6.101.50:43359
2025-09-05 09:11:37,673 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,673 - distributed.worker - INFO -          dashboard at:          10.6.101.50:42505
2025-09-05 09:11:37,673 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,673 - distributed.worker - INFO -          dashboard at:          10.6.101.50:40137
2025-09-05 09:11:37,673 - distributed.worker - INFO -          dashboard at:          10.6.101.50:44453
2025-09-05 09:11:37,673 - distributed.worker - INFO -          dashboard at:          10.6.101.50:40743
2025-09-05 09:11:37,673 - distributed.worker - INFO -          dashboard at:          10.6.101.50:33487
2025-09-05 09:11:37,673 - distributed.worker - INFO -          dashboard at:          10.6.101.50:40425
2025-09-05 09:11:37,673 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,673 - distributed.worker - INFO -          dashboard at:          10.6.101.50:43857
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO -          dashboard at:          10.6.101.50:41739
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,673 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,673 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,673 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,673 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,673 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:37,673 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,673 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:37,673 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:37,673 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-e7wo6hdt
2025-09-05 09:11:37,673 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:37,673 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:37,673 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:37,673 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:37,673 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:37,673 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:37,673 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:37,673 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:37,673 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:37,673 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:37,673 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:37,673 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-nlb9h7ib
2025-09-05 09:11:37,673 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:37,673 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-up0cnnv0
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ky808nqy
2025-09-05 09:11:37,673 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:37,673 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:37,673 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:37,673 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:37,673 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-54xspm2t
2025-09-05 09:11:37,673 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:37,673 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:37,673 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-8grhfir5
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-g8d0pmnb
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-c0zc2lam
2025-09-05 09:11:37,673 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-i8yfq_4k
2025-09-05 09:11:37,673 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-uanbcg93
2025-09-05 09:11:37,673 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-bl6js22x
2025-09-05 09:11:37,673 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-kgyf57np
2025-09-05 09:11:37,673 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-krr2r8hq
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,674 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,683 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:37433
2025-09-05 09:11:37,683 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:37433
2025-09-05 09:11:37,683 - distributed.worker - INFO -          dashboard at:          10.6.101.50:41631
2025-09-05 09:11:37,683 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,683 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,683 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:37,683 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:37,683 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-z8ghohmr
2025-09-05 09:11:37,684 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,687 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:33259
2025-09-05 09:11:37,687 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:33259
2025-09-05 09:11:37,688 - distributed.worker - INFO -          dashboard at:          10.6.101.50:44627
2025-09-05 09:11:37,688 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,688 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,688 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:37,688 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:37,688 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-izyi4gdm
2025-09-05 09:11:37,688 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,690 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:38521
2025-09-05 09:11:37,691 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:38521
2025-09-05 09:11:37,691 - distributed.worker - INFO -          dashboard at:          10.6.101.50:43385
2025-09-05 09:11:37,691 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,691 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,691 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:37,691 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:37,691 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-2spl0cy3
2025-09-05 09:11:37,691 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,691 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:42639
2025-09-05 09:11:37,691 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:42639
2025-09-05 09:11:37,691 - distributed.worker - INFO -          dashboard at:          10.6.101.50:42215
2025-09-05 09:11:37,691 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,691 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,691 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:37,691 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:37,691 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-pon4ia4f
2025-09-05 09:11:37,691 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,695 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:37,696 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,696 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,696 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:37,698 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:42017
2025-09-05 09:11:37,698 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:42017
2025-09-05 09:11:37,699 - distributed.worker - INFO -          dashboard at:          10.6.101.50:36065
2025-09-05 09:11:37,699 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,699 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,699 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:37,699 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:37,699 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-xddw6g1o
2025-09-05 09:11:37,699 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,700 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:37,700 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,700 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,701 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:37,705 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:37,706 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,706 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,707 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:37,716 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:37,717 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,717 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,718 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:37,718 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,718 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,719 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:37,719 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:37,720 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:37,721 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,721 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,722 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:37,723 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:37,723 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,723 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,724 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:37,725 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:37,725 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,725 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,726 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:37,727 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:37,727 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,727 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,728 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:37,729 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:37,729 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,729 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,729 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:37,731 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,731 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:37,731 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:37,731 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,732 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,732 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,732 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:37,733 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:37,733 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,733 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,734 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:37,734 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:37,735 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:37,735 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,735 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,737 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:37,752 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:37,753 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,753 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,755 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:37,756 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:37,757 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:37,757 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,757 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,757 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:37,758 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,758 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,758 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,759 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,759 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:37,760 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:37,760 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:37,794 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:45687
2025-09-05 09:11:37,794 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:45687
2025-09-05 09:11:37,794 - distributed.worker - INFO -          dashboard at:          10.6.101.50:43935
2025-09-05 09:11:37,794 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,794 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,794 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:37,794 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:37,794 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-iieiaes1
2025-09-05 09:11:37,794 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,816 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:37,817 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,817 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,819 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:37,868 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:44189
2025-09-05 09:11:37,868 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:44189
2025-09-05 09:11:37,868 - distributed.worker - INFO -          dashboard at:          10.6.101.50:40397
2025-09-05 09:11:37,868 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,868 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,868 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:37,869 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:37,869 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-n46s9jo1
2025-09-05 09:11:37,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,883 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:46067
2025-09-05 09:11:37,883 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:46067
2025-09-05 09:11:37,883 - distributed.worker - INFO -          dashboard at:          10.6.101.50:36981
2025-09-05 09:11:37,883 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,883 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,883 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:37,883 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:37,883 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-r6o86_lo
2025-09-05 09:11:37,883 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,897 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:37,898 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,898 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,900 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:37,909 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:37,909 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,909 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,911 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:37,939 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:38205
2025-09-05 09:11:37,939 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:38205
2025-09-05 09:11:37,939 - distributed.worker - INFO -          dashboard at:          10.6.101.50:46461
2025-09-05 09:11:37,939 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,939 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,939 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:37,939 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:37,940 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-2svx64st
2025-09-05 09:11:37,940 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,968 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:37,969 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:37,969 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:37,970 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,001 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:33279
2025-09-05 09:11:38,002 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:33279
2025-09-05 09:11:38,002 - distributed.worker - INFO -          dashboard at:          10.6.101.50:37543
2025-09-05 09:11:38,002 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,002 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,002 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,002 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,002 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-w_a2m_3c
2025-09-05 09:11:38,002 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,006 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:39851
2025-09-05 09:11:38,006 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:39851
2025-09-05 09:11:38,006 - distributed.worker - INFO -          dashboard at:          10.6.101.50:45929
2025-09-05 09:11:38,006 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,006 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,006 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,006 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,006 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-4w9z7zbv
2025-09-05 09:11:38,006 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,011 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:35229
2025-09-05 09:11:38,012 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:35229
2025-09-05 09:11:38,012 - distributed.worker - INFO -          dashboard at:          10.6.101.50:41771
2025-09-05 09:11:38,012 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,012 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,012 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,012 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,012 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-qnfnwa1d
2025-09-05 09:11:38,012 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,020 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:36721
2025-09-05 09:11:38,020 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:36721
2025-09-05 09:11:38,020 - distributed.worker - INFO -          dashboard at:          10.6.101.50:36507
2025-09-05 09:11:38,020 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,020 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,020 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,020 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,020 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-6p71pwtd
2025-09-05 09:11:38,020 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,023 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:43553
2025-09-05 09:11:38,023 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:43553
2025-09-05 09:11:38,023 - distributed.worker - INFO -          dashboard at:          10.6.101.50:44229
2025-09-05 09:11:38,023 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,023 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,023 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,023 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,023 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-j_i9wczi
2025-09-05 09:11:38,023 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,025 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,026 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,026 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,028 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,031 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:33781
2025-09-05 09:11:38,031 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:33781
2025-09-05 09:11:38,031 - distributed.worker - INFO -          dashboard at:          10.6.101.50:40579
2025-09-05 09:11:38,031 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,031 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,031 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,031 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,031 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-dyedzs17
2025-09-05 09:11:38,031 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,032 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,033 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,033 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,035 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,038 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,039 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,039 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,040 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,043 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,044 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,044 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,045 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,049 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,049 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,050 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,051 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,053 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:46827
2025-09-05 09:11:38,053 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:46827
2025-09-05 09:11:38,053 - distributed.worker - INFO -          dashboard at:          10.6.101.50:45205
2025-09-05 09:11:38,053 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,053 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,053 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,053 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,053 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-_t7v9_lr
2025-09-05 09:11:38,053 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,056 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:37007
2025-09-05 09:11:38,056 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:37007
2025-09-05 09:11:38,056 - distributed.worker - INFO -          dashboard at:          10.6.101.50:41413
2025-09-05 09:11:38,056 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,057 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,057 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,057 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,057 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-d4odzx4l
2025-09-05 09:11:38,057 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,059 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,060 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,060 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,061 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,064 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,064 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,064 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,065 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,077 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,078 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,078 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,079 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,081 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:33287
2025-09-05 09:11:38,081 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:33287
2025-09-05 09:11:38,081 - distributed.worker - INFO -          dashboard at:          10.6.101.50:38771
2025-09-05 09:11:38,081 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,081 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,081 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,081 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,081 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-lu62_tzo
2025-09-05 09:11:38,081 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,088 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:34201
2025-09-05 09:11:38,088 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:34201
2025-09-05 09:11:38,088 - distributed.worker - INFO -          dashboard at:          10.6.101.50:35177
2025-09-05 09:11:38,088 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,088 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,088 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,089 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,089 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ixbqatc3
2025-09-05 09:11:38,089 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,089 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:40811
2025-09-05 09:11:38,089 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:40811
2025-09-05 09:11:38,089 - distributed.worker - INFO -          dashboard at:          10.6.101.50:45017
2025-09-05 09:11:38,089 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,089 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,089 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,089 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,089 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-pfiss0a4
2025-09-05 09:11:38,089 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,091 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:40653
2025-09-05 09:11:38,091 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:40653
2025-09-05 09:11:38,092 - distributed.worker - INFO -          dashboard at:          10.6.101.50:44073
2025-09-05 09:11:38,092 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,092 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,092 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,092 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,092 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-cumbw4dc
2025-09-05 09:11:38,092 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,092 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:43747
2025-09-05 09:11:38,092 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:43747
2025-09-05 09:11:38,092 - distributed.worker - INFO -          dashboard at:          10.6.101.50:37137
2025-09-05 09:11:38,092 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,092 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,092 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,092 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,092 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-jwnrpa24
2025-09-05 09:11:38,092 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,104 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:40733
2025-09-05 09:11:38,104 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:40733
2025-09-05 09:11:38,104 - distributed.worker - INFO -          dashboard at:          10.6.101.50:32995
2025-09-05 09:11:38,104 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,104 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,104 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,104 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,104 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ybbmfv87
2025-09-05 09:11:38,104 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,105 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,106 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,106 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,106 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:37617
2025-09-05 09:11:38,106 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:46093
2025-09-05 09:11:38,106 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:37617
2025-09-05 09:11:38,106 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:46093
2025-09-05 09:11:38,106 - distributed.worker - INFO -          dashboard at:          10.6.101.50:42929
2025-09-05 09:11:38,106 - distributed.worker - INFO -          dashboard at:          10.6.101.50:39321
2025-09-05 09:11:38,106 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,106 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,106 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,106 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,106 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,106 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,106 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,106 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,106 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-cseh2sww
2025-09-05 09:11:38,106 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-bap3xf10
2025-09-05 09:11:38,106 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,106 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,107 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,109 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:42971
2025-09-05 09:11:38,109 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:42971
2025-09-05 09:11:38,109 - distributed.worker - INFO -          dashboard at:          10.6.101.50:38695
2025-09-05 09:11:38,109 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,109 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,109 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,109 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,109 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-g7ccy0ep
2025-09-05 09:11:38,109 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,110 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:33227
2025-09-05 09:11:38,110 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:33227
2025-09-05 09:11:38,110 - distributed.worker - INFO -          dashboard at:          10.6.101.50:37585
2025-09-05 09:11:38,110 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,110 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,110 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,110 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,110 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:33561
2025-09-05 09:11:38,110 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-b4vzgw5w
2025-09-05 09:11:38,110 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:33561
2025-09-05 09:11:38,110 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,110 - distributed.worker - INFO -          dashboard at:          10.6.101.50:34937
2025-09-05 09:11:38,110 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,110 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,110 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,110 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,110 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-wvsfvoty
2025-09-05 09:11:38,110 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,111 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:41719
2025-09-05 09:11:38,111 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:41719
2025-09-05 09:11:38,111 - distributed.worker - INFO -          dashboard at:          10.6.101.50:42493
2025-09-05 09:11:38,111 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,111 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,111 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,111 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,111 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-knrvwbe5
2025-09-05 09:11:38,111 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,111 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,112 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,112 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,113 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,115 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:36383
2025-09-05 09:11:38,115 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:36383
2025-09-05 09:11:38,115 - distributed.worker - INFO -          dashboard at:          10.6.101.50:40639
2025-09-05 09:11:38,115 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,115 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,115 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,115 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,115 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-_fyt5417
2025-09-05 09:11:38,115 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,116 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:33193
2025-09-05 09:11:38,116 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,116 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:33193
2025-09-05 09:11:38,116 - distributed.worker - INFO -          dashboard at:          10.6.101.50:34189
2025-09-05 09:11:38,116 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,116 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,116 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,116 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,116 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-c03_kv9q
2025-09-05 09:11:38,116 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,117 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,117 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,118 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:42689
2025-09-05 09:11:38,118 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:42689
2025-09-05 09:11:38,118 - distributed.worker - INFO -          dashboard at:          10.6.101.50:43029
2025-09-05 09:11:38,118 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,118 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,118 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,118 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,118 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-u9t3hpiz
2025-09-05 09:11:38,118 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,118 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,120 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:43917
2025-09-05 09:11:38,121 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:43917
2025-09-05 09:11:38,121 - distributed.worker - INFO -          dashboard at:          10.6.101.50:38333
2025-09-05 09:11:38,121 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,121 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,121 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,121 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,121 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-xhrtwjq1
2025-09-05 09:11:38,121 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,121 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,121 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:40071
2025-09-05 09:11:38,121 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:40071
2025-09-05 09:11:38,121 - distributed.worker - INFO -          dashboard at:          10.6.101.50:43267
2025-09-05 09:11:38,121 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,121 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,121 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,121 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,121 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-yh5xtj16
2025-09-05 09:11:38,121 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,122 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,122 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,124 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,124 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,125 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,125 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,126 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:41305
2025-09-05 09:11:38,126 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,126 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:41305
2025-09-05 09:11:38,126 - distributed.worker - INFO -          dashboard at:          10.6.101.50:45559
2025-09-05 09:11:38,126 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,126 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,126 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,126 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,126 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ucz8yi2i
2025-09-05 09:11:38,126 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,126 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,126 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,126 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,127 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,129 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:33837
2025-09-05 09:11:38,129 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:33837
2025-09-05 09:11:38,129 - distributed.worker - INFO -          dashboard at:          10.6.101.50:36697
2025-09-05 09:11:38,129 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:40951
2025-09-05 09:11:38,129 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,129 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,129 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:40951
2025-09-05 09:11:38,129 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,129 - distributed.worker - INFO -          dashboard at:          10.6.101.50:37709
2025-09-05 09:11:38,129 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,129 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,129 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-neozn2na
2025-09-05 09:11:38,129 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,129 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,129 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,129 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,129 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-cgio74ub
2025-09-05 09:11:38,129 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,129 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:35947
2025-09-05 09:11:38,129 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:35947
2025-09-05 09:11:38,129 - distributed.worker - INFO -          dashboard at:          10.6.101.50:36873
2025-09-05 09:11:38,129 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,129 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,130 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,130 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,130 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-7tyxh7po
2025-09-05 09:11:38,130 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,132 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,133 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,133 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,134 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,136 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,136 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.50:39625
2025-09-05 09:11:38,136 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.50:39625
2025-09-05 09:11:38,136 - distributed.worker - INFO -          dashboard at:          10.6.101.50:45903
2025-09-05 09:11:38,136 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,136 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,136 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:38,136 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:38,136 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-tvnrkwx0
2025-09-05 09:11:38,136 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,137 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,137 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,137 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,138 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,138 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,138 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,140 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,140 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,141 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,141 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,142 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,143 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,143 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,143 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,144 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,144 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,144 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,144 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,145 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,147 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,148 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,148 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,149 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,150 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,150 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,150 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,150 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,152 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,152 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,152 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,152 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,153 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,153 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,153 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,153 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,154 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,154 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,155 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,155 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,155 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,155 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,156 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,156 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,157 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,158 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,158 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,159 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,160 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,160 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,160 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,160 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,161 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,162 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,162 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,162 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:38,163 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:38,163 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:38,163 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:38,164 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:12:06,646 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,646 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,646 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,647 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,647 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,647 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,647 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,647 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,648 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,648 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,648 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,648 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,648 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,648 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,648 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,648 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,648 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,648 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,648 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,649 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,649 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,649 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,649 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,649 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,649 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,649 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,649 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,649 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,650 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,650 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,649 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,650 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,650 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,650 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,650 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,650 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,650 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,650 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,650 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,651 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,651 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,651 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,651 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,651 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,651 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,651 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,651 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,651 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,651 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,651 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,651 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,652 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,652 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,652 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,652 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,650 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,652 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,652 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,652 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,652 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,652 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,652 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,650 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,652 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,650 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,653 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,650 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,651 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,653 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,653 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,653 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,653 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,653 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,653 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,654 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,654 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,654 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,654 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,654 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,652 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,655 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,653 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,653 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,656 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,654 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,654 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,655 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,655 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,656 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,657 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,660 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,661 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,661 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,662 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,662 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,663 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,663 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,663 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,664 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,665 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,665 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,667 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,668 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,670 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:09,447 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,447 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,447 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,448 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,448 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,447 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,448 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,448 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,448 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,448 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,449 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,449 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,449 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,448 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,449 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,449 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,450 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,450 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,448 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,450 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,450 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,450 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,451 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,451 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,450 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,450 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,450 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,451 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,450 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,451 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,450 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,451 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,451 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,451 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,451 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,451 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,451 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,451 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,448 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,451 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,452 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,452 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,452 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,452 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,451 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,452 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,452 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,452 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,453 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,453 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,452 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,451 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,453 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,453 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,452 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,452 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,453 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,453 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,453 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,453 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,450 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,453 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,453 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,450 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,453 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,454 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,450 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,454 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,454 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,454 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,454 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,454 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,453 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,453 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,455 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,454 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,455 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,455 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,455 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,452 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,455 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,455 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,453 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,452 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,456 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,456 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,453 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,452 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,456 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,453 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,456 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,456 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,457 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,457 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,457 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,454 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,457 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,458 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,458 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,458 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,458 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,459 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,459 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,461 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,829 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,829 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,829 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,829 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,829 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,829 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,829 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,830 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,830 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,830 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,830 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,830 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,830 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,830 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,831 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,831 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,831 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,831 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,831 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,831 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,831 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,831 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,832 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,832 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,832 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,832 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,832 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,832 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,832 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,832 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,832 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,833 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,833 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,833 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,833 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,833 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,833 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,833 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,833 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,833 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,833 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,834 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,834 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,834 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,834 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,834 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,834 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,834 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,834 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,834 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,834 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,834 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,835 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,835 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,835 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,835 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,835 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,834 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,835 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,835 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,835 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,835 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,835 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,835 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,835 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,835 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,835 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,836 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,836 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,837 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,837 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,837 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,837 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,837 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,837 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,837 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,837 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,837 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,837 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,837 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,837 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,838 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,838 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,838 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,837 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,838 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,838 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,838 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,838 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,838 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,838 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,838 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,838 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,839 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,839 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,839 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,840 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,840 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,840 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,840 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,840 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,840 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,841 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,846 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:10,288 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,288 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,289 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,289 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,289 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,289 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,289 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,289 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,289 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,289 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,289 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,290 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,290 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,290 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,290 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,290 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,290 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,290 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,290 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,290 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,291 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,291 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,291 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,291 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,291 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,291 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,291 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,291 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,291 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,291 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,291 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,291 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,291 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,291 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,292 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,292 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,292 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,292 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,292 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,292 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,292 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,292 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,292 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,292 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,292 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,292 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,292 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,292 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,292 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,292 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,292 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,293 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,293 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,293 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,293 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,293 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,293 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,293 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,293 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,293 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,293 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,293 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,293 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,293 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,293 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,293 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,294 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,294 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,294 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,294 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,294 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,294 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,294 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,294 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,294 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,294 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,294 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,294 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,294 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,294 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,294 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,294 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,295 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,295 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,295 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,295 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,295 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,295 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,295 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,295 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,296 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,296 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,296 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,296 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,296 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,296 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,297 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,297 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,297 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,297 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,297 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,297 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,297 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,298 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:16:17,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,925 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:19,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:19,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:19,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:19,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:19,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:19,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:19,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:19,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:19,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:19,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:19,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:19,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:19,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:48,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:48,604 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:48,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:49,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:49,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:49,479 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:49,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:51,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:51,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:51,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:51,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:51,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:53,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,228 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,796 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,922 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,980 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:55,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:55,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:04,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:05,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:05,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:05,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:05,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:05,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:05,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:05,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:06,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:06,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:06,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:06,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:06,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:10,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,092 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,445 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,619 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,619 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,622 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,895 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,276 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:19,355 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:19,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:21,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:21,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:22,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:23,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:25,652 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:25,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:28,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:29,101 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:29,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:29,995 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:30,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,444 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:33,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:39,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:39,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:40,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:40,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:41,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:44,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:44,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:44,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:45,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:46,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,097 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,466 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,310 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,823 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:55,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:55,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:55,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:55,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:55,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:55,963 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:58,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:59,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:01,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:01,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:01,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:03,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:07,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:08,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:08,430 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:11,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:13,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:13,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:13,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:13,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,530 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,446 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,038 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,665 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,282 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,301 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,650 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:43,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,441 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,770 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:49,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:49,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:51,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:51,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:52,830 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:52,848 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:53,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:53,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:54,466 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:55,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:55,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:56,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:57,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:58,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:59,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:07,128 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:07,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:07,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:07,754 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,613 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,953 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,953 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,100 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,101 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,650 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,738 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,752 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:19,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:20,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:20,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:20,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:21,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:21,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:22,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:32,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:34,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:34,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:34,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,300 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,459 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,461 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,850 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,943 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,369 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:46,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:47,101 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:47,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,044 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:53,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:55,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:55,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:55,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:56,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:56,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:56,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:57,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:03,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:11,281 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,280 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,280 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,282 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:42689. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,280 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,280 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,282 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:35947. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,280 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,282 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:42017. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,280 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,280 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,283 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:42543. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,280 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,280 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,283 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:42639. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,283 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:34085. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,281 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,283 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:37439. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,281 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,281 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,281 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,281 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,283 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:38999. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,281 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,281 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,283 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:37687. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,282 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,282 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:38521. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,281 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,281 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,281 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:33259. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,285 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:46827. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,282 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,282 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,281 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,282 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:40733. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,284 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:45303. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,284 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:43747. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,282 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:36383. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,282 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,281 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,282 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:40653. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,282 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,282 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:40951. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,285 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:37007. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,281 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,285 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:43917. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,285 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:33561. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,285 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:33837. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,285 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:39851. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,285 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:33781. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,286 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:36721. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,286 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:33193. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,286 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:42971. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,282 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,286 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:43553. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,286 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:41305. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,286 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:45687. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,286 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:34201. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,280 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.101.50:42804 remote=tcp://10.6.101.37:8753>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:20:11,287 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:35229. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,287 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:37617. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,287 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:33279. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,282 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,281 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,290 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:33227. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,290 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:46093. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,287 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,291 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:46067. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,301 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:42785'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,303 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:42269'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2161, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2187, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1745, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 911, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,315 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:44199'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,316 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:39123'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,316 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:41589'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,317 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:32943'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,317 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1689, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,317 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:37271'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,317 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3391, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,317 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3576, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,317 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:42429'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,318 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 7572, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,318 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:46653'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,318 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3348, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,318 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:35233'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,318 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2338, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,318 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,318 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3357, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,318 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1614, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,318 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:44897'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,318 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2357, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,319 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2999, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,319 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,319 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,319 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:37455'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,319 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2060, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,319 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,319 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:43013'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,319 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,319 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,319 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2071, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,319 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,319 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,319 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,319 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,319 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:32941'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,319 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,319 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2946, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,319 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2332, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,319 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,319 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,319 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,320 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1702, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,320 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:41727'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,320 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1831, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,320 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3846, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,320 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2376, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,320 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3827, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,320 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1763, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,320 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3589, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,321 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,321 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.66:44873, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,321 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 910, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,321 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,321 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,321 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,321 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1110, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,321 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.47:37477, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,321 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,321 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,321 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,321 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,321 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 6736, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,321 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,321 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.43:33649, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,321 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,321 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,321 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,321 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2067, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,322 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,322 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,322 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.49:39907, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,322 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,322 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,322 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1303, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,322 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,322 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,322 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,322 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,322 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,322 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,322 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,322 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,322 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,322 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,322 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,322 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,323 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,323 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,323 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,331 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:43521'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,332 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:42829'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,332 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:46235'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,332 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:41775'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,332 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:43563'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,332 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:39043'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,333 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:35623'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,333 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:44301'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,333 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2354, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,333 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:37185'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,333 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1305, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,333 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:40355'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,333 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3275, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,333 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:42691'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,333 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3255, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,333 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:39453'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,333 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.55:40503, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,334 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:44487'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,334 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3393, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,334 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.37:36545, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,334 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2118, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,334 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:40995'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,334 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3871, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,334 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1901, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,334 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2669, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,334 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.54:45943, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,334 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3346, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,334 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:37489'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,334 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3286, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,334 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2905, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,334 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3468, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,334 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2331, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,334 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2681, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,334 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1294, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,334 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3336, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,335 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:44671'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,335 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2889, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,335 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3395, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,335 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1301, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,335 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:35921'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,335 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2896, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,335 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:44703'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,335 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 538, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,336 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:46641'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,336 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.61:43631, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,336 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.38:37493, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,336 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2732, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,336 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:42413'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,336 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.64:45319, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,336 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 560, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,336 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2330, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,336 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1579, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,336 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:34027'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,336 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.64:36105, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,336 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1097, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,337 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 4040, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,337 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,337 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,337 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,337 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:44687'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,337 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.67:40313, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,337 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,337 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,337 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,337 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3670, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,337 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,337 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,337 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3105, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,337 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,337 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,337 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2545, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,337 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,337 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3201, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,337 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,337 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3587, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,337 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2077, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,337 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,337 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,337 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,337 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3132, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,337 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,338 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,338 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,338 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1762, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,338 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,338 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1306, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,338 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,338 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,338 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,338 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,338 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.44:36823, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,338 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,338 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,338 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,338 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,338 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,338 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,338 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,338 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,338 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,338 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,338 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,338 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,338 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,338 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2903, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,339 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,339 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,339 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,339 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,339 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,339 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3394, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,339 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,339 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,339 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,339 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,339 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,339 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.53:43003, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,339 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,339 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2012, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,339 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,339 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,339 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1996, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,339 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,339 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,339 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,340 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,340 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,340 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,340 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,340 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,340 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,340 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,340 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,341 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,341 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,341 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,341 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,341 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,341 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,342 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,356 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,196 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:12,199 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:33899. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:12,212 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.50:42192 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:12,217 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:43357'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,218 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:12,219 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:12,219 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:12,219 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:12,219 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:12,224 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14d8c0d74c90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:12,232 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,308 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:12,310 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:40405. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:12,317 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.50:42184 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.101.50:42184 remote=tcp://10.6.101.37:8753>: Stream is closed
2025-09-05 09:20:12,322 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:37831'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,323 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:12,323 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:12,324 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:12,324 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:12,324 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:12,328 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:12,334 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,407 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,360 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,428 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,428 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,881 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:46235'. Reason: nanny-close-gracefully
2025-09-05 09:20:13,885 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:46235' closed.
2025-09-05 09:20:14,235 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,338 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,411 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,596 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:14,598 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:33413. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:14,616 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.50:42182 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:14,620 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:46495'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,622 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3519, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:14,622 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:14,622 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:14,623 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:14,623 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:14,623 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:14,627 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x148ad98e4a90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:14,691 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:43357'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,692 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:43357' closed.
2025-09-05 09:20:14,792 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:39123'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,793 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:37831'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,799 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:39123' closed.
2025-09-05 09:20:14,800 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:37831' closed.
2025-09-05 09:20:15,222 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,224 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,432 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:15,432 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:15,603 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,833 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,888 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:41589'. Reason: nanny-close-gracefully
2025-09-05 09:20:15,890 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:41589' closed.
2025-09-05 09:20:15,904 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:44301'. Reason: nanny-close-gracefully
2025-09-05 09:20:15,905 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:44301' closed.
2025-09-05 09:20:15,955 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:15,955 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:15,957 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:40279. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,958 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:37433. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:15,957 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:15,959 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:38205. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,961 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:15,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:15,974 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.50:42376 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:15,974 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.50:42304 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:15,975 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.50:42294 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:15,978 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:37187'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,979 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:15,979 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:15,980 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:15,980 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:15,980 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:15,980 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:41223'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,980 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:15,981 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:15,981 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:32795'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,981 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:15,981 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:15,981 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:15,982 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:15,982 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:15,982 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:15,982 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:15,982 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:15,984 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x150ef7e17a50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:15,984 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x151deb1ca2d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:15,985 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x148ef1d2f2d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:15,989 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,990 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,992 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,200 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,227 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,230 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,370 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,578 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,607 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,646 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,729 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,753 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:34027'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,753 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:44487'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,755 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:34027' closed.
2025-09-05 09:20:17,755 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:44487' closed.
2025-09-05 09:20:17,837 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,965 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,993 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,993 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,995 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,003 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:41775'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,004 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:41775' closed.
2025-09-05 09:20:18,048 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,050 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,226 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:44897'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,228 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:44897' closed.
2025-09-05 09:20:18,372 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:41223'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,373 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:37187'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,374 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:41223' closed.
2025-09-05 09:20:18,374 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:37187' closed.
2025-09-05 09:20:18,417 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:44703'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,418 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:44703' closed.
2025-09-05 09:20:18,434 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:32795'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,436 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:32795' closed.
2025-09-05 09:20:18,944 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:18,947 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:39625. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,956 - distributed.worker - ERROR - failed during get data with tcp://10.6.101.50:39625 -> tcp://10.6.101.58:40173
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.101.50:39625 remote=tcp://10.6.101.58:41856>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-05 09:20:18,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:18,966 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.50:42640 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:18,969 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:36329'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,970 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:18,971 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:18,971 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:18,971 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:18,971 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:18,973 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14d5a6305250>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:18,979 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,204 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,295 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,374 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,582 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,613 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:39453'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,614 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:39453' closed.
2025-09-05 09:20:19,646 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,645 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:19,648 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:38817. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,650 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:19,664 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.50:42214 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:19,673 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:34111'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,674 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:19,674 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:19,674 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:19,675 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:19,675 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:19,676 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:19,682 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,733 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,738 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,760 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,770 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:43521'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,771 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:43521' closed.
2025-09-05 09:20:19,782 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,782 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,784 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,783 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:19,786 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:41719. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:19,803 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.50:42576 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:19,807 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:41833'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,808 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:19,808 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:19,808 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:19,809 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:19,809 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:19,810 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14addad488d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:19,816 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,874 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,875 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:19,878 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:40811. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,878 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:19,896 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.50:42478 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:19,900 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:40255'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,901 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:19,901 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:19,901 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:19,901 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:19,901 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:19,905 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,903 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x153bddf7c810>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:19,908 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,937 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,968 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,969 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:43563'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,971 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:43563' closed.
2025-09-05 09:20:19,986 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,990 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,989 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:19,991 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:44189. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:20,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:20,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:20,008 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.50:42354 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:20,012 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:36057'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:20,011 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:20,013 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:20,013 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:20,013 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:39381. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:20,013 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:20,013 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:20,013 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:20,014 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x153df7951890>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:20,020 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,028 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.50:42282 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:20,032 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:38669'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:20,033 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:20,033 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:20,033 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:20,034 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:20,034 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:20,035 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x147df16be710>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:20,038 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,040 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,044 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,052 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,054 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,062 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:44199'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,063 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:44199' closed.
2025-09-05 09:20:20,104 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:20,107 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:42645. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:20,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:20,126 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.50:42260 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:20,130 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:39187'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:20,131 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:20,131 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:20,131 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:20,131 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:20,131 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:20,132 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1473ba596b50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:20,137 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,158 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,180 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:41727'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,180 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:41727' closed.
2025-09-05 09:20:20,198 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,199 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,200 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:20,202 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:40071. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:20,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:20,218 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.50:42608 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:20,228 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:34805'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:20,229 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2206, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:20,229 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:20,229 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:20,229 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:20,229 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:20,229 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:20,230 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,230 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x153b38467c10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:20,244 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,247 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,248 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:20,250 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.50:33287. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:20,253 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:20,268 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.50:42458 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:20,272 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.50:37103'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:20,273 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:20,273 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:20,273 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:20,273 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:20,273 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:20,274 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14da6a89ccd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:20,279 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,431 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:43013'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,431 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:43013' closed.
2025-09-05 09:20:20,444 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:46495'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,445 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:46495' closed.
2025-09-05 09:20:20,982 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,299 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,365 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:36329'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,367 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:36329' closed.
2025-09-05 09:20:21,652 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,685 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,694 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:44671'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,695 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:44671' closed.
2025-09-05 09:20:21,742 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,764 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,786 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,786 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,788 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,819 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,878 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,883 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,909 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,911 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,941 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,972 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,990 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,994 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,023 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,042 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,043 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,048 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,140 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,145 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:34111'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,149 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:34111' closed.
2025-09-05 09:20:22,162 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,202 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,203 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,203 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:44687'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,204 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:44687' closed.
2025-09-05 09:20:22,230 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:39043'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,231 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:39043' closed.
2025-09-05 09:20:22,234 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,240 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:42269'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,241 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:42269' closed.
2025-09-05 09:20:22,247 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:40355'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,248 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,248 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:40355' closed.
2025-09-05 09:20:22,257 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,282 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,312 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:42691'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,313 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:42691' closed.
2025-09-05 09:20:22,324 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:37489'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,325 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:37489' closed.
2025-09-05 09:20:22,364 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:46653'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,365 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:46653' closed.
2025-09-05 09:20:22,379 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:40255'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,380 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:40255' closed.
2025-09-05 09:20:22,402 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:37185'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,403 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:37185' closed.
2025-09-05 09:20:22,427 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:41833'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,428 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:41833' closed.
2025-09-05 09:20:22,448 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:42413'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,449 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:42413' closed.
2025-09-05 09:20:22,483 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:42429'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,484 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:42429' closed.
2025-09-05 09:20:22,493 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:35233'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,494 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:35233' closed.
2025-09-05 09:20:22,507 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:36057'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,508 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:36057' closed.
2025-09-05 09:20:22,519 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:38669'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,520 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:38669' closed.
2025-09-05 09:20:22,539 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:32943'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,540 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:32943' closed.
2025-09-05 09:20:22,551 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:37271'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,552 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:37271' closed.
2025-09-05 09:20:22,558 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:42829'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,559 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:42829' closed.
2025-09-05 09:20:22,629 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:37455'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,630 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:37455' closed.
2025-09-05 09:20:22,647 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:39187'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,649 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:39187' closed.
2025-09-05 09:20:22,662 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:35921'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,663 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:46641'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,664 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:35921' closed.
2025-09-05 09:20:22,664 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:46641' closed.
2025-09-05 09:20:22,687 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:40995'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,688 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:40995' closed.
2025-09-05 09:20:22,703 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:42785'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,704 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:42785' closed.
2025-09-05 09:20:22,737 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:35623'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,737 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:35623' closed.
2025-09-05 09:20:22,758 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:34805'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,759 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:32941'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,759 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:34805' closed.
2025-09-05 09:20:22,760 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:32941' closed.
2025-09-05 09:20:22,793 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.50:37103'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,794 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.50:37103' closed.
2025-09-05 09:20:22,797 - distributed.dask_worker - INFO - End worker
