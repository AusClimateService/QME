Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 10:52:42,421 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:45895'
2025-09-03 10:52:42,431 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:43129'
2025-09-03 10:52:42,437 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:36439'
2025-09-03 10:52:42,445 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:36409'
2025-09-03 10:52:42,449 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:45103'
2025-09-03 10:52:42,525 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:33295'
2025-09-03 10:52:42,529 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:44841'
2025-09-03 10:52:42,534 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:35083'
2025-09-03 10:52:42,538 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:42443'
2025-09-03 10:52:42,546 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:35281'
2025-09-03 10:52:42,551 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:42119'
2025-09-03 10:52:42,555 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:42179'
2025-09-03 10:52:42,559 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:41963'
2025-09-03 10:52:42,564 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:44329'
2025-09-03 10:52:42,569 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:43079'
2025-09-03 10:52:42,573 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:36643'
2025-09-03 10:52:42,578 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:41187'
2025-09-03 10:52:42,584 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:46565'
2025-09-03 10:52:42,588 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:41899'
2025-09-03 10:52:42,592 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:40165'
2025-09-03 10:52:42,597 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:44843'
2025-09-03 10:52:42,602 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:45651'
2025-09-03 10:52:42,605 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:34997'
2025-09-03 10:52:42,611 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:46849'
2025-09-03 10:52:42,615 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:38893'
2025-09-03 10:52:42,619 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:37305'
2025-09-03 10:52:43,267 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:32931
2025-09-03 10:52:43,267 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:35889
2025-09-03 10:52:43,267 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:32931
2025-09-03 10:52:43,267 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:35889
2025-09-03 10:52:43,267 - distributed.worker - INFO -          dashboard at:          10.6.105.10:35295
2025-09-03 10:52:43,267 - distributed.worker - INFO -          dashboard at:          10.6.105.10:37087
2025-09-03 10:52:43,267 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,267 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,267 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,267 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,267 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,267 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,267 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,267 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,267 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-6c_ny_i3
2025-09-03 10:52:43,267 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-in6yszio
2025-09-03 10:52:43,267 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,267 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,267 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:40933
2025-09-03 10:52:43,267 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:40933
2025-09-03 10:52:43,267 - distributed.worker - INFO -          dashboard at:          10.6.105.10:43453
2025-09-03 10:52:43,268 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,268 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,268 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,268 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,268 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-swn02d8m
2025-09-03 10:52:43,268 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,373 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:44785
2025-09-03 10:52:43,373 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:44785
2025-09-03 10:52:43,373 - distributed.worker - INFO -          dashboard at:          10.6.105.10:39731
2025-09-03 10:52:43,373 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,373 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,373 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,373 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,373 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-p43dd031
2025-09-03 10:52:43,373 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,407 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:37709
2025-09-03 10:52:43,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:37709
2025-09-03 10:52:43,407 - distributed.worker - INFO -          dashboard at:          10.6.105.10:46811
2025-09-03 10:52:43,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,407 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,407 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,407 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,407 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-taer9v1i
2025-09-03 10:52:43,407 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,410 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:42807'
2025-09-03 10:52:43,414 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:42269'
2025-09-03 10:52:43,419 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:44255'
2025-09-03 10:52:43,422 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:45703
2025-09-03 10:52:43,422 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:45703
2025-09-03 10:52:43,422 - distributed.worker - INFO -          dashboard at:          10.6.105.10:40605
2025-09-03 10:52:43,422 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,422 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,422 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,422 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,422 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-nhkep8_z
2025-09-03 10:52:43,422 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,424 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:38327'
2025-09-03 10:52:43,427 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:43049
2025-09-03 10:52:43,427 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:43049
2025-09-03 10:52:43,427 - distributed.worker - INFO -          dashboard at:          10.6.105.10:37779
2025-09-03 10:52:43,427 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,427 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,427 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,427 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,427 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-0ixgx7x3
2025-09-03 10:52:43,427 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,428 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:44149'
2025-09-03 10:52:43,436 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:42295'
2025-09-03 10:52:43,440 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:43751'
2025-09-03 10:52:43,443 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:36739
2025-09-03 10:52:43,444 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:36739
2025-09-03 10:52:43,444 - distributed.worker - INFO -          dashboard at:          10.6.105.10:42775
2025-09-03 10:52:43,444 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,444 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,444 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,444 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,444 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-k5w97j34
2025-09-03 10:52:43,444 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,444 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:37529'
2025-09-03 10:52:43,449 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:35075
2025-09-03 10:52:43,449 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:35075
2025-09-03 10:52:43,449 - distributed.worker - INFO -          dashboard at:          10.6.105.10:45635
2025-09-03 10:52:43,449 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,449 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,449 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,449 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,449 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-8ug7kxms
2025-09-03 10:52:43,449 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,450 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:39777'
2025-09-03 10:52:43,455 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:44883'
2025-09-03 10:52:43,460 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:36195'
2025-09-03 10:52:43,465 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:40301'
2025-09-03 10:52:43,470 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:39255'
2025-09-03 10:52:43,472 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:40821
2025-09-03 10:52:43,472 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:40821
2025-09-03 10:52:43,472 - distributed.worker - INFO -          dashboard at:          10.6.105.10:33633
2025-09-03 10:52:43,472 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,472 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,472 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,472 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,472 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9so90xkp
2025-09-03 10:52:43,472 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,473 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:45409
2025-09-03 10:52:43,473 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:45409
2025-09-03 10:52:43,473 - distributed.worker - INFO -          dashboard at:          10.6.105.10:35689
2025-09-03 10:52:43,473 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,473 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,473 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,473 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,473 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ibw3yl87
2025-09-03 10:52:43,473 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,474 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:39631
2025-09-03 10:52:43,474 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:39631
2025-09-03 10:52:43,474 - distributed.worker - INFO -          dashboard at:          10.6.105.10:45053
2025-09-03 10:52:43,474 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,474 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,474 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,474 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,474 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-bx9j_eoj
2025-09-03 10:52:43,474 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,475 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:43881'
2025-09-03 10:52:43,479 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:41987'
2025-09-03 10:52:43,488 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:38175
2025-09-03 10:52:43,488 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:38175
2025-09-03 10:52:43,488 - distributed.worker - INFO -          dashboard at:          10.6.105.10:45619
2025-09-03 10:52:43,488 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,488 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,488 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,488 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,488 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-07p9l9ti
2025-09-03 10:52:43,488 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,491 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:37743
2025-09-03 10:52:43,491 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:37743
2025-09-03 10:52:43,491 - distributed.worker - INFO -          dashboard at:          10.6.105.10:43331
2025-09-03 10:52:43,491 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,491 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,491 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,491 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,491 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-4w4a7e8m
2025-09-03 10:52:43,491 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,551 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:39597
2025-09-03 10:52:43,551 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:39597
2025-09-03 10:52:43,551 - distributed.worker - INFO -          dashboard at:          10.6.105.10:46251
2025-09-03 10:52:43,551 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,551 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,551 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,551 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,551 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-kwlqai23
2025-09-03 10:52:43,551 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,551 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:38873
2025-09-03 10:52:43,551 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:38873
2025-09-03 10:52:43,551 - distributed.worker - INFO -          dashboard at:          10.6.105.10:41647
2025-09-03 10:52:43,551 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,551 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,551 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,551 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,551 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-23yp0b5l
2025-09-03 10:52:43,551 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,552 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:45827
2025-09-03 10:52:43,552 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:45827
2025-09-03 10:52:43,552 - distributed.worker - INFO -          dashboard at:          10.6.105.10:36735
2025-09-03 10:52:43,552 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,552 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,552 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,552 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,552 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-gief2o05
2025-09-03 10:52:43,552 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,553 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:37801
2025-09-03 10:52:43,553 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:37801
2025-09-03 10:52:43,553 - distributed.worker - INFO -          dashboard at:          10.6.105.10:37469
2025-09-03 10:52:43,553 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,553 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,553 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,553 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,553 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-gzi1cb0z
2025-09-03 10:52:43,553 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,561 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:37657
2025-09-03 10:52:43,561 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:37657
2025-09-03 10:52:43,561 - distributed.worker - INFO -          dashboard at:          10.6.105.10:44259
2025-09-03 10:52:43,561 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,561 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,561 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,561 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,561 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-8b0_5y9k
2025-09-03 10:52:43,562 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,562 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:41433
2025-09-03 10:52:43,562 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:41433
2025-09-03 10:52:43,562 - distributed.worker - INFO -          dashboard at:          10.6.105.10:38517
2025-09-03 10:52:43,562 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,562 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,562 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,562 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,562 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-yx10v8nj
2025-09-03 10:52:43,562 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,562 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:36665
2025-09-03 10:52:43,562 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:36665
2025-09-03 10:52:43,562 - distributed.worker - INFO -          dashboard at:          10.6.105.10:40505
2025-09-03 10:52:43,562 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,562 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,562 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,562 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,562 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-xh7kjt40
2025-09-03 10:52:43,562 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,570 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:44857
2025-09-03 10:52:43,570 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:44857
2025-09-03 10:52:43,570 - distributed.worker - INFO -          dashboard at:          10.6.105.10:42895
2025-09-03 10:52:43,570 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,570 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,570 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,570 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,570 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-je0wcne5
2025-09-03 10:52:43,570 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,575 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:43971
2025-09-03 10:52:43,575 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:43971
2025-09-03 10:52:43,575 - distributed.worker - INFO -          dashboard at:          10.6.105.10:34199
2025-09-03 10:52:43,575 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,575 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,576 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,576 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,576 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-xl9_s59z
2025-09-03 10:52:43,576 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,577 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:44755
2025-09-03 10:52:43,577 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:44755
2025-09-03 10:52:43,577 - distributed.worker - INFO -          dashboard at:          10.6.105.10:36805
2025-09-03 10:52:43,577 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,577 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,577 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,577 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,577 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ygeyyvqp
2025-09-03 10:52:43,577 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,579 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:37233
2025-09-03 10:52:43,579 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:37233
2025-09-03 10:52:43,579 - distributed.worker - INFO -          dashboard at:          10.6.105.10:45817
2025-09-03 10:52:43,579 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,579 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,579 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,579 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,579 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-tf8nfrb8
2025-09-03 10:52:43,579 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,582 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:42447
2025-09-03 10:52:43,582 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:42447
2025-09-03 10:52:43,582 - distributed.worker - INFO -          dashboard at:          10.6.105.10:46237
2025-09-03 10:52:43,582 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,582 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,582 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,582 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,582 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-5716twt0
2025-09-03 10:52:43,582 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,586 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:33739'
2025-09-03 10:52:43,591 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:45317'
2025-09-03 10:52:43,596 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:42691'
2025-09-03 10:52:43,599 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:33321'
2025-09-03 10:52:43,605 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:35545'
2025-09-03 10:52:43,608 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:39121'
2025-09-03 10:52:43,611 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:38439'
2025-09-03 10:52:43,616 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:40765'
2025-09-03 10:52:43,620 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:39233'
2025-09-03 10:52:43,624 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:33445'
2025-09-03 10:52:43,629 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:33435'
2025-09-03 10:52:43,633 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:40971'
2025-09-03 10:52:43,638 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:36091'
2025-09-03 10:52:43,644 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:39079'
2025-09-03 10:52:43,647 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:42293'
2025-09-03 10:52:43,653 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:43213'
2025-09-03 10:52:43,660 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:46345'
2025-09-03 10:52:43,664 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:42843'
2025-09-03 10:52:43,669 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:37971'
2025-09-03 10:52:43,673 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:33955'
2025-09-03 10:52:43,678 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:41667'
2025-09-03 10:52:43,682 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:36633'
2025-09-03 10:52:43,686 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:43847'
2025-09-03 10:52:43,692 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:45131'
2025-09-03 10:52:43,697 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:43731'
2025-09-03 10:52:43,701 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:42461'
2025-09-03 10:52:43,707 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:36455'
2025-09-03 10:52:43,712 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:41779'
2025-09-03 10:52:43,716 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:37199'
2025-09-03 10:52:43,721 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:39697'
2025-09-03 10:52:43,725 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:42603'
2025-09-03 10:52:43,728 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:34567'
2025-09-03 10:52:43,734 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:34615'
2025-09-03 10:52:43,738 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:36155'
2025-09-03 10:52:43,742 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:39305'
2025-09-03 10:52:43,746 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:35881'
2025-09-03 10:52:43,753 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:40701'
2025-09-03 10:52:43,774 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:43559'
2025-09-03 10:52:43,781 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:34565'
2025-09-03 10:52:43,784 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:40833'
2025-09-03 10:52:43,787 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:43683'
2025-09-03 10:52:43,789 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:33549'
2025-09-03 10:52:43,793 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:33991'
2025-09-03 10:52:43,797 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:37707'
2025-09-03 10:52:43,802 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:40857'
2025-09-03 10:52:43,806 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:35047'
2025-09-03 10:52:43,812 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:36619'
2025-09-03 10:52:43,816 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:38135'
2025-09-03 10:52:43,821 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:40627'
2025-09-03 10:52:43,827 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:40695'
2025-09-03 10:52:43,833 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:42853'
2025-09-03 10:52:43,838 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:39001'
2025-09-03 10:52:43,843 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:35351'
2025-09-03 10:52:43,850 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:34549'
2025-09-03 10:52:43,856 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:33885'
2025-09-03 10:52:43,861 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:38443'
2025-09-03 10:52:43,865 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:42763'
2025-09-03 10:52:43,868 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:36783'
2025-09-03 10:52:43,873 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:35627'
2025-09-03 10:52:43,878 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:34711'
2025-09-03 10:52:43,883 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:44001'
2025-09-03 10:52:43,887 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:40947'
2025-09-03 10:52:44,131 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.10:44173'
2025-09-03 10:52:44,286 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:44839
2025-09-03 10:52:44,286 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:44839
2025-09-03 10:52:44,286 - distributed.worker - INFO -          dashboard at:          10.6.105.10:41651
2025-09-03 10:52:44,286 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,286 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,286 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:44,286 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:44,286 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-a_gca0hr
2025-09-03 10:52:44,286 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,306 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:33455
2025-09-03 10:52:44,306 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:33455
2025-09-03 10:52:44,306 - distributed.worker - INFO -          dashboard at:          10.6.105.10:37521
2025-09-03 10:52:44,306 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,306 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,306 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:44,306 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:44,306 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-zejv9v9p
2025-09-03 10:52:44,306 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,309 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:43481
2025-09-03 10:52:44,309 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:43481
2025-09-03 10:52:44,309 - distributed.worker - INFO -          dashboard at:          10.6.105.10:34473
2025-09-03 10:52:44,309 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,309 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:44,309 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:44,309 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-iuaibuk8
2025-09-03 10:52:44,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,316 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:44,317 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,317 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,318 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:44,331 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:44,332 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:36677
2025-09-03 10:52:44,332 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:36677
2025-09-03 10:52:44,332 - distributed.worker - INFO -          dashboard at:          10.6.105.10:39089
2025-09-03 10:52:44,332 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,332 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,332 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:44,332 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:44,332 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-i6id12tp
2025-09-03 10:52:44,332 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,332 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,332 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,334 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:42985
2025-09-03 10:52:44,334 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:42985
2025-09-03 10:52:44,334 - distributed.worker - INFO -          dashboard at:          10.6.105.10:43471
2025-09-03 10:52:44,334 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:44,334 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:44,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:44,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-2ppgr3sj
2025-09-03 10:52:44,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,336 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:41983
2025-09-03 10:52:44,336 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:41983
2025-09-03 10:52:44,336 - distributed.worker - INFO -          dashboard at:          10.6.105.10:37953
2025-09-03 10:52:44,336 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,336 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,336 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:44,336 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:44,336 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-l_hr93ry
2025-09-03 10:52:44,336 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,341 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:45685
2025-09-03 10:52:44,341 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:45685
2025-09-03 10:52:44,341 - distributed.worker - INFO -          dashboard at:          10.6.105.10:45057
2025-09-03 10:52:44,341 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,341 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,341 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:44,341 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:44,341 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-h3_fjixa
2025-09-03 10:52:44,341 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,345 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:44,346 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,347 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,348 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:44,348 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:33035
2025-09-03 10:52:44,348 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:33035
2025-09-03 10:52:44,348 - distributed.worker - INFO -          dashboard at:          10.6.105.10:38435
2025-09-03 10:52:44,348 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,348 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,349 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:44,349 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:44,349 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-t9tg5au3
2025-09-03 10:52:44,349 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,593 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:35107
2025-09-03 10:52:44,593 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:35107
2025-09-03 10:52:44,594 - distributed.worker - INFO -          dashboard at:          10.6.105.10:36525
2025-09-03 10:52:44,594 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,594 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,594 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:44,594 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:44,594 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ps1uk3p5
2025-09-03 10:52:44,594 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,596 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:41353
2025-09-03 10:52:44,596 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:41353
2025-09-03 10:52:44,596 - distributed.worker - INFO -          dashboard at:          10.6.105.10:34515
2025-09-03 10:52:44,596 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,596 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,596 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:44,596 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:44,596 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ladoysh9
2025-09-03 10:52:44,596 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,602 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:33957
2025-09-03 10:52:44,602 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:33957
2025-09-03 10:52:44,602 - distributed.worker - INFO -          dashboard at:          10.6.105.10:37891
2025-09-03 10:52:44,602 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,602 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,602 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:44,602 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:44,602 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-2g2a208b
2025-09-03 10:52:44,602 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,644 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:38291
2025-09-03 10:52:44,644 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:38291
2025-09-03 10:52:44,644 - distributed.worker - INFO -          dashboard at:          10.6.105.10:34697
2025-09-03 10:52:44,644 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,644 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,644 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:44,644 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:44,644 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-altc_7mp
2025-09-03 10:52:44,644 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,648 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:39833
2025-09-03 10:52:44,648 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:39833
2025-09-03 10:52:44,648 - distributed.worker - INFO -          dashboard at:          10.6.105.10:39221
2025-09-03 10:52:44,648 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,648 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,648 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:44,648 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:44,648 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-3x3p7ygh
2025-09-03 10:52:44,648 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,654 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:39793
2025-09-03 10:52:44,655 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:39793
2025-09-03 10:52:44,655 - distributed.worker - INFO -          dashboard at:          10.6.105.10:44047
2025-09-03 10:52:44,655 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,655 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,655 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:44,655 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:44,655 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ediqopv9
2025-09-03 10:52:44,655 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,662 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:37781
2025-09-03 10:52:44,662 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:37781
2025-09-03 10:52:44,662 - distributed.worker - INFO -          dashboard at:          10.6.105.10:41061
2025-09-03 10:52:44,662 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,662 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,662 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:44,662 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:44,662 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-gp8agumu
2025-09-03 10:52:44,662 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,787 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:44,788 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,788 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,790 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:44,802 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:44,804 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,804 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,805 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:44,846 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:44,848 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,848 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,849 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:44,861 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:44,862 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,863 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,864 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:44,876 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:44,877 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,879 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:44,891 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:44,892 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,893 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,894 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:44,906 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:44,907 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,907 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,909 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:44,967 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:44,968 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,968 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,970 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:45,175 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:43029
2025-09-03 10:52:45,175 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:43029
2025-09-03 10:52:45,175 - distributed.worker - INFO -          dashboard at:          10.6.105.10:41917
2025-09-03 10:52:45,175 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,175 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,175 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,175 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,175 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-t4fp35ug
2025-09-03 10:52:45,176 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,180 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:43565
2025-09-03 10:52:45,180 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:43565
2025-09-03 10:52:45,180 - distributed.worker - INFO -          dashboard at:          10.6.105.10:37399
2025-09-03 10:52:45,180 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,180 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,180 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,180 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,180 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-nglgar7y
2025-09-03 10:52:45,180 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,186 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:34343
2025-09-03 10:52:45,186 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:34343
2025-09-03 10:52:45,187 - distributed.worker - INFO -          dashboard at:          10.6.105.10:41441
2025-09-03 10:52:45,187 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,187 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,187 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,187 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,187 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-62uk_0qw
2025-09-03 10:52:45,187 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,192 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:40559
2025-09-03 10:52:45,193 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:40559
2025-09-03 10:52:45,193 - distributed.worker - INFO -          dashboard at:          10.6.105.10:38123
2025-09-03 10:52:45,193 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,193 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,193 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,193 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,193 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-sgjt49zi
2025-09-03 10:52:45,193 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,197 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:41991
2025-09-03 10:52:45,197 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:41991
2025-09-03 10:52:45,197 - distributed.worker - INFO -          dashboard at:          10.6.105.10:39235
2025-09-03 10:52:45,197 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,197 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,197 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,197 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,197 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-v2b4nzt3
2025-09-03 10:52:45,197 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,202 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:40619
2025-09-03 10:52:45,202 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:40619
2025-09-03 10:52:45,202 - distributed.worker - INFO -          dashboard at:          10.6.105.10:46153
2025-09-03 10:52:45,202 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,202 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,202 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,202 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,203 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-v6hqpdcz
2025-09-03 10:52:45,203 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,213 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:43465
2025-09-03 10:52:45,213 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:43465
2025-09-03 10:52:45,213 - distributed.worker - INFO -          dashboard at:          10.6.105.10:38063
2025-09-03 10:52:45,213 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,213 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,213 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,213 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,213 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-vfk0yjn2
2025-09-03 10:52:45,213 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,286 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:43717
2025-09-03 10:52:45,286 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:43717
2025-09-03 10:52:45,286 - distributed.worker - INFO -          dashboard at:          10.6.105.10:39023
2025-09-03 10:52:45,286 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,286 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,286 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,286 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,286 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-wxjci9e8
2025-09-03 10:52:45,286 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,294 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:36083
2025-09-03 10:52:45,294 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:36083
2025-09-03 10:52:45,294 - distributed.worker - INFO -          dashboard at:          10.6.105.10:42191
2025-09-03 10:52:45,294 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,294 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,294 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,294 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,294 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-l3jzmsvr
2025-09-03 10:52:45,294 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,297 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:46693
2025-09-03 10:52:45,297 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:46693
2025-09-03 10:52:45,297 - distributed.worker - INFO -          dashboard at:          10.6.105.10:45153
2025-09-03 10:52:45,297 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,297 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,297 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,297 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,297 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-8ol77svi
2025-09-03 10:52:45,297 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,299 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:34703
2025-09-03 10:52:45,299 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:34703
2025-09-03 10:52:45,299 - distributed.worker - INFO -          dashboard at:          10.6.105.10:44981
2025-09-03 10:52:45,299 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,299 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,299 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,299 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,299 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-u4f390jt
2025-09-03 10:52:45,299 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,307 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:46611
2025-09-03 10:52:45,307 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:46611
2025-09-03 10:52:45,307 - distributed.worker - INFO -          dashboard at:          10.6.105.10:36771
2025-09-03 10:52:45,307 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,307 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,307 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,307 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,307 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ta87z9h4
2025-09-03 10:52:45,307 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,345 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:38649
2025-09-03 10:52:45,345 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:38649
2025-09-03 10:52:45,345 - distributed.worker - INFO -          dashboard at:          10.6.105.10:45829
2025-09-03 10:52:45,345 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,345 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,345 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,345 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,345 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-7kv9ujcu
2025-09-03 10:52:45,346 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,346 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:37405
2025-09-03 10:52:45,347 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:37405
2025-09-03 10:52:45,347 - distributed.worker - INFO -          dashboard at:          10.6.105.10:41313
2025-09-03 10:52:45,347 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,347 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,347 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,347 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,347 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-bhn8f43x
2025-09-03 10:52:45,347 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,374 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:37377
2025-09-03 10:52:45,374 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:37377
2025-09-03 10:52:45,374 - distributed.worker - INFO -          dashboard at:          10.6.105.10:36939
2025-09-03 10:52:45,374 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,374 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,374 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,374 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,374 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-hf9q_o03
2025-09-03 10:52:45,375 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,387 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:40019
2025-09-03 10:52:45,387 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:40019
2025-09-03 10:52:45,387 - distributed.worker - INFO -          dashboard at:          10.6.105.10:34475
2025-09-03 10:52:45,387 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,387 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,387 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,387 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,387 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-4izme7kd
2025-09-03 10:52:45,387 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,393 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:34983
2025-09-03 10:52:45,393 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:34983
2025-09-03 10:52:45,393 - distributed.worker - INFO -          dashboard at:          10.6.105.10:39941
2025-09-03 10:52:45,393 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,393 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,393 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,393 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-nz8_wv_z
2025-09-03 10:52:45,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,408 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:42283
2025-09-03 10:52:45,408 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:39785
2025-09-03 10:52:45,408 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:42283
2025-09-03 10:52:45,408 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:39785
2025-09-03 10:52:45,408 - distributed.worker - INFO -          dashboard at:          10.6.105.10:44061
2025-09-03 10:52:45,408 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,408 - distributed.worker - INFO -          dashboard at:          10.6.105.10:36141
2025-09-03 10:52:45,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,408 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,408 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-h68yw22u
2025-09-03 10:52:45,408 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-bd9h_be_
2025-09-03 10:52:45,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,410 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:38023
2025-09-03 10:52:45,411 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:38023
2025-09-03 10:52:45,411 - distributed.worker - INFO -          dashboard at:          10.6.105.10:38227
2025-09-03 10:52:45,411 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,411 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,411 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,411 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,411 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ih6d3lkz
2025-09-03 10:52:45,411 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,417 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:42075
2025-09-03 10:52:45,417 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:42075
2025-09-03 10:52:45,417 - distributed.worker - INFO -          dashboard at:          10.6.105.10:44965
2025-09-03 10:52:45,417 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,417 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,417 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,417 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,417 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-72k4q4_0
2025-09-03 10:52:45,417 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,471 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:38879
2025-09-03 10:52:45,471 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:38879
2025-09-03 10:52:45,471 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:35287
2025-09-03 10:52:45,471 - distributed.worker - INFO -          dashboard at:          10.6.105.10:46105
2025-09-03 10:52:45,471 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,471 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:35287
2025-09-03 10:52:45,471 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,471 - distributed.worker - INFO -          dashboard at:          10.6.105.10:35063
2025-09-03 10:52:45,472 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,472 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,472 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,472 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,472 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-x4okugtx
2025-09-03 10:52:45,472 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,472 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,472 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,472 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-caisa5fp
2025-09-03 10:52:45,472 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,477 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:43637
2025-09-03 10:52:45,477 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:43637
2025-09-03 10:52:45,477 - distributed.worker - INFO -          dashboard at:          10.6.105.10:36161
2025-09-03 10:52:45,477 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,477 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,477 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,477 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,477 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-1gj0twwv
2025-09-03 10:52:45,477 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,606 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:36719
2025-09-03 10:52:45,606 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:36719
2025-09-03 10:52:45,606 - distributed.worker - INFO -          dashboard at:          10.6.105.10:33165
2025-09-03 10:52:45,606 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,606 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,606 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,606 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,606 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-v78dh1rv
2025-09-03 10:52:45,606 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,637 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:38053
2025-09-03 10:52:45,638 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:38053
2025-09-03 10:52:45,638 - distributed.worker - INFO -          dashboard at:          10.6.105.10:44989
2025-09-03 10:52:45,638 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,638 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,638 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,638 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,638 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-yj74ej1i
2025-09-03 10:52:45,638 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,642 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:45107
2025-09-03 10:52:45,643 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:45107
2025-09-03 10:52:45,643 - distributed.worker - INFO -          dashboard at:          10.6.105.10:42345
2025-09-03 10:52:45,643 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,643 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,643 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,643 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,643 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-3bz7abfg
2025-09-03 10:52:45,643 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,650 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:36973
2025-09-03 10:52:45,650 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:36973
2025-09-03 10:52:45,650 - distributed.worker - INFO -          dashboard at:          10.6.105.10:45257
2025-09-03 10:52:45,650 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,650 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,650 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,650 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,650 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-phpu2upj
2025-09-03 10:52:45,650 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,686 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:35475
2025-09-03 10:52:45,686 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:35475
2025-09-03 10:52:45,686 - distributed.worker - INFO -          dashboard at:          10.6.105.10:42281
2025-09-03 10:52:45,686 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,686 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,686 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,686 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,686 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-jyv6rbzw
2025-09-03 10:52:45,686 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,756 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:40207
2025-09-03 10:52:45,756 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:40207
2025-09-03 10:52:45,756 - distributed.worker - INFO -          dashboard at:          10.6.105.10:34421
2025-09-03 10:52:45,756 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,756 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,757 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,757 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,757 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-733sdom4
2025-09-03 10:52:45,757 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:33323
2025-09-03 10:52:45,757 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,757 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:33323
2025-09-03 10:52:45,757 - distributed.worker - INFO -          dashboard at:          10.6.105.10:45833
2025-09-03 10:52:45,757 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,757 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,757 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,757 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,757 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-gxrdvi8h
2025-09-03 10:52:45,757 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,838 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:45201
2025-09-03 10:52:45,838 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:45201
2025-09-03 10:52:45,838 - distributed.worker - INFO -          dashboard at:          10.6.105.10:41973
2025-09-03 10:52:45,838 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,838 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,838 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,838 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,838 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-63qwjmtk
2025-09-03 10:52:45,838 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,918 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:42681
2025-09-03 10:52:45,918 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:42681
2025-09-03 10:52:45,918 - distributed.worker - INFO -          dashboard at:          10.6.105.10:41377
2025-09-03 10:52:45,918 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,918 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,918 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,918 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,918 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-k5ad4gub
2025-09-03 10:52:45,918 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,929 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:43623
2025-09-03 10:52:45,929 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:43623
2025-09-03 10:52:45,929 - distributed.worker - INFO -          dashboard at:          10.6.105.10:42505
2025-09-03 10:52:45,929 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,929 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,929 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,929 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,929 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-kkxj2qnz
2025-09-03 10:52:45,930 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,956 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:33421
2025-09-03 10:52:45,956 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:33421
2025-09-03 10:52:45,956 - distributed.worker - INFO -          dashboard at:          10.6.105.10:41059
2025-09-03 10:52:45,956 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,956 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,956 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,956 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,956 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-flzgaqpp
2025-09-03 10:52:45,956 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,962 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:46045
2025-09-03 10:52:45,962 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:46045
2025-09-03 10:52:45,962 - distributed.worker - INFO -          dashboard at:          10.6.105.10:35999
2025-09-03 10:52:45,962 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,962 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,962 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,962 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,962 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-xgu72u8l
2025-09-03 10:52:45,962 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,979 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:34267
2025-09-03 10:52:45,979 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:34267
2025-09-03 10:52:45,979 - distributed.worker - INFO -          dashboard at:          10.6.105.10:44993
2025-09-03 10:52:45,979 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,979 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,979 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:45,979 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:45,979 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-p9409sy5
2025-09-03 10:52:45,979 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,027 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:35567
2025-09-03 10:52:46,027 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:35567
2025-09-03 10:52:46,027 - distributed.worker - INFO -          dashboard at:          10.6.105.10:35621
2025-09-03 10:52:46,027 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,027 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,027 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,027 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,027 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-0v3pat9a
2025-09-03 10:52:46,027 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,037 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:43647
2025-09-03 10:52:46,037 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:43647
2025-09-03 10:52:46,037 - distributed.worker - INFO -          dashboard at:          10.6.105.10:43415
2025-09-03 10:52:46,037 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,037 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,037 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,037 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,037 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ar_xoqwa
2025-09-03 10:52:46,037 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,047 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:35687
2025-09-03 10:52:46,047 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:35687
2025-09-03 10:52:46,047 - distributed.worker - INFO -          dashboard at:          10.6.105.10:42877
2025-09-03 10:52:46,047 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,047 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,047 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,048 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-mgtcsm3y
2025-09-03 10:52:46,048 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,060 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:44471
2025-09-03 10:52:46,060 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:44471
2025-09-03 10:52:46,060 - distributed.worker - INFO -          dashboard at:          10.6.105.10:45003
2025-09-03 10:52:46,060 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,060 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,060 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:33279
2025-09-03 10:52:46,060 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,060 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,060 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:33279
2025-09-03 10:52:46,061 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-rwwufg74
2025-09-03 10:52:46,061 - distributed.worker - INFO -          dashboard at:          10.6.105.10:40783
2025-09-03 10:52:46,061 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,061 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,061 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,061 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,061 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,061 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-rz4fjqim
2025-09-03 10:52:46,061 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,062 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:35237
2025-09-03 10:52:46,062 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:35237
2025-09-03 10:52:46,062 - distributed.worker - INFO -          dashboard at:          10.6.105.10:42733
2025-09-03 10:52:46,063 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,063 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,063 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,063 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,063 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-u_6bg8ga
2025-09-03 10:52:46,063 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,063 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:35413
2025-09-03 10:52:46,063 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:35413
2025-09-03 10:52:46,063 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:38363
2025-09-03 10:52:46,063 - distributed.worker - INFO -          dashboard at:          10.6.105.10:38043
2025-09-03 10:52:46,063 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:38363
2025-09-03 10:52:46,063 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,063 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,063 - distributed.worker - INFO -          dashboard at:          10.6.105.10:34907
2025-09-03 10:52:46,063 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,064 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,064 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,064 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,064 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,064 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-8gjnv4ln
2025-09-03 10:52:46,064 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,064 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,064 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-1kpydtur
2025-09-03 10:52:46,064 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,065 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:36921
2025-09-03 10:52:46,066 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:36921
2025-09-03 10:52:46,066 - distributed.worker - INFO -          dashboard at:          10.6.105.10:40667
2025-09-03 10:52:46,066 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,066 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,066 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,066 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,066 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-qdb0zqo9
2025-09-03 10:52:46,066 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,066 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:45411
2025-09-03 10:52:46,066 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:45411
2025-09-03 10:52:46,066 - distributed.worker - INFO -          dashboard at:          10.6.105.10:33497
2025-09-03 10:52:46,066 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,066 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,067 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,067 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,067 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-xyfll9br
2025-09-03 10:52:46,067 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,069 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:38831
2025-09-03 10:52:46,069 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:38831
2025-09-03 10:52:46,069 - distributed.worker - INFO -          dashboard at:          10.6.105.10:40943
2025-09-03 10:52:46,069 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,069 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,069 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,069 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,069 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-xfcnzhcn
2025-09-03 10:52:46,070 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,072 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:40193
2025-09-03 10:52:46,072 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:40193
2025-09-03 10:52:46,072 - distributed.worker - INFO -          dashboard at:          10.6.105.10:34327
2025-09-03 10:52:46,073 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,073 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,073 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,073 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,073 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-uawyd4j0
2025-09-03 10:52:46,073 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,074 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:33537
2025-09-03 10:52:46,074 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:33537
2025-09-03 10:52:46,074 - distributed.worker - INFO -          dashboard at:          10.6.105.10:42271
2025-09-03 10:52:46,074 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,074 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,074 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,074 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,074 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-50p32_xt
2025-09-03 10:52:46,074 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,075 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:41575
2025-09-03 10:52:46,075 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:41575
2025-09-03 10:52:46,075 - distributed.worker - INFO -          dashboard at:          10.6.105.10:43095
2025-09-03 10:52:46,075 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,075 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,075 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,075 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,075 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ttxcetx8
2025-09-03 10:52:46,075 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,079 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:35707
2025-09-03 10:52:46,079 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:35707
2025-09-03 10:52:46,079 - distributed.worker - INFO -          dashboard at:          10.6.105.10:43259
2025-09-03 10:52:46,079 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,079 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,079 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,079 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,079 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-flpocwa7
2025-09-03 10:52:46,079 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,080 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:42903
2025-09-03 10:52:46,080 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:42903
2025-09-03 10:52:46,080 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:35691
2025-09-03 10:52:46,080 - distributed.worker - INFO -          dashboard at:          10.6.105.10:34425
2025-09-03 10:52:46,080 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:35691
2025-09-03 10:52:46,080 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,080 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,080 - distributed.worker - INFO -          dashboard at:          10.6.105.10:37281
2025-09-03 10:52:46,080 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,080 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,080 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,080 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,080 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,080 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-h_s7nrd7
2025-09-03 10:52:46,080 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,080 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,080 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-k3_08r89
2025-09-03 10:52:46,080 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,083 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:41739
2025-09-03 10:52:46,083 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:41739
2025-09-03 10:52:46,083 - distributed.worker - INFO -          dashboard at:          10.6.105.10:45857
2025-09-03 10:52:46,084 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,084 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,084 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,084 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,084 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ifcxsbc6
2025-09-03 10:52:46,084 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,085 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:37945
2025-09-03 10:52:46,085 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:39349
2025-09-03 10:52:46,085 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:37945
2025-09-03 10:52:46,085 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:39349
2025-09-03 10:52:46,085 - distributed.worker - INFO -          dashboard at:          10.6.105.10:35845
2025-09-03 10:52:46,085 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,085 - distributed.worker - INFO -          dashboard at:          10.6.105.10:46605
2025-09-03 10:52:46,085 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,085 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,085 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,085 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,085 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,085 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,085 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,085 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-qg8tnuhe
2025-09-03 10:52:46,085 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:44877
2025-09-03 10:52:46,085 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-4srcqj8d
2025-09-03 10:52:46,085 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,085 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:44877
2025-09-03 10:52:46,085 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,085 - distributed.worker - INFO -          dashboard at:          10.6.105.10:46017
2025-09-03 10:52:46,085 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,085 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,085 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,085 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,085 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:44291
2025-09-03 10:52:46,085 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-kdd3ljfs
2025-09-03 10:52:46,085 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:44291
2025-09-03 10:52:46,085 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,085 - distributed.worker - INFO -          dashboard at:          10.6.105.10:46189
2025-09-03 10:52:46,085 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,085 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,085 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,085 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,085 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-8hi5pl58
2025-09-03 10:52:46,085 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,088 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:40069
2025-09-03 10:52:46,088 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:40069
2025-09-03 10:52:46,088 - distributed.worker - INFO -          dashboard at:          10.6.105.10:34999
2025-09-03 10:52:46,088 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,088 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,088 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,088 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,088 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-qpmno3ct
2025-09-03 10:52:46,088 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,090 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:39391
2025-09-03 10:52:46,090 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:39391
2025-09-03 10:52:46,090 - distributed.worker - INFO -          dashboard at:          10.6.105.10:45637
2025-09-03 10:52:46,090 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,090 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,090 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,090 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,090 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-3q0a3tym
2025-09-03 10:52:46,090 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,093 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:41341
2025-09-03 10:52:46,093 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:41341
2025-09-03 10:52:46,093 - distributed.worker - INFO -          dashboard at:          10.6.105.10:36975
2025-09-03 10:52:46,093 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,093 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,093 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,093 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,093 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-k2ki5_o_
2025-09-03 10:52:46,093 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,094 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.10:34793
2025-09-03 10:52:46,094 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.10:34793
2025-09-03 10:52:46,094 - distributed.worker - INFO -          dashboard at:          10.6.105.10:43827
2025-09-03 10:52:46,094 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,094 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,094 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:46,094 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:46,094 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-h08ug8rz
2025-09-03 10:52:46,094 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,052 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:47,053 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,053 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,055 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:47,172 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:47,173 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,175 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:47,202 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:47,203 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,203 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,205 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:47,888 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:47,890 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,890 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,891 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:47,904 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:47,905 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,905 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,907 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:47,997 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:47,998 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,999 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,000 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,031 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,032 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,033 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,034 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,046 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,048 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,048 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,050 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,062 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,063 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,063 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,065 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,077 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,078 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,078 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,080 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,092 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,093 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,093 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,095 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,143 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,144 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,144 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,146 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,158 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,159 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,159 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,161 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,174 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,175 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,175 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,177 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,189 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,190 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,190 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,192 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,205 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,206 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,206 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,208 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,220 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,221 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,221 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,223 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,236 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,237 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,239 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,814 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,815 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,815 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,817 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,926 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,928 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,928 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,930 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,942 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,943 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,943 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,945 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,958 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,959 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,960 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,961 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,973 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,974 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,975 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,976 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,988 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,989 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,990 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,991 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:51,098 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:51,100 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,100 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,102 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:51,114 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:51,116 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,116 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,117 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:51,193 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:51,194 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,196 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:51,430 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:51,432 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,432 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,434 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:51,446 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:51,447 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,448 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,449 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:51,462 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:51,463 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,463 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,465 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:51,477 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:51,478 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,478 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,480 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:51,492 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:51,494 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,494 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,496 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:51,508 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:51,509 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,509 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,511 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:51,524 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:51,525 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,525 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,527 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:51,539 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:51,541 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,541 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,543 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:51,554 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:51,556 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,556 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,558 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:51,586 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:51,587 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,588 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,589 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:51,633 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:51,634 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,635 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,636 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:51,664 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:51,666 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,666 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,667 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:51,696 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:51,698 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,698 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,699 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:52,226 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:52,228 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,228 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,230 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:52,829 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:52,830 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,831 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,832 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:52,846 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:52,847 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,847 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,849 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:52,862 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:52,863 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,863 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,865 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:52,877 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:52,879 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,879 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,880 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:52,894 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:52,895 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,895 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,897 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:52,909 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:52,911 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,911 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,913 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:52,925 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:52,927 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,927 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,928 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:52,941 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:52,942 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,942 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,944 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:52,957 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:52,958 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,958 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,960 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,020 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,022 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,022 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,023 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,036 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,038 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,038 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,040 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,052 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,054 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,054 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,056 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,084 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,086 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,086 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,087 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,100 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,101 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,101 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,103 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,115 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,117 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,117 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,119 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,132 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,133 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,133 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,135 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,147 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,149 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,149 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,150 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,163 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,165 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,165 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,167 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,179 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,181 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,181 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,182 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,195 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,197 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,197 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,198 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,211 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,212 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,212 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,213 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,227 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,229 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,229 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,231 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,243 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,245 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,247 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,259 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,261 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,261 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,263 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,279 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,281 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,281 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,282 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,295 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,296 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,296 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,298 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,311 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,312 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,313 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,315 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,327 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,328 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,329 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,330 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,343 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,345 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,345 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,346 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,969 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,970 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,970 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,972 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,984 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,986 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,986 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,987 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,001 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,015 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,016 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,016 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,018 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,018 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,020 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,024 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,033 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,034 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,035 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,036 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,068 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,070 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,070 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,072 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,086 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,088 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,088 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,090 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,103 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,105 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,105 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,107 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,120 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,122 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,122 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,124 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:07,991 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:07,992 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:07,992 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:07,994 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:08,008 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:08,009 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:08,010 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:08,011 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:08,025 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:08,026 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:08,026 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:08,028 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:08,041 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:08,043 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:08,043 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:08,045 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:08,058 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:08,060 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:08,060 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:08,062 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:08,075 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:08,076 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:08,076 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:08,078 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:08,091 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:08,093 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:08,093 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:08,094 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:08,108 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:08,110 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:08,110 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:08,111 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:08,125 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:08,126 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:08,126 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:08,128 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:08,143 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:08,148 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:08,148 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:08,153 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:08,158 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:08,160 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:08,160 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:08,162 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:08,175 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:08,177 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:08,177 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:08,178 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:08,192 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:08,193 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:08,194 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:08,195 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:08,209 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:08,210 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:08,211 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:08,212 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:24,977 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:24,990 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:25,023 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:25,022 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:32,075 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:32,092 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:32,110 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:32,127 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:39,080 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:39,096 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:39,117 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:39,131 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:39,151 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:39,163 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:39,180 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:39,198 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:39,215 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:56:38,267 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,272 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,480 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,486 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,755 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,757 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,893 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,897 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,171 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,176 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,222 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,223 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,232 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,233 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,309 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,311 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,465 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,471 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,545 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,546 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,602 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,607 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,663 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,664 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,986 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,988 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,014 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,020 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,064 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,070 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,102 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,104 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,363 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,368 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,776 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,777 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,411 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,413 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,621 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,626 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,820 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,822 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,888 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,896 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,039 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,041 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,160 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,162 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,684 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,685 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,818 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,820 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,848 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,849 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,141 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,142 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,518 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,521 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,117 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,122 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,572 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,575 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,621 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,622 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,686 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,693 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,821 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,826 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,064 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,066 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,261 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,266 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,343 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,349 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,376 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,383 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,637 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,640 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,877 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,879 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,902 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,905 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,079 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,081 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,328 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,330 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,480 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,484 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,528 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,536 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,587 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,588 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,591 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,595 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,606 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,608 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,795 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,796 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,867 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,869 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,874 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,880 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,986 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,987 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,321 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,327 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,484 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,490 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,286 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,287 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,288 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,288 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,288 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,292 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,359 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,364 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,442 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,447 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,689 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,693 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,711 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,717 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,756 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,762 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,960 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,961 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,121 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,123 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,660 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,666 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,850 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,851 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,075 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,077 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,160 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,164 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,376 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,377 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,418 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,423 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,527 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,532 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,916 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,922 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,057 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,057 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,058 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,063 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,239 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,240 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,244 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,249 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,420 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,429 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,441 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,446 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,757 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,759 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,763 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,765 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,775 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,780 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,146 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,148 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,680 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,687 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,423 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,425 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,491 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,492 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,017 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,018 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,048 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,051 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,052 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,054 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,076 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,082 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,345 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,350 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,412 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,418 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:56,695 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:56,701 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:57,556 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:57,557 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:58,525 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:58,526 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:59,537 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:59,539 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:59,760 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:59,765 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:00,194 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:00,196 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:00,273 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:00,274 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:00,572 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:00,578 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:01,075 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:01,077 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:01,775 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:01,781 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:02,085 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:02,086 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:03,687 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:03,689 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:04,048 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:04,050 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:09,992 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:09,994 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,002 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,004 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,017 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,019 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,022 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,025 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,337 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,340 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,364 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,367 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,386 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,388 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,496 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,498 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,503 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,505 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,511 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,516 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,516 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,518 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,520 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,525 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,528 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,530 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,542 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,544 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,619 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,626 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,647 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,649 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,662 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,664 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,666 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,668 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,749 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,754 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,756 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,758 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,792 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,794 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,880 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,883 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,890 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,892 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,113 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,115 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,123 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,125 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,130 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,132 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,132 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,134 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,165 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,167 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,167 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,169 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,170 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,172 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,263 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,264 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,264 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,266 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,266 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,271 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,276 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,279 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,288 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,295 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,296 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,298 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,298 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,300 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,301 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,302 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,304 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,384 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,386 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,392 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,395 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,462 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,464 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,515 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,516 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,517 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,518 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,586 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,595 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,598 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,598 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,602 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,602 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,604 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,604 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,608 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,610 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,616 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,618 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,688 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,690 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,701 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,703 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,749 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,750 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,754 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,751 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,771 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,773 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,812 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,814 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,824 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,826 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,860 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,867 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,891 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,890 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,893 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,895 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,968 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,970 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,019 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,022 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,061 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,063 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,126 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,128 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,181 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,183 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,284 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,289 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,291 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,293 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,293 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,298 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,305 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,307 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,348 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,353 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,415 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,420 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,473 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,475 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,503 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,505 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,550 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,552 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,623 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,625 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,681 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,683 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,745 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,748 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,753 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,755 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,796 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,798 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,819 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,826 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,840 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,842 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,879 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,882 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,883 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,885 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,895 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,896 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,921 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,923 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,952 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,954 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,980 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,982 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,040 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,042 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,109 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,112 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,208 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,210 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,239 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,246 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,263 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,265 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,300 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,302 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,328 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,330 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,764 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,766 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,837 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,839 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,858 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,860 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:14,932 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:14,934 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:14,945 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:14,948 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:15,013 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:15,015 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:15,297 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:15,299 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:15,462 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:15,464 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:15,693 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:15,695 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:16,275 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:16,277 - distributed.utils - INFO - Reload module qme_vars from .py file
