Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-05 09:11:41,058 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:42527'
2025-09-05 09:11:41,067 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:43657'
2025-09-05 09:11:41,070 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:40329'
2025-09-05 09:11:41,075 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:43699'
2025-09-05 09:11:41,079 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:43729'
2025-09-05 09:11:41,083 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:41009'
2025-09-05 09:11:41,087 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:36119'
2025-09-05 09:11:41,091 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:39825'
2025-09-05 09:11:41,095 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:43509'
2025-09-05 09:11:41,100 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:41709'
2025-09-05 09:11:41,104 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:44261'
2025-09-05 09:11:41,107 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:45439'
2025-09-05 09:11:41,112 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:43589'
2025-09-05 09:11:41,116 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:37379'
2025-09-05 09:11:41,120 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:43231'
2025-09-05 09:11:41,124 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:33847'
2025-09-05 09:11:41,129 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:34705'
2025-09-05 09:11:41,135 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:46343'
2025-09-05 09:11:41,139 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:37533'
2025-09-05 09:11:41,143 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:41531'
2025-09-05 09:11:41,211 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:43053'
2025-09-05 09:11:41,215 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:39745'
2025-09-05 09:11:41,219 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:44383'
2025-09-05 09:11:41,222 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:39247'
2025-09-05 09:11:41,226 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:39977'
2025-09-05 09:11:41,231 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:42827'
2025-09-05 09:11:41,236 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:44279'
2025-09-05 09:11:41,240 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:32845'
2025-09-05 09:11:41,244 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:45219'
2025-09-05 09:11:41,249 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:39899'
2025-09-05 09:11:41,254 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:44013'
2025-09-05 09:11:41,258 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:34139'
2025-09-05 09:11:41,262 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:33903'
2025-09-05 09:11:41,267 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:45517'
2025-09-05 09:11:41,271 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:44731'
2025-09-05 09:11:41,275 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:46581'
2025-09-05 09:11:41,280 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:34971'
2025-09-05 09:11:41,284 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:40253'
2025-09-05 09:11:41,289 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:36679'
2025-09-05 09:11:41,294 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:32839'
2025-09-05 09:11:41,299 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:32969'
2025-09-05 09:11:41,302 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:35845'
2025-09-05 09:11:41,308 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:41891'
2025-09-05 09:11:41,312 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:46695'
2025-09-05 09:11:41,319 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:32867'
2025-09-05 09:11:41,324 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:44839'
2025-09-05 09:11:41,327 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:38075'
2025-09-05 09:11:41,331 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:38655'
2025-09-05 09:11:41,334 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:41181'
2025-09-05 09:11:41,339 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:46253'
2025-09-05 09:11:41,347 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:38211'
2025-09-05 09:11:41,351 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.54:42355'
2025-09-05 09:11:42,522 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:36175
2025-09-05 09:11:42,522 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:40645
2025-09-05 09:11:42,522 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:36701
2025-09-05 09:11:42,522 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:37667
2025-09-05 09:11:42,522 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:34881
2025-09-05 09:11:42,522 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:40255
2025-09-05 09:11:42,522 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:43543
2025-09-05 09:11:42,522 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:42707
2025-09-05 09:11:42,522 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:36175
2025-09-05 09:11:42,522 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:44295
2025-09-05 09:11:42,522 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:37051
2025-09-05 09:11:42,523 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:40645
2025-09-05 09:11:42,522 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:35221
2025-09-05 09:11:42,522 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:45095
2025-09-05 09:11:42,522 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:38091
2025-09-05 09:11:42,522 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:46781
2025-09-05 09:11:42,523 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:36701
2025-09-05 09:11:42,523 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:35663
2025-09-05 09:11:42,523 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:45943
2025-09-05 09:11:42,523 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:36735
2025-09-05 09:11:42,523 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:37667
2025-09-05 09:11:42,523 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:44351
2025-09-05 09:11:42,523 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:34881
2025-09-05 09:11:42,523 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:33409
2025-09-05 09:11:42,523 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:40255
2025-09-05 09:11:42,523 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:43543
2025-09-05 09:11:42,523 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:42707
2025-09-05 09:11:42,523 - distributed.worker - INFO -          dashboard at:          10.6.101.54:42201
2025-09-05 09:11:42,523 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:44295
2025-09-05 09:11:42,523 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:37051
2025-09-05 09:11:42,523 - distributed.worker - INFO -          dashboard at:          10.6.101.54:46293
2025-09-05 09:11:42,523 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:35221
2025-09-05 09:11:42,523 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:45095
2025-09-05 09:11:42,523 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:38091
2025-09-05 09:11:42,523 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:46781
2025-09-05 09:11:42,523 - distributed.worker - INFO -          dashboard at:          10.6.101.54:38125
2025-09-05 09:11:42,523 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:39075
2025-09-05 09:11:42,523 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:35663
2025-09-05 09:11:42,523 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:45943
2025-09-05 09:11:42,523 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:36735
2025-09-05 09:11:42,523 - distributed.worker - INFO -          dashboard at:          10.6.101.54:46377
2025-09-05 09:11:42,523 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:44351
2025-09-05 09:11:42,523 - distributed.worker - INFO -          dashboard at:          10.6.101.54:33993
2025-09-05 09:11:42,523 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:33409
2025-09-05 09:11:42,523 - distributed.worker - INFO -          dashboard at:          10.6.101.54:41487
2025-09-05 09:11:42,523 - distributed.worker - INFO -          dashboard at:          10.6.101.54:42519
2025-09-05 09:11:42,523 - distributed.worker - INFO -          dashboard at:          10.6.101.54:34131
2025-09-05 09:11:42,523 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,523 - distributed.worker - INFO -          dashboard at:          10.6.101.54:43519
2025-09-05 09:11:42,523 - distributed.worker - INFO -          dashboard at:          10.6.101.54:42561
2025-09-05 09:11:42,523 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,523 - distributed.worker - INFO -          dashboard at:          10.6.101.54:39657
2025-09-05 09:11:42,523 - distributed.worker - INFO -          dashboard at:          10.6.101.54:42539
2025-09-05 09:11:42,523 - distributed.worker - INFO -          dashboard at:          10.6.101.54:45569
2025-09-05 09:11:42,523 - distributed.worker - INFO -          dashboard at:          10.6.101.54:37655
2025-09-05 09:11:42,523 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,523 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:39075
2025-09-05 09:11:42,523 - distributed.worker - INFO -          dashboard at:          10.6.101.54:40801
2025-09-05 09:11:42,523 - distributed.worker - INFO -          dashboard at:          10.6.101.54:38769
2025-09-05 09:11:42,523 - distributed.worker - INFO -          dashboard at:          10.6.101.54:32957
2025-09-05 09:11:42,523 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,523 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,523 - distributed.worker - INFO -          dashboard at:          10.6.101.54:35765
2025-09-05 09:11:42,523 - distributed.worker - INFO -          dashboard at:          10.6.101.54:33179
2025-09-05 09:11:42,523 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,523 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,523 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,523 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,523 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,523 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,523 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,523 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,523 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,523 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,523 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,523 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,523 - distributed.worker - INFO -          dashboard at:          10.6.101.54:43439
2025-09-05 09:11:42,523 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,523 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,523 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,523 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,523 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,523 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,523 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,523 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,523 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,523 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,523 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,523 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,523 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,523 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,523 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,523 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,523 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,523 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,523 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,523 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,523 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,524 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,524 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,524 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,524 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,524 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,524 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,524 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,524 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,524 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,524 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,524 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,524 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,524 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-bqmoeshd
2025-09-05 09:11:42,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,524 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-4ua6297m
2025-09-05 09:11:42,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,524 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-hhmdb7f1
2025-09-05 09:11:42,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,524 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,524 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-01nyv1bm
2025-09-05 09:11:42,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,524 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-dco07d_b
2025-09-05 09:11:42,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,524 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ms8ydwfd
2025-09-05 09:11:42,524 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-18dxhw18
2025-09-05 09:11:42,524 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-p28uzhhu
2025-09-05 09:11:42,524 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-i33urc71
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-j_atwb47
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-q6yr82v5
2025-09-05 09:11:42,524 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-yz3isve0
2025-09-05 09:11:42,524 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-hdvec7bn
2025-09-05 09:11:42,524 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-t7p5ijs9
2025-09-05 09:11:42,524 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-q9zc03ta
2025-09-05 09:11:42,524 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-8a8xz6v6
2025-09-05 09:11:42,524 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-u2_u_piy
2025-09-05 09:11:42,524 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-v7rpqix0
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-la1_p7gv
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-7wvf4gyl
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,525 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:42887
2025-09-05 09:11:42,525 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:42887
2025-09-05 09:11:42,525 - distributed.worker - INFO -          dashboard at:          10.6.101.54:38459
2025-09-05 09:11:42,525 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,525 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,525 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,525 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,525 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-473j70aq
2025-09-05 09:11:42,525 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,531 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:45657
2025-09-05 09:11:42,531 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:45657
2025-09-05 09:11:42,532 - distributed.worker - INFO -          dashboard at:          10.6.101.54:44987
2025-09-05 09:11:42,532 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,532 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,532 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,532 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,532 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-svma2_4q
2025-09-05 09:11:42,532 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,548 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,548 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,549 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,549 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,553 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,553 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,553 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,554 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,556 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,557 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,557 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,558 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,560 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,560 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,560 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,561 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,564 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,565 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,565 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,566 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,567 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,568 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,568 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,570 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,570 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,571 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,571 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,572 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,573 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,573 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,573 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,574 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,574 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,575 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:43563
2025-09-05 09:11:42,575 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:43563
2025-09-05 09:11:42,575 - distributed.worker - INFO -          dashboard at:          10.6.101.54:35539
2025-09-05 09:11:42,575 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,575 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,575 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,575 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,575 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-j7i4m15a
2025-09-05 09:11:42,575 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,575 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,576 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,576 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:46509
2025-09-05 09:11:42,576 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:46509
2025-09-05 09:11:42,576 - distributed.worker - INFO -          dashboard at:          10.6.101.54:39969
2025-09-05 09:11:42,576 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,576 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,576 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,576 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,576 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-40eada8d
2025-09-05 09:11:42,576 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,576 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,577 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,578 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,578 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,578 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,579 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,579 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,580 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,580 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,581 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,581 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,581 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,582 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,583 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,583 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,583 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,584 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,585 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,585 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,585 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,585 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,587 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,587 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,587 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,587 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,588 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,588 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,588 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,588 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,589 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,589 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,590 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,590 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,591 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,591 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,591 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,591 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,592 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,592 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,593 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,593 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,594 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,594 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,594 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,595 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,596 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,596 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,596 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,596 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,597 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,598 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,598 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,599 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,617 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,618 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,618 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,619 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,620 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,620 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,620 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,621 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,624 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:43233
2025-09-05 09:11:42,624 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:43233
2025-09-05 09:11:42,625 - distributed.worker - INFO -          dashboard at:          10.6.101.54:39917
2025-09-05 09:11:42,625 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,625 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,625 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,625 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,625 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-9i9mi1v6
2025-09-05 09:11:42,625 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,632 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:40569
2025-09-05 09:11:42,632 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:40569
2025-09-05 09:11:42,632 - distributed.worker - INFO -          dashboard at:          10.6.101.54:32871
2025-09-05 09:11:42,632 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,632 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,632 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,632 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,632 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-y3paq9w1
2025-09-05 09:11:42,633 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,645 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,646 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,646 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,648 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,655 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,656 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,656 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,658 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,831 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:39645
2025-09-05 09:11:42,831 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:39645
2025-09-05 09:11:42,832 - distributed.worker - INFO -          dashboard at:          10.6.101.54:43997
2025-09-05 09:11:42,832 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,832 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,832 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,832 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,832 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-cx0z4pp1
2025-09-05 09:11:42,832 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,840 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:42051
2025-09-05 09:11:42,840 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:42051
2025-09-05 09:11:42,840 - distributed.worker - INFO -          dashboard at:          10.6.101.54:34377
2025-09-05 09:11:42,840 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,840 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,840 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,840 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,840 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-dkv620ze
2025-09-05 09:11:42,840 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,852 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:36537
2025-09-05 09:11:42,852 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:36537
2025-09-05 09:11:42,852 - distributed.worker - INFO -          dashboard at:          10.6.101.54:41565
2025-09-05 09:11:42,852 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,852 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,852 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,852 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,852 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-__sibtjn
2025-09-05 09:11:42,852 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,853 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,853 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,853 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,854 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:39405
2025-09-05 09:11:42,854 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,854 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:39405
2025-09-05 09:11:42,854 - distributed.worker - INFO -          dashboard at:          10.6.101.54:33733
2025-09-05 09:11:42,854 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,854 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,854 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,854 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,854 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-oavx56x1
2025-09-05 09:11:42,854 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,860 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,861 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,861 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,863 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,865 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:38799
2025-09-05 09:11:42,866 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:38799
2025-09-05 09:11:42,866 - distributed.worker - INFO -          dashboard at:          10.6.101.54:33225
2025-09-05 09:11:42,866 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,866 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:44513
2025-09-05 09:11:42,866 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,866 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,866 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:44513
2025-09-05 09:11:42,866 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,866 - distributed.worker - INFO -          dashboard at:          10.6.101.54:41045
2025-09-05 09:11:42,866 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-qdkz_b2p
2025-09-05 09:11:42,866 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,866 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,866 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,866 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,866 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,866 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-h2rhiwtk
2025-09-05 09:11:42,866 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,868 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:37769
2025-09-05 09:11:42,868 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:37769
2025-09-05 09:11:42,868 - distributed.worker - INFO -          dashboard at:          10.6.101.54:46711
2025-09-05 09:11:42,868 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,868 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,868 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,868 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,868 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-gv3sfaty
2025-09-05 09:11:42,868 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,870 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:34207
2025-09-05 09:11:42,870 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:34207
2025-09-05 09:11:42,870 - distributed.worker - INFO -          dashboard at:          10.6.101.54:37495
2025-09-05 09:11:42,870 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,870 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,870 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,870 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,870 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-68md_0me
2025-09-05 09:11:42,871 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,872 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:46059
2025-09-05 09:11:42,872 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:46059
2025-09-05 09:11:42,872 - distributed.worker - INFO -          dashboard at:          10.6.101.54:35493
2025-09-05 09:11:42,872 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,872 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,872 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,872 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,872 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-pr6epnc0
2025-09-05 09:11:42,872 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,875 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:38567
2025-09-05 09:11:42,875 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:38567
2025-09-05 09:11:42,875 - distributed.worker - INFO -          dashboard at:          10.6.101.54:41649
2025-09-05 09:11:42,875 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,875 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,875 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,875 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,875 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-gt6s5b9o
2025-09-05 09:11:42,875 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,877 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,878 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,878 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,878 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:45903
2025-09-05 09:11:42,878 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:45903
2025-09-05 09:11:42,878 - distributed.worker - INFO -          dashboard at:          10.6.101.54:38927
2025-09-05 09:11:42,878 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,878 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,878 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,878 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,878 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-er1nyk8q
2025-09-05 09:11:42,878 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,879 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,882 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:34585
2025-09-05 09:11:42,882 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:34585
2025-09-05 09:11:42,882 - distributed.worker - INFO -          dashboard at:          10.6.101.54:36097
2025-09-05 09:11:42,882 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,882 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,882 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,882 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,882 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-0fbmu0z_
2025-09-05 09:11:42,882 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,883 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:42409
2025-09-05 09:11:42,883 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:42409
2025-09-05 09:11:42,883 - distributed.worker - INFO -          dashboard at:          10.6.101.54:35769
2025-09-05 09:11:42,883 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,883 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,883 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,883 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,883 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-8afenyu7
2025-09-05 09:11:42,883 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,884 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,885 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:37473
2025-09-05 09:11:42,885 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,885 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:37473
2025-09-05 09:11:42,885 - distributed.worker - INFO -          dashboard at:          10.6.101.54:38365
2025-09-05 09:11:42,885 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,885 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,885 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,885 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,885 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,885 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-jbsn5ofi
2025-09-05 09:11:42,885 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,886 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:33907
2025-09-05 09:11:42,886 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:33907
2025-09-05 09:11:42,886 - distributed.worker - INFO -          dashboard at:          10.6.101.54:45133
2025-09-05 09:11:42,886 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,886 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,886 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,886 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,886 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,886 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-qq1meu9y
2025-09-05 09:11:42,886 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,887 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:40075
2025-09-05 09:11:42,887 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:40075
2025-09-05 09:11:42,887 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,887 - distributed.worker - INFO -          dashboard at:          10.6.101.54:39993
2025-09-05 09:11:42,887 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,887 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,887 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,887 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,887 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-h3cuzrpx
2025-09-05 09:11:42,887 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,887 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,887 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,888 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,892 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,892 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:45479
2025-09-05 09:11:42,892 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:45479
2025-09-05 09:11:42,892 - distributed.worker - INFO -          dashboard at:          10.6.101.54:39747
2025-09-05 09:11:42,892 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,892 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,892 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,892 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,892 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,893 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-70nf9fjw
2025-09-05 09:11:42,893 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,893 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,894 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,894 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,895 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,895 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:37327
2025-09-05 09:11:42,896 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:37327
2025-09-05 09:11:42,895 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,896 - distributed.worker - INFO -          dashboard at:          10.6.101.54:34343
2025-09-05 09:11:42,896 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,896 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,896 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,896 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,896 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-wvd7__v4
2025-09-05 09:11:42,896 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,897 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,898 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:42865
2025-09-05 09:11:42,898 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:42865
2025-09-05 09:11:42,898 - distributed.worker - INFO -          dashboard at:          10.6.101.54:43917
2025-09-05 09:11:42,898 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,898 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,898 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,898 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,898 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-nq16h9bj
2025-09-05 09:11:42,898 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,898 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,898 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:43085
2025-09-05 09:11:42,898 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:43085
2025-09-05 09:11:42,898 - distributed.worker - INFO -          dashboard at:          10.6.101.54:40153
2025-09-05 09:11:42,898 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,898 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,899 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,898 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:33073
2025-09-05 09:11:42,899 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,899 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:33073
2025-09-05 09:11:42,899 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-q55pk8uf
2025-09-05 09:11:42,899 - distributed.worker - INFO -          dashboard at:          10.6.101.54:39809
2025-09-05 09:11:42,899 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,899 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,899 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,899 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,899 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,899 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-3rwm47p_
2025-09-05 09:11:42,899 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,899 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:36271
2025-09-05 09:11:42,899 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:36271
2025-09-05 09:11:42,899 - distributed.worker - INFO -          dashboard at:          10.6.101.54:37873
2025-09-05 09:11:42,899 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,899 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,899 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,899 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,899 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-r7n875kd
2025-09-05 09:11:42,899 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,899 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,899 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,901 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,902 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:43819
2025-09-05 09:11:42,902 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:43819
2025-09-05 09:11:42,902 - distributed.worker - INFO -          dashboard at:          10.6.101.54:46083
2025-09-05 09:11:42,902 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,902 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,902 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,902 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,902 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ssplna_3
2025-09-05 09:11:42,902 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,902 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,903 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,903 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,904 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,904 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,904 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,905 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,905 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,905 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:40671
2025-09-05 09:11:42,905 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:40671
2025-09-05 09:11:42,905 - distributed.worker - INFO -          dashboard at:          10.6.101.54:43731
2025-09-05 09:11:42,905 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,905 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,905 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,905 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,905 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-53k4unb6
2025-09-05 09:11:42,905 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,911 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,912 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,912 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,913 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,913 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,914 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,914 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,915 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,917 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:39873
2025-09-05 09:11:42,917 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:39873
2025-09-05 09:11:42,917 - distributed.worker - INFO -          dashboard at:          10.6.101.54:35029
2025-09-05 09:11:42,917 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,917 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,917 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,917 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,917 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-u20y1ajt
2025-09-05 09:11:42,917 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,918 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,919 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,919 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,920 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,923 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,924 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,924 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,925 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,926 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,926 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,926 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,926 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,927 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,928 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,928 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,928 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,929 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,929 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,929 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,930 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,931 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,931 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,931 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,931 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,931 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.54:44305
2025-09-05 09:11:42,931 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,931 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.54:44305
2025-09-05 09:11:42,931 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,931 - distributed.worker - INFO -          dashboard at:          10.6.101.54:35867
2025-09-05 09:11:42,931 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,931 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,931 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:42,931 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:42,931 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-0_bkt0fu
2025-09-05 09:11:42,931 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,932 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,932 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,935 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,936 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,936 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,937 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,937 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,938 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,939 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,939 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,940 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,940 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,940 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,940 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,941 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,942 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,942 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,943 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,943 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,944 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,944 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,945 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,954 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,955 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,955 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,956 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:42,961 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:42,962 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:42,962 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:42,963 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:12:06,664 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,664 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,664 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,664 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,664 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,664 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,664 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,665 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,665 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,665 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,665 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,665 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,665 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,666 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,666 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,666 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,666 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,666 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,666 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,666 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,666 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,666 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,667 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,666 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,667 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,667 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,667 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,667 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,667 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,667 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,667 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,667 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,667 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,667 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,667 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,668 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,667 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,667 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,668 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,668 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,668 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,668 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,668 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,668 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,668 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,668 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,668 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,668 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,669 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,669 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,668 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,669 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,669 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,669 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,669 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,669 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,669 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,669 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,669 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,669 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,669 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,669 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,669 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,669 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,670 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,670 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,670 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,670 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,671 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,671 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,671 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,671 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,669 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,671 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,669 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,669 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,671 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,672 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,672 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,672 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,672 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,672 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,672 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,672 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,672 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,672 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,672 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,670 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,672 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,671 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,671 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,672 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,673 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,674 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,677 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,679 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,679 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,680 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,680 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,682 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,682 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,682 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,681 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,685 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:09,465 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,465 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,465 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,465 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,466 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,466 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,466 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,465 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,466 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,466 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,466 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,466 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,467 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,467 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,467 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,467 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,467 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,467 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,467 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,468 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,468 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,468 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,468 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,468 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,468 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,468 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,468 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,468 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,468 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,468 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,468 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,468 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,468 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,469 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,466 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,468 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,469 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,466 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,469 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,469 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,469 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,469 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,467 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,469 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,467 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,469 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,469 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,469 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,469 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,469 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,469 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,469 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,470 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,470 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,470 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,469 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,470 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,470 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,470 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,469 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,470 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,470 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,470 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,471 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,471 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,471 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,471 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,468 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,469 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,471 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,471 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,471 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,471 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,470 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,469 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,472 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,472 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,472 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,472 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,471 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,469 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,472 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,472 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,472 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,472 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,470 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,473 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,473 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,473 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,473 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,473 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,473 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,473 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,474 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,471 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,474 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,471 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,474 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,474 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,474 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,474 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,475 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,476 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,476 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,848 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,848 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,848 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,848 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,848 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,848 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,849 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,849 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,849 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,849 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,849 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,849 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,849 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,849 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,850 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,850 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,850 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,850 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,850 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,850 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,850 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,850 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,851 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,850 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,851 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,851 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,851 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,851 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,851 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,851 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,851 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,852 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,852 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,852 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,852 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,852 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,852 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,852 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,852 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,852 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,852 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,853 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,853 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,853 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,853 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,853 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,853 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,853 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,853 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,853 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,853 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,853 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,853 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,854 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,854 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,854 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,854 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,854 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,854 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,854 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,854 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,854 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,854 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,854 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,854 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,854 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,855 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,855 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,855 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,855 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,855 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,855 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,855 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,855 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,856 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,856 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,856 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,856 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,856 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,856 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,857 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,857 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,857 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,857 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,857 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,857 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,858 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,858 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,858 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,858 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,858 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,858 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,858 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,858 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,858 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,858 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,858 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,858 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,858 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,858 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,859 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,859 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,859 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,859 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:10,307 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,307 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,307 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,307 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,307 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,307 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,307 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,308 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,308 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,308 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,308 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,308 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,308 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,308 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,308 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,309 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,309 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,309 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,309 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,309 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,309 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,309 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,309 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,309 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,309 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,310 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,310 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,310 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,310 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,310 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,310 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,310 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,310 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,310 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,310 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,310 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,310 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,311 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,310 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,310 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,310 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,310 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,310 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,310 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,310 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,311 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,311 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,311 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,311 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,311 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,311 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,311 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,311 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,311 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,311 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,311 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,311 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,311 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,311 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,311 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,311 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,311 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,312 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,312 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,312 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,312 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,312 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,312 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,312 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,312 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,312 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,312 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,312 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,312 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,312 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,312 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,312 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,312 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,313 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,313 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,313 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,313 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,313 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,313 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,313 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,313 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,314 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,314 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,314 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,314 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,314 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,314 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,315 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,315 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,315 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,315 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,315 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,315 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,316 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,316 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,316 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,316 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,316 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,316 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:16:17,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,634 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,634 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,719 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,719 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,723 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,741 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,748 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,751 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,796 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,802 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:48,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:48,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:51,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:51,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:53,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:53,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:53,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:53,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:53,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:55,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:55,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:55,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:57,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:57,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,660 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,665 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:59,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:59,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:00,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:00,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:00,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:02,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:05,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:09,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:09,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:10,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:11,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:11,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:11,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,922 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:21,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:21,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:22,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:22,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:28,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:28,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:28,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:29,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,367 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:33,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:33,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:33,748 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,287 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,748 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:39,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:39,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:40,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,796 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:51,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:51,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,786 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,130 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:55,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:55,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:55,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:55,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:56,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:56,741 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:58,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:59,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:00,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:09,313 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:09,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:12,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,667 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,917 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:18,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:32,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,190 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:38,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,667 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,754 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:40,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:40,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:40,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:40,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:40,543 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:40,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:40,830 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:40,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:40,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:42,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,559 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,187 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:49,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:49,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:49,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:49,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:52,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:53,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:53,543 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:53,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:00,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:06,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:07,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:07,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:07,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:07,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:07,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:07,748 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,239 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,250 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:15,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:19,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:20,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:20,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:20,551 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:32,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:34,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:34,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:35,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:35,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:35,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:35,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,305 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,530 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,883 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,873 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,381 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:47,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,981 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,067 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,355 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,608 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,848 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:51,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:51,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:53,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:56,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:58,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:58,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:59,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:10,884 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.101.37:45023
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2876, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.101.54:46116 remote=tcp://10.6.101.37:45023>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:20:11,276 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,276 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:34881. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,275 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,275 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,275 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,275 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,275 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,275 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,277 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:44305. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,275 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,275 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,276 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,275 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,275 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,277 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:40671. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,275 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,276 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,276 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,275 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,275 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,277 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:39873. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,275 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,275 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,275 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,275 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,278 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:40075. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,275 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,276 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,278 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:45903. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,276 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,278 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:37769. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,275 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,278 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:42887. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,275 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,276 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,278 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:43563. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,278 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:39075. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,276 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,278 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:36537. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,278 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:43819. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,279 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:33073. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,276 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,279 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:46509. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,279 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:35663. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,276 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,279 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:40645. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,279 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:46781. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,277 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,279 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:33907. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,279 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:38799. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,277 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,279 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:44513. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,279 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:44351. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,280 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:34207. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,280 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:37051. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,280 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:45095. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,280 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:39405. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,280 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:37667. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,280 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:36271. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,281 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:36175. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,279 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,281 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:43543. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,281 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:39645. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,281 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:45657. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,276 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,279 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,281 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:42051. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,282 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:37327. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,282 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:42865. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,282 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:46059. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,284 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:37379'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,287 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.50:39381, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,288 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.50:41719, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,288 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.37:39803, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,288 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1547, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,288 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:40253'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.65:40403, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,288 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.50:40811, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,288 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.67:45963, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,289 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,289 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2698, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,289 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,290 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1419, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,290 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,290 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,290 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,290 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,290 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,290 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,291 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,291 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,296 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:38211'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,296 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:32867'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,296 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:32839'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,297 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:41891'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,297 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:44279'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3256, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:43231'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3278, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:45219'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,297 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:41531'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.50:40811, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2092, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:41181'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3817, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3567, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1973, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2862, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:44383'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3635, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1726, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1965, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 551, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,298 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:34971'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,299 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2369, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2364, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,299 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:43729'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,299 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1421, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,299 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 535, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,299 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:44731'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,299 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2699, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,299 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.44:36915, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,299 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:43699'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,299 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2506, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,299 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:41009'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,300 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:38655'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,300 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3211, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,300 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2294, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,300 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2415, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,300 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:46695'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,300 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3633, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,300 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.50:44189, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,300 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:32845'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,300 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2518, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,300 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2664, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,300 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3650, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,301 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:36119'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1598, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2823, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.67:45963, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1281, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.54:36701, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,301 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:40329'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1629, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 7481, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 999, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,301 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:39899'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 873, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.58:42983, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2371, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,302 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:34705'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.63:40773, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1974, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1037, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2752, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:43509'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 941, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 4027, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.58:42479, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2087, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,303 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:33903'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2374, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1791, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2183, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1776, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3772, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2549, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1512, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2528, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 7794, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,307 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:46253'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,307 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:37533'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,308 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:35845'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,308 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:39745'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,308 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:33847'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,308 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:42355'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,308 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:32969'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,309 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:43657'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1112, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1234, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.66:45055, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.47:42143, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,310 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 7813, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,310 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1000, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,310 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1143, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,310 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2476, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,310 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 6775, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,310 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1685, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,310 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.41:46137, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2700, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3104, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 7309, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2676, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3094, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.63:33025, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,312 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1270, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,312 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.38:41159, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,314 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:42827'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,318 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,321 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:38091. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,322 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3292, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,322 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3771, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,323 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,332 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:11,331 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.54:46442 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:11,335 - distributed.worker - ERROR - failed during get data with tcp://10.6.101.54:38091 -> tcp://10.6.101.46:38093
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.101.54:38091 remote=tcp://10.6.101.46:46600>: Stream is closed
2025-09-05 09:20:11,337 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:43589'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,339 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2373, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,339 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,339 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,339 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,339 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,339 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,676 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,679 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:40569. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,692 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.54:46580 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:11,696 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:34139'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,697 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,698 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,698 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,698 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,698 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,705 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:11,718 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,720 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:38567. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,726 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.54:46644 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.101.54:46644 remote=tcp://10.6.101.37:8753>: Stream is closed
2025-09-05 09:20:11,731 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:39977'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,733 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.54:42409, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,733 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.49:41561, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,733 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,734 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,734 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,734 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,734 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,742 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:11,930 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,932 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:42409. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,944 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.54:46684 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:11,957 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:36679'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,958 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,958 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,958 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,959 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,959 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,967 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,336 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,708 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,746 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,888 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:39745'. Reason: nanny-close-gracefully
2025-09-05 09:20:13,894 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:39745' closed.
2025-09-05 09:20:13,931 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,931 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,935 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:13,937 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:35221. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:13,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:13,947 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.54:46412 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.101.54:46412 remote=tcp://10.6.101.37:8753>: Stream is closed
2025-09-05 09:20:13,958 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:45439'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:13,959 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.46:39177, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:13,959 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.46:44645, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:13,959 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:13,959 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:13,959 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:13,959 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:13,959 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:13,962 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:13,968 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,971 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,118 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:34139'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,119 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:34139' closed.
2025-09-05 09:20:14,144 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:39977'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,145 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:39977' closed.
2025-09-05 09:20:14,363 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:36679'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,364 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:36679' closed.
2025-09-05 09:20:14,748 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,657 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,923 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,921 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:15,924 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:37473. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,925 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:15,935 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:15,936 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:15,939 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.54:46688 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:15,954 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:44013'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,955 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:15,955 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:15,956 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:15,956 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:15,956 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:15,960 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x152a018819d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:15,970 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,972 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,329 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:16,331 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:45479. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,333 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,342 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:16,347 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.54:46714 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:16,351 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:44839'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,352 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:16,352 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:16,352 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:16,352 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:16,352 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:16,357 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:43589'. Reason: nanny-close-gracefully
2025-09-05 09:20:16,355 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1514d2afa0d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:16,358 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:43589' closed.
2025-09-05 09:20:16,361 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,428 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:40329'. Reason: nanny-close-gracefully
2025-09-05 09:20:16,430 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:40329' closed.
2025-09-05 09:20:16,455 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:45439'. Reason: nanny-close-gracefully
2025-09-05 09:20:16,458 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:45439' closed.
2025-09-05 09:20:16,753 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,855 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,159 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:38211'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,161 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:38211' closed.
2025-09-05 09:20:17,212 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,237 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,661 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,928 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,974 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,069 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:43729'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,070 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:43729' closed.
2025-09-05 09:20:18,142 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,143 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,145 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,317 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:37379'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,319 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:37379' closed.
2025-09-05 09:20:18,337 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,351 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:44013'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,352 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:44013' closed.
2025-09-05 09:20:18,364 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,415 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,437 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,622 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,621 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:18,624 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,624 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:44295. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,626 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,633 - distributed.worker - ERROR - failed during get data with tcp://10.6.101.54:44295 -> tcp://10.6.101.55:45075
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.101.54:44295 remote=tcp://10.6.101.55:36448>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-05 09:20:18,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:18,645 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.54:46406 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:18,651 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:43053'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,652 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:18,652 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:18,652 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:18,652 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:18,652 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:18,654 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14c395bf4c10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:18,660 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,695 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:18,697 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:42707. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,708 - distributed.worker - ERROR - failed during get data with tcp://10.6.101.54:42707 -> tcp://10.6.101.51:44691
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.101.54:42707 remote=tcp://10.6.101.51:51786>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-05 09:20:18,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:18,718 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.54:46400 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:18,720 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:42527'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,721 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:18,721 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:18,721 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:18,721 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:18,721 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:18,723 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15351afdeb90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:18,728 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,732 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:34971'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,734 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:34971' closed.
2025-09-05 09:20:18,744 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,744 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,752 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:44839'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,754 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:44839' closed.
2025-09-05 09:20:18,858 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,112 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:19,114 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:34585. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,115 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:19,117 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:43233. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:19,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:19,126 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.101.43:40755
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.101.54:42644 remote=tcp://10.6.101.43:40755>: Stream is closed
2025-09-05 09:20:19,130 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.54:46670 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:19,133 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:45517'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,134 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:19,135 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:19,135 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:19,135 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:19,135 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:19,136 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.54:46568 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:19,139 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:46581'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,137 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14d19e3d59d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:19,140 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:19,140 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:19,140 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:19,140 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:19,140 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:19,143 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,142 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15374652d310>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:19,146 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,216 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,241 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,258 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:46253'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,259 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:46253' closed.
2025-09-05 09:20:19,279 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,300 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:19,301 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,301 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:19,304 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:45943. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,302 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:19,302 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:19,303 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:19,304 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:33409. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,304 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:43085. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,305 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:36735. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:19,318 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:19,318 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:19,321 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.54:46462 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:19,322 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.54:46472 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:19,322 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.54:46498 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:19,323 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.54:46748 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:19,325 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:46343'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,326 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:19,326 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:19,326 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:19,326 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:41709'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,326 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:19,326 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:19,326 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:19,327 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:19,327 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:19,327 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:19,327 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:19,327 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:39247'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,328 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:19,328 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:38075'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,328 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:19,328 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:19,328 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:19,328 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:19,329 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:19,329 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:19,329 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:19,329 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:19,329 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:19,327 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x154669d1ec50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:19,329 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1466147bd010>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:19,330 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x150ddd5b1410>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:19,331 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:19,334 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,334 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,336 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,337 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,491 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,629 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:35845'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,630 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:35845' closed.
2025-09-05 09:20:19,692 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:42827'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,693 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:42827' closed.
2025-09-05 09:20:19,753 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,850 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,870 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:19,873 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:40255. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:19,893 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.54:46380 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:19,898 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:39825'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,899 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:19,899 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:19,899 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:19,899 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:19,899 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:19,901 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x150b25759ad0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:19,907 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,912 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,913 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,021 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,112 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,146 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,148 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,150 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,150 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:20,152 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.54:36701. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:20,155 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:20,167 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,173 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,172 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.54:46346 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:20,177 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.54:44261'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:20,178 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:20,178 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:20,178 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:20,178 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:20,178 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:20,180 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x153aca67da50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:20,185 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,187 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,218 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,241 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,249 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,419 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,441 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,548 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:41891'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,549 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:41891' closed.
2025-09-05 09:20:20,554 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:41531'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,555 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:41531' closed.
2025-09-05 09:20:20,590 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:34705'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,593 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:34705' closed.
2025-09-05 09:20:20,626 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,628 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,629 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,663 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,731 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,748 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,748 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,842 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:33847'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,843 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:33847' closed.
2025-09-05 09:20:20,847 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:42355'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,848 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:42355' closed.
2025-09-05 09:20:21,015 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:45219'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,016 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:45219' closed.
2025-09-05 09:20:21,041 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:33903'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,042 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:33903' closed.
2025-09-05 09:20:21,061 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:43053'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,062 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:43053' closed.
2025-09-05 09:20:21,068 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:32845'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,069 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:32845' closed.
2025-09-05 09:20:21,116 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:42527'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,117 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:42527' closed.
2025-09-05 09:20:21,146 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,231 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:43231'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,231 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:32839'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,232 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:43231' closed.
2025-09-05 09:20:21,233 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:32839' closed.
2025-09-05 09:20:21,283 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,305 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,337 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,337 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,339 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,495 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,609 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:46581'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,610 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:45517'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,611 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:46581' closed.
2025-09-05 09:20:21,611 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:45517' closed.
2025-09-05 09:20:21,660 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:40253'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,661 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:40253' closed.
2025-09-05 09:20:21,696 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:41181'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,697 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:41181' closed.
2025-09-05 09:20:21,731 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:46343'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,732 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:41709'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,733 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:46343' closed.
2025-09-05 09:20:21,734 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:41709' closed.
2025-09-05 09:20:21,758 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,806 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:39247'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,806 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:39247' closed.
2025-09-05 09:20:21,830 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:38075'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,830 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:38075' closed.
2025-09-05 09:20:21,854 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,886 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:43509'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,886 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:43509' closed.
2025-09-05 09:20:21,909 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,916 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,917 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,025 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,117 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,160 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,171 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,177 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,189 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,191 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,222 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,225 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:44383'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,226 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:44383' closed.
2025-09-05 09:20:22,245 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,253 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,274 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:32969'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,275 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:32969' closed.
2025-09-05 09:20:22,290 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:39825'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,291 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:39825' closed.
2025-09-05 09:20:22,327 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:43699'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,328 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:43699' closed.
2025-09-05 09:20:22,383 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:37533'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,384 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:37533' closed.
2025-09-05 09:20:22,471 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:38655'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,472 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:38655' closed.
2025-09-05 09:20:22,516 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:39899'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,517 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:39899' closed.
2025-09-05 09:20:22,619 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:44279'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,620 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:44279' closed.
2025-09-05 09:20:22,642 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:32867'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,643 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:32867' closed.
2025-09-05 09:20:22,655 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:36119'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,656 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:41009'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,657 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:36119' closed.
2025-09-05 09:20:22,657 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:41009' closed.
2025-09-05 09:20:22,680 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:44261'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,681 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:46695'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,682 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:44261' closed.
2025-09-05 09:20:22,683 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:46695' closed.
2025-09-05 09:20:22,704 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:43657'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,705 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:43657' closed.
2025-09-05 09:20:22,744 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.54:44731'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,745 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.54:44731' closed.
2025-09-05 09:20:22,747 - distributed.dask_worker - INFO - End worker
