2025-12-01 09:28:39,193 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:36239'
2025-12-01 09:28:39,202 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:34271'
2025-12-01 09:28:39,206 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:44193'
2025-12-01 09:28:39,209 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:43337'
2025-12-01 09:28:39,213 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:44417'
2025-12-01 09:28:39,217 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:37075'
2025-12-01 09:28:39,221 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:33133'
2025-12-01 09:28:39,225 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:35643'
2025-12-01 09:28:39,230 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:41853'
2025-12-01 09:28:39,235 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:37377'
2025-12-01 09:28:39,239 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:37683'
2025-12-01 09:28:39,243 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:43785'
2025-12-01 09:28:39,247 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:38179'
2025-12-01 09:28:39,251 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:34621'
2025-12-01 09:28:39,256 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:38163'
2025-12-01 09:28:39,260 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:41669'
2025-12-01 09:28:39,265 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:33727'
2025-12-01 09:28:39,269 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:40949'
2025-12-01 09:28:39,273 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:37247'
2025-12-01 09:28:39,278 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:44521'
2025-12-01 09:28:39,282 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:44977'
2025-12-01 09:28:39,381 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:37779'
2025-12-01 09:28:39,385 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:38169'
2025-12-01 09:28:39,389 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:36877'
2025-12-01 09:28:39,393 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:41421'
2025-12-01 09:28:39,398 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:35007'
2025-12-01 09:28:39,403 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:38621'
2025-12-01 09:28:39,406 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:37843'
2025-12-01 09:28:39,411 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:39105'
2025-12-01 09:28:39,415 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:45091'
2025-12-01 09:28:39,419 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:35055'
2025-12-01 09:28:39,425 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:38863'
2025-12-01 09:28:39,429 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:44285'
2025-12-01 09:28:39,433 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:37383'
2025-12-01 09:28:39,437 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:40859'
2025-12-01 09:28:39,443 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:45297'
2025-12-01 09:28:39,446 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:44161'
2025-12-01 09:28:39,452 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:33679'
2025-12-01 09:28:39,457 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:35627'
2025-12-01 09:28:39,461 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:43851'
2025-12-01 09:28:39,464 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:46759'
2025-12-01 09:28:39,467 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:33443'
2025-12-01 09:28:39,471 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:46109'
2025-12-01 09:28:39,473 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:35785'
2025-12-01 09:28:39,477 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:35847'
2025-12-01 09:28:39,482 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:39281'
2025-12-01 09:28:39,486 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:38311'
2025-12-01 09:28:39,490 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:44179'
2025-12-01 09:28:40,594 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:33335
2025-12-01 09:28:40,594 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:33335
2025-12-01 09:28:40,594 - distributed.worker - INFO -          dashboard at:            10.6.5.29:34591
2025-12-01 09:28:40,594 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,594 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,594 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,594 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,594 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-ydqbqwr_
2025-12-01 09:28:40,594 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,623 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,623 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,623 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,625 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:41935
2025-12-01 09:28:40,625 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:41935
2025-12-01 09:28:40,625 - distributed.worker - INFO -          dashboard at:            10.6.5.29:40089
2025-12-01 09:28:40,625 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,625 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,625 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,625 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,625 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-o77ehfvu
2025-12-01 09:28:40,625 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,629 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:40,638 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,639 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,639 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,642 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:40,737 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:42469
2025-12-01 09:28:40,737 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:42469
2025-12-01 09:28:40,737 - distributed.worker - INFO -          dashboard at:            10.6.5.29:44655
2025-12-01 09:28:40,737 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,737 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,737 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,737 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,737 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-fcqx153d
2025-12-01 09:28:40,737 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,749 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,750 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,750 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,768 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:40,771 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:37679
2025-12-01 09:28:40,771 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:37679
2025-12-01 09:28:40,771 - distributed.worker - INFO -          dashboard at:            10.6.5.29:37771
2025-12-01 09:28:40,771 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,771 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,771 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,771 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,771 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-517et0eo
2025-12-01 09:28:40,771 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,772 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:35131
2025-12-01 09:28:40,772 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:35131
2025-12-01 09:28:40,772 - distributed.worker - INFO -          dashboard at:            10.6.5.29:44015
2025-12-01 09:28:40,772 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,772 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,772 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,772 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,772 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-iswv1bi1
2025-12-01 09:28:40,772 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,776 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:46031
2025-12-01 09:28:40,776 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:46031
2025-12-01 09:28:40,776 - distributed.worker - INFO -          dashboard at:            10.6.5.29:46301
2025-12-01 09:28:40,776 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,776 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,776 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,776 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,776 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-1bd6lsbd
2025-12-01 09:28:40,776 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,782 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,782 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,782 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,783 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:40,788 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:38759
2025-12-01 09:28:40,788 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:38759
2025-12-01 09:28:40,788 - distributed.worker - INFO -          dashboard at:            10.6.5.29:42595
2025-12-01 09:28:40,788 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,788 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,788 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,788 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,788 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-k74j44g_
2025-12-01 09:28:40,788 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,791 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,792 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,792 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,792 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,793 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,793 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,793 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:40,794 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:40,797 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:34839
2025-12-01 09:28:40,797 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:34839
2025-12-01 09:28:40,797 - distributed.worker - INFO -          dashboard at:            10.6.5.29:39791
2025-12-01 09:28:40,797 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,797 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,797 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,797 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,797 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-9lcb2omg
2025-12-01 09:28:40,797 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,799 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:44073
2025-12-01 09:28:40,799 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:44073
2025-12-01 09:28:40,799 - distributed.worker - INFO -          dashboard at:            10.6.5.29:45035
2025-12-01 09:28:40,799 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,799 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,799 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,799 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,799 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-oiprg43d
2025-12-01 09:28:40,799 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,799 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:39957
2025-12-01 09:28:40,800 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:39957
2025-12-01 09:28:40,800 - distributed.worker - INFO -          dashboard at:            10.6.5.29:42259
2025-12-01 09:28:40,800 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,800 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,800 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,800 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,800 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-8_qtzr58
2025-12-01 09:28:40,800 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,800 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,801 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:39849
2025-12-01 09:28:40,801 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,801 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:39849
2025-12-01 09:28:40,801 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,801 - distributed.worker - INFO -          dashboard at:            10.6.5.29:43399
2025-12-01 09:28:40,801 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,801 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,801 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,801 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,801 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-3ps8o1dh
2025-12-01 09:28:40,801 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,802 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:40,803 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:40353
2025-12-01 09:28:40,803 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:40353
2025-12-01 09:28:40,803 - distributed.worker - INFO -          dashboard at:            10.6.5.29:44979
2025-12-01 09:28:40,803 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,803 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,803 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,803 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,803 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-ofe2txeb
2025-12-01 09:28:40,803 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,810 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,811 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,811 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,811 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,812 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,812 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,812 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:40,812 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,812 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:40,813 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,813 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,814 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:40,817 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,817 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,817 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,819 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:40,820 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,821 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,822 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,823 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:40,828 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:32849
2025-12-01 09:28:40,828 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:32849
2025-12-01 09:28:40,828 - distributed.worker - INFO -          dashboard at:            10.6.5.29:41389
2025-12-01 09:28:40,828 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,828 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,828 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,828 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,828 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-mrib3xh7
2025-12-01 09:28:40,828 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,829 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:36213
2025-12-01 09:28:40,829 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:39175
2025-12-01 09:28:40,829 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:36213
2025-12-01 09:28:40,829 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:39175
2025-12-01 09:28:40,829 - distributed.worker - INFO -          dashboard at:            10.6.5.29:43361
2025-12-01 09:28:40,829 - distributed.worker - INFO -          dashboard at:            10.6.5.29:42635
2025-12-01 09:28:40,829 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,829 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,829 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,829 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,829 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,829 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,829 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,829 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,829 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-8fnuox24
2025-12-01 09:28:40,829 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-zblsxczw
2025-12-01 09:28:40,829 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,829 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,831 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:39593
2025-12-01 09:28:40,831 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:39593
2025-12-01 09:28:40,831 - distributed.worker - INFO -          dashboard at:            10.6.5.29:46187
2025-12-01 09:28:40,831 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,831 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,831 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,831 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,831 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-m7sfxev1
2025-12-01 09:28:40,831 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,834 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:43679
2025-12-01 09:28:40,834 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:43679
2025-12-01 09:28:40,834 - distributed.worker - INFO -          dashboard at:            10.6.5.29:37275
2025-12-01 09:28:40,834 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,834 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,834 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,834 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,835 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-ptcu9fy7
2025-12-01 09:28:40,835 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,840 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,840 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,840 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,841 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,841 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:40,841 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,842 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,842 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:40,846 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,847 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,847 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,848 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:42997
2025-12-01 09:28:40,848 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,848 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:42997
2025-12-01 09:28:40,849 - distributed.worker - INFO -          dashboard at:            10.6.5.29:33011
2025-12-01 09:28:40,849 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,849 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,849 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,849 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,849 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-ic008w6b
2025-12-01 09:28:40,849 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,849 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,849 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:40,849 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,850 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:40,851 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,852 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,852 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,853 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:40,868 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,869 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,869 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,870 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:40,924 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:39417
2025-12-01 09:28:40,924 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:39417
2025-12-01 09:28:40,924 - distributed.worker - INFO -          dashboard at:            10.6.5.29:38143
2025-12-01 09:28:40,924 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,924 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,924 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,924 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,924 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-dikos709
2025-12-01 09:28:40,925 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,928 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:36759
2025-12-01 09:28:40,928 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:36759
2025-12-01 09:28:40,928 - distributed.worker - INFO -          dashboard at:            10.6.5.29:42479
2025-12-01 09:28:40,928 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,928 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,928 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,928 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,928 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-hg8nok85
2025-12-01 09:28:40,928 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,932 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:42143
2025-12-01 09:28:40,932 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:42143
2025-12-01 09:28:40,933 - distributed.worker - INFO -          dashboard at:            10.6.5.29:34429
2025-12-01 09:28:40,933 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,933 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,933 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,933 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,933 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-d4es1nhp
2025-12-01 09:28:40,933 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,936 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,937 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,937 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,938 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:40,948 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,949 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,949 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,951 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:40,953 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,954 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,954 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,955 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:35325
2025-12-01 09:28:40,955 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:35325
2025-12-01 09:28:40,955 - distributed.worker - INFO -          dashboard at:            10.6.5.29:40907
2025-12-01 09:28:40,955 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,955 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,955 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,955 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,955 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-n5zjqun4
2025-12-01 09:28:40,955 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,955 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:40,963 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:43347
2025-12-01 09:28:40,963 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:43347
2025-12-01 09:28:40,963 - distributed.worker - INFO -          dashboard at:            10.6.5.29:44077
2025-12-01 09:28:40,963 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,963 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,963 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:40,963 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:40,963 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-ibano0i6
2025-12-01 09:28:40,963 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,976 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,976 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,976 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,978 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:40,983 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:40,984 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:40,984 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:40,985 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,011 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:33161
2025-12-01 09:28:41,011 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:33161
2025-12-01 09:28:41,011 - distributed.worker - INFO -          dashboard at:            10.6.5.29:33109
2025-12-01 09:28:41,011 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,011 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,011 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,012 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,012 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-nsdillb7
2025-12-01 09:28:41,012 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,029 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:35963
2025-12-01 09:28:41,029 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:35963
2025-12-01 09:28:41,030 - distributed.worker - INFO -          dashboard at:            10.6.5.29:46345
2025-12-01 09:28:41,030 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,030 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,030 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,030 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,030 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:33179
2025-12-01 09:28:41,030 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-mzsqygau
2025-12-01 09:28:41,030 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:33179
2025-12-01 09:28:41,030 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,030 - distributed.worker - INFO -          dashboard at:            10.6.5.29:39415
2025-12-01 09:28:41,030 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,030 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,030 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,030 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,030 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-0bpjy5es
2025-12-01 09:28:41,030 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,031 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:38263
2025-12-01 09:28:41,031 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:34071
2025-12-01 09:28:41,031 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:38263
2025-12-01 09:28:41,031 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:34071
2025-12-01 09:28:41,031 - distributed.worker - INFO -          dashboard at:            10.6.5.29:42809
2025-12-01 09:28:41,031 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:42171
2025-12-01 09:28:41,031 - distributed.worker - INFO -          dashboard at:            10.6.5.29:33573
2025-12-01 09:28:41,031 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,031 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:44929
2025-12-01 09:28:41,031 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,031 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:42171
2025-12-01 09:28:41,031 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,031 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,031 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:44929
2025-12-01 09:28:41,031 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,031 - distributed.worker - INFO -          dashboard at:            10.6.5.29:46865
2025-12-01 09:28:41,031 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,031 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,031 - distributed.worker - INFO -          dashboard at:            10.6.5.29:45973
2025-12-01 09:28:41,031 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,031 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,031 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-1eqnuavk
2025-12-01 09:28:41,031 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,031 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,031 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,031 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-07i7g320
2025-12-01 09:28:41,031 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,031 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,031 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,031 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,032 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,032 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-qec0mvrg
2025-12-01 09:28:41,032 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,032 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,032 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-sq3ux1k4
2025-12-01 09:28:41,032 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,034 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:40873
2025-12-01 09:28:41,034 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:40873
2025-12-01 09:28:41,034 - distributed.worker - INFO -          dashboard at:            10.6.5.29:40245
2025-12-01 09:28:41,034 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,035 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,035 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,035 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,035 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-bod5vvcx
2025-12-01 09:28:41,035 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,037 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,037 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:46841
2025-12-01 09:28:41,037 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:46841
2025-12-01 09:28:41,037 - distributed.worker - INFO -          dashboard at:            10.6.5.29:42981
2025-12-01 09:28:41,037 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,037 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,037 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,037 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,037 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-2getzs9h
2025-12-01 09:28:41,037 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,037 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,038 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,039 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,043 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,043 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,043 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,044 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,045 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,046 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,046 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,046 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,046 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,046 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,047 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,048 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,048 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,048 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,048 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,049 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,050 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,050 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,050 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,050 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:43111
2025-12-01 09:28:41,051 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:43111
2025-12-01 09:28:41,050 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:33093
2025-12-01 09:28:41,051 - distributed.worker - INFO -          dashboard at:            10.6.5.29:41127
2025-12-01 09:28:41,051 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:33093
2025-12-01 09:28:41,051 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,051 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,051 - distributed.worker - INFO -          dashboard at:            10.6.5.29:35713
2025-12-01 09:28:41,051 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,051 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,051 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,051 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,051 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,051 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,051 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-ot2feznk
2025-12-01 09:28:41,051 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,051 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,051 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-6pz2mpjj
2025-12-01 09:28:41,051 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,052 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:42275
2025-12-01 09:28:41,052 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:42275
2025-12-01 09:28:41,052 - distributed.worker - INFO -          dashboard at:            10.6.5.29:34331
2025-12-01 09:28:41,052 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,052 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,052 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,052 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,052 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-sjw844ad
2025-12-01 09:28:41,052 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,053 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,054 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,054 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,054 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,055 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,055 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,055 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,055 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,056 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:37099
2025-12-01 09:28:41,056 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:37099
2025-12-01 09:28:41,056 - distributed.worker - INFO -          dashboard at:            10.6.5.29:37137
2025-12-01 09:28:41,056 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,056 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,056 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,056 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,056 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-sv5q7j3q
2025-12-01 09:28:41,056 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,057 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,057 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,057 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,059 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,065 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:34559
2025-12-01 09:28:41,065 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:34559
2025-12-01 09:28:41,065 - distributed.worker - INFO -          dashboard at:            10.6.5.29:34521
2025-12-01 09:28:41,065 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,065 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,065 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,065 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,065 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-bdu2ko1a
2025-12-01 09:28:41,065 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,066 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,066 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,066 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,067 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,068 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:35185
2025-12-01 09:28:41,068 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:35185
2025-12-01 09:28:41,068 - distributed.worker - INFO -          dashboard at:            10.6.5.29:41139
2025-12-01 09:28:41,068 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,069 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,068 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,069 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,069 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,069 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-b3_y8abj
2025-12-01 09:28:41,069 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,069 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,069 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,070 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,070 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:35355
2025-12-01 09:28:41,070 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:35355
2025-12-01 09:28:41,070 - distributed.worker - INFO -          dashboard at:            10.6.5.29:34453
2025-12-01 09:28:41,070 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,070 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,070 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,070 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,070 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,070 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-21evb_om
2025-12-01 09:28:41,071 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,071 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,071 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,072 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,076 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,077 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,077 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,078 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,085 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,086 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,086 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,086 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,087 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,087 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,087 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,088 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:41147
2025-12-01 09:28:41,088 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:41147
2025-12-01 09:28:41,088 - distributed.worker - INFO -          dashboard at:            10.6.5.29:36099
2025-12-01 09:28:41,088 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,088 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,088 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,088 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,088 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,088 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-wqpzd7af
2025-12-01 09:28:41,088 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,089 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:40585
2025-12-01 09:28:41,089 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:40585
2025-12-01 09:28:41,090 - distributed.worker - INFO -          dashboard at:            10.6.5.29:43625
2025-12-01 09:28:41,090 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,090 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,090 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,090 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,090 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-hnt22vxr
2025-12-01 09:28:41,090 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,090 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:39063
2025-12-01 09:28:41,090 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:39063
2025-12-01 09:28:41,090 - distributed.worker - INFO -          dashboard at:            10.6.5.29:40249
2025-12-01 09:28:41,090 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,090 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,090 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,090 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,090 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-h4yfxk6y
2025-12-01 09:28:41,090 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,090 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:41035
2025-12-01 09:28:41,091 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:41035
2025-12-01 09:28:41,091 - distributed.worker - INFO -          dashboard at:            10.6.5.29:45397
2025-12-01 09:28:41,091 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,091 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,091 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,091 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,091 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-_g1pxir6
2025-12-01 09:28:41,091 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,091 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,092 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,092 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,093 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,094 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:32799
2025-12-01 09:28:41,094 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:32799
2025-12-01 09:28:41,094 - distributed.worker - INFO -          dashboard at:            10.6.5.29:39097
2025-12-01 09:28:41,094 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,094 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,094 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,094 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,094 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-ivejwj8j
2025-12-01 09:28:41,094 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,096 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:33793
2025-12-01 09:28:41,097 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:33793
2025-12-01 09:28:41,097 - distributed.worker - INFO -          dashboard at:            10.6.5.29:43279
2025-12-01 09:28:41,097 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,097 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,097 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,097 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,097 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-vcwfrqn8
2025-12-01 09:28:41,097 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,101 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:35323
2025-12-01 09:28:41,101 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:35323
2025-12-01 09:28:41,101 - distributed.worker - INFO -          dashboard at:            10.6.5.29:44523
2025-12-01 09:28:41,101 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,101 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,101 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,102 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,102 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-l8jaf8yn
2025-12-01 09:28:41,102 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,103 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:37761
2025-12-01 09:28:41,104 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:37761
2025-12-01 09:28:41,104 - distributed.worker - INFO -          dashboard at:            10.6.5.29:46213
2025-12-01 09:28:41,104 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,104 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,104 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,104 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,104 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-leoigat5
2025-12-01 09:28:41,104 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,106 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,107 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,107 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,107 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,109 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,110 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,110 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,111 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,111 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,111 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,112 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,112 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,113 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,113 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,113 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,114 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,115 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,115 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,115 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,116 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,116 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,116 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,116 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,117 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,117 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,117 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,117 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,118 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,121 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,122 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,122 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,123 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:41,129 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:37499
2025-12-01 09:28:41,129 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:37499
2025-12-01 09:28:41,129 - distributed.worker - INFO -          dashboard at:            10.6.5.29:36069
2025-12-01 09:28:41,129 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,129 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,129 - distributed.worker - INFO -               Threads:                          1
2025-12-01 09:28:41,129 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-01 09:28:41,129 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-oqn4__gl
2025-12-01 09:28:41,129 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,140 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-01 09:28:41,141 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-01 09:28:41,141 - distributed.worker - INFO - -------------------------------------------------
2025-12-01 09:28:41,141 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-01 09:28:46,830 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,830 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,831 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,831 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,831 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,831 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,832 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,832 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,832 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,832 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,832 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,832 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,832 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,832 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,832 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,833 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,833 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,833 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,833 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,833 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,833 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,833 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,833 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,833 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,834 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,834 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,834 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,834 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,834 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,834 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,834 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,835 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,835 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,835 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,835 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,835 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,835 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,835 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,835 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,835 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,836 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,836 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,836 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,836 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,836 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,836 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,836 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,836 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,836 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,836 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,837 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,837 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,837 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,837 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,837 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,837 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,837 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,837 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,837 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,837 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,837 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,838 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,838 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,838 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,838 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,838 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,838 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,838 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,838 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,838 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,838 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,839 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,839 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,839 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,839 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,839 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,839 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,839 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,839 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,839 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,839 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,839 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,840 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,840 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,840 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,840 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,840 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,840 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,840 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-01 09:28:46,840 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,841 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,841 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,841 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,841 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,841 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:46,842 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-01 09:28:48,835 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,836 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,837 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,837 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,835 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,835 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,838 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,838 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,836 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,839 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,836 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,840 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,840 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,840 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,839 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,838 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,837 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,840 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,839 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,840 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,840 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,841 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,840 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,838 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,841 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,841 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,841 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,841 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,839 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,842 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,842 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,842 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,839 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,842 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,842 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,839 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,842 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,841 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,842 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,842 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,842 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,842 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,843 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,842 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,843 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,842 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,842 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,843 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,843 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,843 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,844 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,844 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,844 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,842 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,844 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,843 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,844 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,843 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,844 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,844 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,845 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,844 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,845 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,845 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,845 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,845 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,845 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,845 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,846 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,844 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,846 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,846 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,847 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,845 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,846 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,847 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,846 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,847 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,847 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,847 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,847 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,847 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,846 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,847 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,848 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,847 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,848 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,848 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,848 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-01 09:28:48,849 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,849 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,849 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,849 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,849 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,849 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,850 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-01 09:28:48,868 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,869 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,869 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,869 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,869 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,870 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,870 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,870 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,870 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,870 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,871 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,871 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,871 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,871 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,871 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,872 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,872 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,872 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,872 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,872 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,872 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,872 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,872 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,873 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,873 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,873 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,873 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,873 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,873 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,873 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,873 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,873 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,874 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,873 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,874 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,874 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,874 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,874 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,874 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,874 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,874 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,875 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,875 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,875 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,875 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,875 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,875 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,875 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,875 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,875 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,875 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,875 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,875 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,875 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,875 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,876 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,876 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,876 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,876 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,876 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,876 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,876 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,876 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,877 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,877 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,877 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,877 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,877 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,877 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,877 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,877 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,877 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,877 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,877 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,877 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,878 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,878 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,878 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,878 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,878 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,879 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,879 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,879 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,879 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,879 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,880 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,880 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-01 09:28:48,880 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,880 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,880 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,880 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,881 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,881 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,881 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,881 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,883 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-01 09:28:48,905 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,905 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,905 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,905 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,906 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,906 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,906 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,906 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,906 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,907 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,907 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,907 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,907 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,907 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,907 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,907 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,908 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,908 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,908 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,908 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,908 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,908 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,908 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,909 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,909 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,909 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,909 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,909 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,909 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,909 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,909 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,910 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,910 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,910 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,910 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,910 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,910 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,910 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,910 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,910 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,910 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,911 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,911 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,911 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,911 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,911 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,911 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,911 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,911 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,911 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,911 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,912 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,912 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,912 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,912 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,912 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,912 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,912 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,912 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,912 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,913 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,913 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,913 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,913 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,913 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,913 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,913 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,913 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,913 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,913 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,914 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,914 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,914 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,914 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,914 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,914 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,914 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,914 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,914 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,915 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,915 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,915 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,915 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,915 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,915 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,915 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,915 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,915 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,915 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,916 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,916 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,916 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-01 09:28:48,916 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,916 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,917 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:28:48,917 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-01 09:30:14,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,908 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,917 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,917 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,917 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:30:47,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:29,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:29,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:29,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:29,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:29,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:29,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:41,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:41,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:41,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:41,034 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,130 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:32:59,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:02,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:38,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:38,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:38,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:38,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:38,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:38,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:38,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:38,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:38,847 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:38,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:38,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:38,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:38,922 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:38,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:38,922 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:38,925 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:38,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:38,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:38,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:46,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 43.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:46,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 43.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:46,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 43.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:46,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 43.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:34:46,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 43.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:35:09,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:35:09,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:36:02,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:10,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:10,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:10,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:10,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:10,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 73.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:10,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:10,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:10,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:10,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 73.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:10,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:10,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:10,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:10,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:10,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:10,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 73.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:10,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:10,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:10,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 73.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:10,971 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:10,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:10,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 73.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:25,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 73.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:25,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 73.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:25,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 73.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:25,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 73.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:25,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 73.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:25,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 73.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:25,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 73.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:25,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 74.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:25,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 74.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:25,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 74.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:25,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 74.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:25,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 74.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:25,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 74.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:25,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 74.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:25,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 74.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:25,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 74.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:25,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 74.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:37:25,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 74.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:20,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:20,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:20,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:20,128 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:42,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:44,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:44,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:44,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:44,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:44,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:38:44,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:39:10,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:39:10,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:39:10,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 69.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 69.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 69.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:40:17,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:15,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:15,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:15,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:15,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:15,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:44,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:44,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:44,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:44,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:44,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:44,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:44,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:44,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:44,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:44,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:45,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:45,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:45,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:45,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:45,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:45,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:45,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:45,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:45,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:59,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:59,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:59,067 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:59,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:41:59,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:42:47,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:42:48,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:42:48,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:42:49,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:42:49,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:42:49,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:43:47,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:43:47,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:43:47,868 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:43:47,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:43:47,868 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:43:47,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:43:47,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:43:47,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:43:47,873 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:43:47,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:43:47,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:43:47,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:43:47,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:43:47,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:43:47,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:43:47,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:43:47,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:43:47,883 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:43:47,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:44:57,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:45:01,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:45:01,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:45:01,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:45:01,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:46:25,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 89.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:46:25,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 89.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:46:25,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 89.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:46:25,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 89.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:46:25,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 89.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:46:25,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 89.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:46:25,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 89.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:46:25,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 89.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:46:25,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 89.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:46:25,453 - distributed.core - INFO - Event loop was unresponsive in Worker for 89.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:46:25,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 89.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:46:25,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 87.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:46:25,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 87.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:46:25,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 87.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:46:25,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 87.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:46:25,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 89.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:46:25,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 87.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:46:25,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 89.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:46:25,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 87.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:31,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:31,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:31,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:31,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:31,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:31,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:31,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:31,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:31,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:31,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:31,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:31,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:31,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:31,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:31,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:31,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:31,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:31,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:31,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:31,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:31,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:57,981 - distributed.core - INFO - Event loop was unresponsive in Worker for 52.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:57,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 52.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:48:57,984 - distributed.core - INFO - Event loop was unresponsive in Worker for 52.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:49:22,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 77.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:14,943 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:14,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:14,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:14,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:14,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:57,848 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:57,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:57,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:57,851 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:57,851 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:57,851 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:57,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:57,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:57,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:57,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:57,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:57,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:57,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:57,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:57,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:57,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:57,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:57,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:57,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:57,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:50:57,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:51:05,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 48.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:51:05,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 48.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:51:05,140 - distributed.core - INFO - Event loop was unresponsive in Worker for 48.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:51:05,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 48.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:51:05,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 48.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:51:05,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 48.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:51:05,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 48.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:51:09,721 - distributed.core - INFO - Event loop was unresponsive in Worker for 53.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:15,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:15,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:15,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:15,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:15,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:15,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:15,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:15,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:15,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:15,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:15,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:15,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:15,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:15,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:15,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:15,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:15,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:15,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:15,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:15,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:28,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:28,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:28,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:28,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:28,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:28,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:28,238 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:28,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:38,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 47.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:38,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 47.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:52:38,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 47.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,543 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,543 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,604 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,604 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:53:48,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:54:04,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:54:04,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:54:04,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:54:04,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:54:04,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:54:04,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:54:04,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:54:04,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:54:04,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:54:47,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:54:47,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:54:48,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:54:48,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:54:48,848 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:54:48,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:54:48,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:55:35,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 49.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:55:35,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 49.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:55:35,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 49.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:55:35,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 49.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:55:35,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 49.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:55:35,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 49.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:55:35,275 - distributed.core - INFO - Event loop was unresponsive in Worker for 49.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:55:35,276 - distributed.core - INFO - Event loop was unresponsive in Worker for 49.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:55:35,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 49.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:55:35,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 49.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:55:35,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 49.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:55:35,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 49.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:55:35,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:55:35,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 47.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:55:35,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:55:38,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 51.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:02,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:02,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:02,984 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:02,984 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:02,985 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:02,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:02,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:02,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:02,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:03,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:03,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:03,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:03,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:03,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:03,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:03,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:03,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:03,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:03,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:03,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:38,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:38,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:38,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:38,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:38,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:38,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:42,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 67.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:42,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 67.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:42,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 67.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:42,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 67.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:42,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 67.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:42,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 67.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:42,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 67.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:57:42,034 - distributed.core - INFO - Event loop was unresponsive in Worker for 67.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:23,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:24,667 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:24,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:24,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:24,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:24,691 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:53,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:53,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:53,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:53,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:53,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:53,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:53,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:53,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:53,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:53,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:54,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:54,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:54,250 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:54,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:54,254 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:54,255 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:54,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:54,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:58:54,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:59:04,034 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 09:59:38,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:17,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:28,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 49.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:28,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 49.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:28,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 49.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:28,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 49.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:00:28,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 49.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:01:34,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:01:36,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:01:36,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:01:36,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:01:37,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:11,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:11,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:11,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:11,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:11,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:11,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:11,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:11,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:11,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:11,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:11,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:11,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:11,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:11,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:11,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:11,441 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:11,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:11,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:12,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:12,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:49,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:49,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:49,995 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:50,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:50,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:50,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:50,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:50,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:50,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:50,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:50,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:50,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:50,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:50,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:50,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:50,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:50,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:50,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:50,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:50,014 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:50,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:50,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:50,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:50,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:02:50,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:03:11,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:03:11,929 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:03:12,067 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,355 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:04:24,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,130 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:05:50,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:06:40,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:06:40,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:06:40,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:06:40,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:06:40,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:06:40,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:06:40,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:06:40,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:06:40,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:06:40,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:06:40,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:07:14,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:07,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:07,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:07,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:07,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:07,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:07,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:07,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:07,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:07,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:07,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:07,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:07,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:07,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:07,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:07,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:07,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:07,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:07,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:07,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:07,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:08,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 51.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:08,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 51.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:08,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 51.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:08,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 51.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:08,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 51.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:08,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 51.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:08:43,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:09:45,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:09:45,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:09:45,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:09:45,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:09:45,532 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:09:45,741 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:09:45,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:09:45,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:09:45,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:09:45,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:09:45,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:09:45,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:09:45,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:09:45,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:09:45,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:09:45,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:09:45,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:09:45,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:09:45,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:09:45,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:09:45,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:09:45,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:10:05,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:10:05,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:10:05,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:10:05,802 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:10:05,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:10:05,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:10:05,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:10:34,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 87.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:10:34,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 87.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:11:43,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:11:43,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:11:44,821 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:11:44,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:11:45,369 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:13:03,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 82.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:03,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:03,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:05,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:05,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:05,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:05,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:51,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 51.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:51,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 51.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:51,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 51.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:51,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 51.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:51,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 51.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:51,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 51.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:51,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 51.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:51,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 51.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:51,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 51.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:52,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 51.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:52,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:52,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:52,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:52,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:52,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:52,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:52,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 52.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:56,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:56,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:56,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:56,287 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:56,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:56,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:14:56,308 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:15:25,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 56.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:15:25,626 - distributed.core - INFO - Event loop was unresponsive in Worker for 56.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:15:25,626 - distributed.core - INFO - Event loop was unresponsive in Worker for 56.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:15:25,626 - distributed.core - INFO - Event loop was unresponsive in Worker for 56.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:15:25,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 56.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:15:25,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 56.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:15:25,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 56.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:16:07,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:16:09,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:16:09,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:16:09,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:04,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:04,458 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:04,458 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:04,458 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:04,459 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:04,459 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:04,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:04,461 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:04,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:04,466 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:04,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:05,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:05,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:05,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:05,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:05,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:05,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:05,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:05,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:05,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:13,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:13,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:13,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:13,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:13,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:17,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:17,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:17,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:17,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:17,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:17,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:17,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:17:17,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:18:11,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:18:12,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:04,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 117.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:04,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 117.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:04,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 117.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:04,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 117.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:04,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 117.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:04,568 - distributed.core - INFO - Event loop was unresponsive in Worker for 117.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:04,568 - distributed.core - INFO - Event loop was unresponsive in Worker for 117.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:04,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 117.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:04,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 117.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:04,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 117.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:05,844 - distributed.core - INFO - Event loop was unresponsive in Worker for 118.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:05,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 118.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:05,847 - distributed.core - INFO - Event loop was unresponsive in Worker for 106.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:05,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 106.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:05,851 - distributed.core - INFO - Event loop was unresponsive in Worker for 106.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:05,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 118.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:05,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 118.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:05,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 106.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:05,860 - distributed.core - INFO - Event loop was unresponsive in Worker for 106.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:05,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 106.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:05,929 - distributed.core - INFO - Event loop was unresponsive in Worker for 106.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:05,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 106.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:05,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 106.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:19,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 119.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:19,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 119.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:20:19,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 119.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:01,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:01,081 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:01,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:41,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:41,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:41,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:41,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:41,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:41,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:41,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:41,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:41,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:41,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:43,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:43,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:43,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:43,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:43,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:43,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:43,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:43,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:54,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:54,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:54,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:22:54,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:23:10,822 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:27,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:27,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:27,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:27,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:27,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:27,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:27,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:27,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:27,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:27,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:27,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:27,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:27,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:27,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:27,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:27,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:27,081 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:27,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:27,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:27,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:27,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:37,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:37,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:37,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:24:37,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:26:03,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:26:03,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:26:03,649 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:26:03,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:26:05,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,492 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,495 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,532 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:01,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:46,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:46,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:46,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:46,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:46,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:27:46,484 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:28:30,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:28:30,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 47.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:28:30,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 47.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:28:30,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:28:30,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 47.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:28:30,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:28:30,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 47.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:28:30,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 47.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:28:30,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 47.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:28:30,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 47.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:28:32,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:28:32,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:28:32,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:28:32,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:28:32,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:28:32,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:28:32,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:28:32,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:28:32,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:28:32,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:28:32,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:28:32,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:29:06,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:29:06,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:29:06,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:29:45,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:29:45,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:29:45,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:29:45,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:29:45,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:11,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:11,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:11,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:11,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:11,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:11,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:11,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:11,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:11,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:11,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:12,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:14,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:14,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:14,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:14,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:14,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:14,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:14,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:14,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:14,459 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:14,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:14,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:31,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:31,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:31,101 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:31,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:30:35,723 - distributed.core - INFO - Event loop was unresponsive in Worker for 48.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:31:08,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:00,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:00,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:00,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:00,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:00,165 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:00,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:00,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:00,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:00,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:00,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:00,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:02,622 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:02,622 - distributed.core - INFO - Event loop was unresponsive in Worker for 61.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:02,623 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:02,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:02,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:02,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:02,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:02,626 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:02,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:02,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:02,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:18,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:18,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:18,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:18,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:18,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:18,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:18,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 57.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:27,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:27,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:27,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:27,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:32:27,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:33:15,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:30,098 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:30,100 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:30,100 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:30,100 - distributed.core - INFO - Event loop was unresponsive in Worker for 78.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:30,101 - distributed.core - INFO - Event loop was unresponsive in Worker for 78.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:30,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:30,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:30,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:30,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 78.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:30,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:30,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:30,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 78.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:30,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:30,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:30,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:30,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:30,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:30,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:30,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:30,176 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:30,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:38,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 73.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:38,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 73.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:38,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 73.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:38,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 73.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:38,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 73.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:42,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 77.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:34:42,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 77.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:35:56,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:35:56,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:03,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:03,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:03,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:03,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:03,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:03,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:21,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:21,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:21,665 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:21,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:21,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:21,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:21,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:21,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:21,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:21,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:21,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:21,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:37,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:37,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:37,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:37,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:37,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:50,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:50,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:50,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:50,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:50,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:50,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:50,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:36:50,723 - distributed.core - INFO - Event loop was unresponsive in Worker for 47.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:37:45,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:37:51,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:37:57,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:38:01,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:38:03,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:38:08,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:38:27,591 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:38:33,754 - distributed.core - INFO - Event loop was unresponsive in Worker for 70.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:38:33,761 - distributed.core - INFO - Event loop was unresponsive in Worker for 70.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:38:56,626 - distributed.core - INFO - Event loop was unresponsive in Worker for 96.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:38:57,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 102.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:34,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:34,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:34,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:34,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:34,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:34,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:34,395 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:34,395 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:48,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:48,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:48,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:48,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:48,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:48,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:48,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:48,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:48,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:48,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:48,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:48,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:48,492 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:48,492 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:48,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:48,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:48,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:41:48,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:42:03,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:42:03,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:42:03,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:42:03,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:42:03,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:42:03,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:42:03,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:42:03,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:42:03,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:29,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:29,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:29,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 54.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:29,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:29,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:29,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:29,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 54.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:29,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:29,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:29,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 54.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:29,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:29,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:29,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:29,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:29,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 54.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:29,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:29,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 54.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:29,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 54.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:29,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:29,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:37,466 - distributed.core - INFO - Event loop was unresponsive in Worker for 53.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:37,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 53.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:37,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 53.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:37,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 53.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:37,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 53.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:37,469 - distributed.core - INFO - Event loop was unresponsive in Worker for 53.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:37,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 53.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:38,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 54.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:38,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 54.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:43:38,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 54.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:13,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:33,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:33,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:33,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:33,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:33,754 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:33,761 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:33,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:45:33,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:46:07,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:46:07,719 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:46:07,796 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:46:07,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:10,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:10,868 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:10,868 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:10,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:10,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:10,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:10,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:10,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:10,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:10,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:10,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:10,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:10,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:10,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:10,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:10,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:10,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:10,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:22,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:22,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:22,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:22,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:22,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:22,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:37,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 72.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:37,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 72.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:37,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 72.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:37,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 72.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:37,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 72.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:37,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 72.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:47:37,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 72.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:48:42,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:48:42,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:48:42,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:48:42,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:48:42,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:48:42,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:17,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:17,858 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:17,859 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:17,860 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:17,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:17,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:17,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:17,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:17,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:17,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:23,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:23,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:23,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:23,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:23,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:23,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:23,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:23,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:23,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:23,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:26,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:26,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:49:26,270 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:27,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:27,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:27,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:27,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:52,034 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:52,034 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:52,035 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:52,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:52,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:52,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:52,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:52,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:52,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:52,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:58,395 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:58,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:58,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:58,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:58,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:58,400 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:58,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:58,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:58,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:58,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:58,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:58,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:58,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:58,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:58,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:58,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:58,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:58,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:50:58,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:51:56,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:51:56,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:51:56,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:51:56,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:51:56,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:51:56,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:52:25,660 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:52:25,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:52:25,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:52:25,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:52:25,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:52:25,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:52:25,665 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:52:25,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:52:25,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:52:25,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:52:25,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:52:25,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:52:25,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:52:32,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:52:49,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:52:49,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:27,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:27,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:27,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:27,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:27,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:27,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:27,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 42.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:28,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:28,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:28,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:28,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:28,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:28,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:28,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:28,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:28,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 42.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:28,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:28,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:28,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:28,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 42.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:28,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:28,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 42.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:30,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:30,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:30,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:30,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:30,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:30,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:30,851 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:31,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:31,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:31,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:31,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:31,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:53:32,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:54:14,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:54:14,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:54:14,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:54:14,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:54:14,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:55:40,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 86.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:55:40,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 86.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:55:40,369 - distributed.core - INFO - Event loop was unresponsive in Worker for 86.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:55:40,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 91.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:55:40,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 86.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:55:40,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 86.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:55:40,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 91.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:55:40,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 91.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:55:40,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 86.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:55:40,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 91.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:55:40,380 - distributed.core - INFO - Event loop was unresponsive in Worker for 91.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:55:40,380 - distributed.core - INFO - Event loop was unresponsive in Worker for 91.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:55:40,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 91.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:55:40,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 91.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:55:40,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 91.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:55:40,396 - distributed.core - INFO - Event loop was unresponsive in Worker for 91.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:57:40,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:57:40,784 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:57:47,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:57:50,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:57:57,604 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:23,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 52.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:27,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:27,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:27,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:27,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:27,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:27,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:27,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:50,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:50,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:50,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:50,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:50,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:50,979 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:50,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:50,980 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:50,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:50,984 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:50,985 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:51,551 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:51,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:51,556 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:51,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:51,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:51,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:58:51,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:12,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:12,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:12,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:12,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:12,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:12,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:12,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:21,298 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:21,300 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:21,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:21,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:21,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:21,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:21,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:21,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:21,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:21,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:21,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:21,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:21,318 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:35,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:35,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:35,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:35,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:35,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:35,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:35,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:35,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:35,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:35,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:35,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:35,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:35,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:35,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:35,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:35,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:35,268 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:35,268 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:35,270 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 10:59:35,275 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:05,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:09,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:09,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:11,128 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:11,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:11,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:20,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:20,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:20,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:22,034 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:22,035 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:22,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:22,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:22,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:22,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:36,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:36,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:36,652 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:36,652 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:36,652 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:36,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:36,652 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:36,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:36,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:36,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:36,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:36,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:36,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:00:36,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:01:19,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:01:19,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:01:19,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:01:19,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:01:19,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:01:19,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:01:19,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:11,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 52.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:11,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 52.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:11,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 52.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:11,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 52.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:11,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 52.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:11,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 52.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:11,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 52.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:11,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 52.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:11,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 52.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:11,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 52.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:11,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 52.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:12,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:12,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:12,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:12,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:12,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:12,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:12,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:12,176 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:12,176 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:12,176 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:12,176 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:12,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:12,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:12,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:12,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:12,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:12,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:12,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:12,187 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:02:12,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:03:06,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:03:26,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:03:26,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:03:26,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:03:54,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:03:54,929 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:03:54,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:03:54,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:03:54,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:03:54,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:03:54,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:03:54,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:03:54,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:03:54,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:03:54,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:03:54,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:03:54,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:03:54,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:03:54,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:04:10,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:04:10,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:04:10,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:04:10,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:04:10,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:04:10,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:04:10,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:04:10,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:04:10,301 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:04:10,301 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:04:10,300 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:04:10,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:04:10,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:04:10,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-1eqnuavk/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-o77ehfvu/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-m7sfxev1/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-ic008w6b/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-l8jaf8yn/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-b3_y8abj/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-0bpjy5es/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-vcwfrqn8/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-iswv1bi1/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-fcqx153d/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-bdu2ko1a/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-_g1pxir6/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-hnt22vxr/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-bod5vvcx/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-21evb_om/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-leoigat5/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-07i7g320/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-6pz2mpjj/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-1bd6lsbd/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-d4es1nhp/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-zblsxczw/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-ot2feznk/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-oqn4__gl/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-ptcu9fy7/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-517et0eo/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-mzsqygau/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-2getzs9h/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-9lcb2omg/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-8_qtzr58/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-ibano0i6/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-ivejwj8j/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-nsdillb7/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-mrib3xh7/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-8fnuox24/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-hg8nok85/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-k74j44g_/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-qec0mvrg/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-sjw844ad/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-wqpzd7af/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-ofe2txeb/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-h4yfxk6y/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-dikos709/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-ydqbqwr_/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-oiprg43d/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-sq3ux1k4/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-n5zjqun4/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-3ps8o1dh/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/155618795.gadi-pbs/dask-scratch-space/worker-sv5q7j3q/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
2025-12-01 11:26:48,883 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:26:49,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:27:07,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:27:07,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:27:07,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:27:07,723 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 11:27:07,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 21:08:26,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 21:08:26,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 21:08:26,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 21:08:31,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 21:08:41,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-01 21:08:41,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 04:30:47,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:13:33,290 - distributed.nanny - INFO - Nanny asking worker to close. Reason: check-worker-ttl-1764610715.3352852
2025-12-02 05:17:16,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 2561.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:17:16,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 2641.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:17:16,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 2481.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:17:16,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 2721.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:17:16,770 - distributed.core - INFO - Event loop was unresponsive in Worker for 2601.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:17:16,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 2521.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:17:16,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 2761.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:17:16,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 2681.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:17:19,179 - distributed.worker - ERROR - Compute Failed
Key:       ('getitem-store-map-881155522756d4ef93b739e1f625ca93', 3, 20, 3)
State:     executing
Task:  <Task ('getitem-store-map-881155522756d4ef93b739e1f625ca93', 3, 20, 3) _execute_subgraph(...)>
Exception: 'BlockingIOError(11, "Unable to synchronously open file (unable to lock file, errno = 11, error message = \'Resource temporarily unavailable\')")'
Traceback: '  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/dask/array/core.py", line 4617, in load_store_chunk\n    out[index] = x\n    ~~~^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/netCDF4_.py", line 81, in __setitem__\n    data = self.get_array(needs_lock=False)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/h5netcdf_.py", line 51, in get_array\n    ds = self.datastore._acquire(needs_lock)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/h5netcdf_.py", line 197, in _acquire\n    with self._manager.acquire_context(needs_lock) as root:\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/contextlib.py", line 137, in __enter__\n    return next(self.gen)\n           ^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/file_manager.py", line 199, in acquire_context\n    file, cached = self._acquire_with_cache_info(needs_lock)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/file_manager.py", line 217, in _acquire_with_cache_info\n    file = self._opener(*self._args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/h5netcdf/core.py", line 1535, in __init__\n    self._h5file = self._h5py.File(\n                   ^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/h5py/_hl/files.py", line 564, in __init__\n    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/h5py/_hl/files.py", line 250, in make_fid\n    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper\n  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper\n  File "h5py/h5f.pyx", line 102, in h5py.h5f.open\n'

2025-12-02 05:17:19,189 - distributed.worker - ERROR - Compute Failed
Key:       ('getitem-store-map-881155522756d4ef93b739e1f625ca93', 4, 10, 0)
State:     executing
Task:  <Task ('getitem-store-map-881155522756d4ef93b739e1f625ca93', 4, 10, 0) _execute_subgraph(...)>
Exception: 'BlockingIOError(11, "Unable to synchronously open file (unable to lock file, errno = 11, error message = \'Resource temporarily unavailable\')")'
Traceback: '  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/dask/array/core.py", line 4617, in load_store_chunk\n    out[index] = x\n    ~~~^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/netCDF4_.py", line 81, in __setitem__\n    data = self.get_array(needs_lock=False)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/h5netcdf_.py", line 51, in get_array\n    ds = self.datastore._acquire(needs_lock)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/h5netcdf_.py", line 197, in _acquire\n    with self._manager.acquire_context(needs_lock) as root:\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/contextlib.py", line 137, in __enter__\n    return next(self.gen)\n           ^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/file_manager.py", line 199, in acquire_context\n    file, cached = self._acquire_with_cache_info(needs_lock)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/file_manager.py", line 217, in _acquire_with_cache_info\n    file = self._opener(*self._args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/h5netcdf/core.py", line 1535, in __init__\n    self._h5file = self._h5py.File(\n                   ^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/h5py/_hl/files.py", line 564, in __init__\n    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/h5py/_hl/files.py", line 250, in make_fid\n    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper\n  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper\n  File "h5py/h5f.pyx", line 102, in h5py.h5f.open\n'

2025-12-02 05:17:19,193 - distributed.worker - ERROR - Compute Failed
Key:       ('getitem-store-map-881155522756d4ef93b739e1f625ca93', 3, 5, 7)
State:     executing
Task:  <Task ('getitem-store-map-881155522756d4ef93b739e1f625ca93', 3, 5, 7) _execute_subgraph(...)>
Exception: 'BlockingIOError(11, "Unable to synchronously open file (unable to lock file, errno = 11, error message = \'Resource temporarily unavailable\')")'
Traceback: '  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/dask/array/core.py", line 4617, in load_store_chunk\n    out[index] = x\n    ~~~^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/netCDF4_.py", line 81, in __setitem__\n    data = self.get_array(needs_lock=False)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/h5netcdf_.py", line 51, in get_array\n    ds = self.datastore._acquire(needs_lock)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/h5netcdf_.py", line 197, in _acquire\n    with self._manager.acquire_context(needs_lock) as root:\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/contextlib.py", line 137, in __enter__\n    return next(self.gen)\n           ^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/file_manager.py", line 199, in acquire_context\n    file, cached = self._acquire_with_cache_info(needs_lock)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/file_manager.py", line 217, in _acquire_with_cache_info\n    file = self._opener(*self._args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/h5netcdf/core.py", line 1535, in __init__\n    self._h5file = self._h5py.File(\n                   ^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/h5py/_hl/files.py", line 564, in __init__\n    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/h5py/_hl/files.py", line 250, in make_fid\n    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper\n  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper\n  File "h5py/h5f.pyx", line 102, in h5py.h5f.open\n'

2025-12-02 05:17:19,196 - distributed.worker - ERROR - Compute Failed
Key:       ('getitem-store-map-881155522756d4ef93b739e1f625ca93', 3, 5, 12)
State:     executing
Task:  <Task ('getitem-store-map-881155522756d4ef93b739e1f625ca93', 3, 5, 12) _execute_subgraph(...)>
Exception: 'BlockingIOError(11, "Unable to synchronously open file (unable to lock file, errno = 11, error message = \'Resource temporarily unavailable\')")'
Traceback: '  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/dask/array/core.py", line 4617, in load_store_chunk\n    out[index] = x\n    ~~~^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/netCDF4_.py", line 81, in __setitem__\n    data = self.get_array(needs_lock=False)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/h5netcdf_.py", line 51, in get_array\n    ds = self.datastore._acquire(needs_lock)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/h5netcdf_.py", line 197, in _acquire\n    with self._manager.acquire_context(needs_lock) as root:\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/contextlib.py", line 137, in __enter__\n    return next(self.gen)\n           ^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/file_manager.py", line 199, in acquire_context\n    file, cached = self._acquire_with_cache_info(needs_lock)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/file_manager.py", line 217, in _acquire_with_cache_info\n    file = self._opener(*self._args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/h5netcdf/core.py", line 1535, in __init__\n    self._h5file = self._h5py.File(\n                   ^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/h5py/_hl/files.py", line 564, in __init__\n    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/h5py/_hl/files.py", line 250, in make_fid\n    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper\n  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper\n  File "h5py/h5f.pyx", line 102, in h5py.h5f.open\n'

2025-12-02 05:17:19,199 - distributed.worker - ERROR - Compute Failed
Key:       ('getitem-store-map-881155522756d4ef93b739e1f625ca93', 4, 12, 23)
State:     executing
Task:  <Task ('getitem-store-map-881155522756d4ef93b739e1f625ca93', 4, 12, 23) _execute_subgraph(...)>
Exception: 'BlockingIOError(11, "Unable to synchronously open file (unable to lock file, errno = 11, error message = \'Resource temporarily unavailable\')")'
Traceback: '  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/dask/array/core.py", line 4617, in load_store_chunk\n    out[index] = x\n    ~~~^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/netCDF4_.py", line 81, in __setitem__\n    data = self.get_array(needs_lock=False)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/h5netcdf_.py", line 51, in get_array\n    ds = self.datastore._acquire(needs_lock)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/h5netcdf_.py", line 197, in _acquire\n    with self._manager.acquire_context(needs_lock) as root:\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/contextlib.py", line 137, in __enter__\n    return next(self.gen)\n           ^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/file_manager.py", line 199, in acquire_context\n    file, cached = self._acquire_with_cache_info(needs_lock)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/xarray/backends/file_manager.py", line 217, in _acquire_with_cache_info\n    file = self._opener(*self._args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/h5netcdf/core.py", line 1535, in __init__\n    self._h5file = self._h5py.File(\n                   ^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/h5py/_hl/files.py", line 564, in __init__\n    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/h5py/_hl/files.py", line 250, in make_fid\n    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper\n  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper\n  File "h5py/h5f.pyx", line 102, in h5py.h5f.open\n'

2025-12-02 05:19:05,943 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:55930 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:19:05,945 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:35836 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:19:05,949 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:51130 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:19:05,952 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:43736 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:19:05,960 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:50652 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:19:05,970 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:39528 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:19:05,975 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:34372 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:19:05,978 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:39964 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:19:05,977 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:56456 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:19:05,973 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:51696 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:19:05,982 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:49090 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:19:05,982 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:41160 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:19:05,982 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:38540 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:19:05,985 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:50696 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:19:05,990 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:38078 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:19:05,989 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:36788 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:19:05,989 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:50850 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:19:05,991 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:56464 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:23:59,539 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:33179
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:44160 remote=tcp://10.6.5.29:33179>: Stream is closed
2025-12-02 05:19:05,993 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:50712 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:23:59,539 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:33179
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:34738 remote=tcp://10.6.5.29:33179>: Stream is closed
2025-12-02 05:19:05,996 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:49070 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:23:59,539 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:60088 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:19:05,999 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:47040 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:23:59,539 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:59822 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:19:05,991 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:45148 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:23:59,539 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:43760 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:19:06,003 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:39542 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:23:59,539 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:38856 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:19:05,997 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:38592 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:23:59,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,540 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:55732 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:23:59,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,540 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:36280 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,540 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:43740 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:23:59,540 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:60780 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:23:59,540 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:44132 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:23:59,540 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:33080 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:23:59,541 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:41312 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:23:59,542 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,542 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,541 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:33179
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:52320 remote=tcp://10.6.5.29:33179>: Stream is closed
2025-12-02 05:23:59,541 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:36014 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:23:59,543 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,543 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,542 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:43534 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,542 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:35858 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:23:59,541 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:44138 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:23:59,542 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:47906 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,542 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:43996 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,542 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:55960 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,542 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:55984 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,542 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:52420 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,542 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:46286 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,543 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:36270 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,541 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:32958 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:23:59,541 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:36804 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:23:59,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,541 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:55942 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:23:59,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,542 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:55916 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,543 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:33179
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:38334 remote=tcp://10.6.5.29:33179>: Stream is closed
2025-12-02 05:23:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,544 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:33179
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:33830 remote=tcp://10.6.5.29:33179>: Stream is closed
2025-12-02 05:23:59,544 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:57118 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,544 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:33179
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:52400 remote=tcp://10.6.5.29:33179>: Stream is closed
2025-12-02 05:23:59,542 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:52530 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:23:59,544 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:46316 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,545 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,544 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:33179
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:52416 remote=tcp://10.6.5.29:33179>: Stream is closed
2025-12-02 05:23:59,544 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:33179
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:52366 remote=tcp://10.6.5.29:33179>: Stream is closed
2025-12-02 05:23:59,546 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,544 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:48266 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,546 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,546 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,543 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:51988 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:23:59,546 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,545 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:33179
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:47024 remote=tcp://10.6.5.29:33179>: Stream is closed
2025-12-02 05:23:59,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,545 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:33179
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:55298 remote=tcp://10.6.5.29:33179>: Stream is closed
2025-12-02 05:23:59,546 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,547 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,547 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,546 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:33179
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:52388 remote=tcp://10.6.5.29:33179>: Stream is closed
2025-12-02 05:23:59,547 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,547 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,547 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,546 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:33179
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:34108 remote=tcp://10.6.5.29:33179>: Stream is closed
2025-12-02 05:23:59,547 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,546 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:57112 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:19:06,002 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:50328 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:19:06,006 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:44238 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:23:59,546 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:33179
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:60288 remote=tcp://10.6.5.29:33179>: Stream is closed
2025-12-02 05:19:06,004 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:52796 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:19:05,997 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:46336 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,548 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,547 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:42710 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:19:06,008 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:40428 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:23:59,548 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:33179
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:52344 remote=tcp://10.6.5.29:33179>: Stream is closed
2025-12-02 05:23:59,548 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:33094 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:23:59,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,549 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:49418 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,545 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:41320 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:23:59,550 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:33098 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:23:59,551 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,550 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:55988 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,551 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,551 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:33179
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:47032 remote=tcp://10.6.5.29:33179>: Stream is closed
2025-12-02 05:23:59,551 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:35484 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,552 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:48250 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,551 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:46284 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,553 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:33179
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:35346 remote=tcp://10.6.5.29:33179>: Stream is closed
2025-12-02 05:23:59,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,554 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:33179
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:34948 remote=tcp://10.6.5.29:33179>: Stream is closed
2025-12-02 05:23:59,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:19:06,003 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:51986 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:19:06,001 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:41246 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:19:06,022 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:35131
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:45722 remote=tcp://10.6.5.29:35131>: Stream is closed
2025-12-02 05:19:06,109 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:59914 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:23:59,558 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,558 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,558 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,558 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,559 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,559 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,559 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,559 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,559 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,559 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,559 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,558 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:55946 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,559 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,559 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,559 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,559 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,560 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,560 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,560 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,558 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:46322 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,558 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:42171
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:45160 remote=tcp://10.6.5.29:42171>: Stream is closed
2025-12-02 05:23:59,560 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,558 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:45402 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,560 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,560 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,560 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,560 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,559 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:33179
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:34774 remote=tcp://10.6.5.29:33179>: Stream is closed
2025-12-02 05:23:59,560 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,560 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,561 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,561 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,561 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,561 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,561 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,560 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:32799
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:56010 remote=tcp://10.6.5.29:32799>: Stream is closed
2025-12-02 05:23:59,561 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,561 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,562 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,562 - distributed.core - INFO - Event loop was unresponsive in Nanny for 626.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,561 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:33179
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:35354 remote=tcp://10.6.5.29:33179>: Stream is closed
2025-12-02 05:23:59,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,574 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,600 - distributed.nanny - INFO - Nanny asking worker to close. Reason: check-worker-ttl-1764613415.336112
2025-12-02 05:23:59,600 - distributed.nanny - INFO - Nanny asking worker to close. Reason: check-worker-ttl-1764613415.336112
2025-12-02 05:23:59,601 - distributed.nanny - INFO - Nanny asking worker to close. Reason: check-worker-ttl-1764613415.336112
2025-12-02 05:23:59,601 - distributed.nanny - INFO - Nanny asking worker to close. Reason: check-worker-ttl-1764613415.336112
2025-12-02 05:23:59,602 - distributed.nanny - INFO - Nanny asking worker to close. Reason: check-worker-ttl-1764613415.336112
2025-12-02 05:23:59,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 400.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 400.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 400.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 293.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 400.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,717 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:39957. Reason: check-worker-ttl-1764610715.3352852
2025-12-02 05:23:59,721 - distributed.core - INFO - Event loop was unresponsive in Worker for 400.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,725 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.
2025-12-02 05:23:59,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 3192.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,727 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-881155522756d4ef93b739e1f625ca93', 4, 7, 9))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-02 05:23:59,727 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-02 05:23:59,727 - distributed.worker - INFO - Removing Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-02 05:23:59,728 - distributed.worker - INFO - Removing Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-02 05:23:59,728 - distributed.worker - INFO - Removing Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-02 05:23:59,728 - distributed.worker - INFO - Removing Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-02 05:23:59,728 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:33179
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2876, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.5.29:33179 after 30 s
2025-12-02 05:23:59,734 - distributed.semaphore - ERROR - Release failed for id=e0c19e1f87ff40c9abe09d8e3a686d2d, lease_id=55934ab807bf46a0b44b273856099150, name=/g/data/xv83/users/at2708/bias_adjustment_acs_qme/qme_dev_hourly/test_hourly_correction/BOM/ACCESS-CM2/ssp370/r4i1p1f1/BARPA-R/v1-r1-ACS-QME-BARRAR2-1980-2022/1hr/wbgtAdjust/v20250311/wbgtAdjust_AUST-11i_ACCESS-CM2_ssp370_r4i1p1f1_BOM_BARPA-R_v1-r1-ACS-QME-BARRAR2-1980-2022_1hr_20310101-20311231.nc. Cluster network might be unstable?
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/semaphore.py", line 486, in _release
    await retry_operation(
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.semaphore_release local=tcp://10.6.5.29:51306 remote=tcp://10.6.5.29:8789>: Stream is closed
2025-12-02 05:23:59,761 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:35185
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:33094>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,769 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:38263. Reason: scheduler-remove-worker
2025-12-02 05:23:59,769 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:38759. Reason: scheduler-remove-worker
2025-12-02 05:23:59,769 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:37679
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:57112>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,772 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:41147. Reason: scheduler-remove-worker
2025-12-02 05:23:59,773 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.
2025-12-02 05:23:59,773 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.
2025-12-02 05:23:59,768 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:33094'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:33094>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 403.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 403.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,774 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:46031
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:33080>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,772 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:57112'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:57112>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,774 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:33080'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:33080>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,775 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:37683'. Reason: scheduler-remove-worker
2025-12-02 05:23:59,775 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.
2025-12-02 05:23:59,775 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:44073
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:43996>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 403.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,776 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-881155522756d4ef93b739e1f625ca93', 4, 12, 22))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-02 05:23:59,776 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:40873
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:44132>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,776 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-02 05:23:59,776 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:43996'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:43996>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,776 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:44179'. Reason: scheduler-remove-worker
2025-12-02 05:23:59,776 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:44132'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:44132>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,777 - distributed.worker - INFO - Removing Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-02 05:23:59,777 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-881155522756d4ef93b739e1f625ca93', 4, 12, 4))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-02 05:23:59,777 - distributed.worker - INFO - Removing Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-02 05:23:59,777 - distributed.worker - INFO - Removing Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-02 05:23:59,778 - distributed.worker - INFO - Removing Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-02 05:23:59,778 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-02 05:23:59,777 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:39849
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:43534>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,778 - distributed.worker - INFO - Removing Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-02 05:23:59,778 - distributed.worker - INFO - Removing Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-02 05:23:59,778 - distributed.worker - INFO - Removing Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-02 05:23:59,778 - distributed.worker - INFO - Removing Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-02 05:23:59,778 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:43534'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:43534>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,779 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:38311'. Reason: scheduler-remove-worker
2025-12-02 05:23:59,779 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-881155522756d4ef93b739e1f625ca93', 4, 7, 11))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-02 05:23:59,779 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:35355
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:59822>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,780 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-02 05:23:59,780 - distributed.worker - INFO - Removing Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-02 05:23:59,780 - distributed.worker - INFO - Removing Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-02 05:23:59,780 - distributed.worker - INFO - Removing Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-02 05:23:59,779 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:59822'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:59822>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,780 - distributed.worker - INFO - Removing Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-02 05:23:59,780 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:42275
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:60088>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,781 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:60088'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:60088>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,782 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:42997
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:56010>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,783 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:56010'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:56010>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,777 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:36759
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:35836>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,784 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:35963
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:55984>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,781 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:35185
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:52796>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,784 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:55984'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:55984>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,785 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:37761
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:36270>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,784 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:35836'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:35836>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,786 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:36270'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:36270>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,787 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:34839
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:46322>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,785 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:52796'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:52796>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,787 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:46322'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:46322>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,788 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:39417. Reason: check-worker-ttl-1764613415.336112
2025-12-02 05:23:59,785 - distributed.semaphore - ERROR - Release failed for id=5599e6e4a2b946a49cab49bc64401ca8, lease_id=842cbb7d60244987baf1d3c97249672f, name=/g/data/xv83/users/at2708/bias_adjustment_acs_qme/qme_dev_hourly/test_hourly_correction/BOM/ACCESS-CM2/ssp370/r4i1p1f1/BARPA-R/v1-r1-ACS-QME-BARRAR2-1980-2022/1hr/wbgtAdjust/v20250311/wbgtAdjust_AUST-11i_ACCESS-CM2_ssp370_r4i1p1f1_BOM_BARPA-R_v1-r1-ACS-QME-BARRAR2-1980-2022_1hr_20310101-20311231.nc. Cluster network might be unstable?
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/semaphore.py", line 486, in _release
    await retry_operation(
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.semaphore_release local=tcp://10.6.5.29:57330 remote=tcp://10.6.5.29:8789>: Stream is closed
2025-12-02 05:23:59,787 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:42997
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:45160>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,788 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:43347
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:55930>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,789 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:33161
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:49090>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,789 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:55930'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:55930>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,789 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:49090'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:49090>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,789 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:45160'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:45160>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,790 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:43679
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:49418>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,791 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:40353
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:34372>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,791 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:49418'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:49418>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,791 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:34372'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:34372>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,792 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:33793
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:50712>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,792 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:44073
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:55732>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,793 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:50712'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:50712>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,794 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:40873
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:50696>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,793 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:55732'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:55732>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,795 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:43111. Reason: scheduler-remove-worker
2025-12-02 05:23:59,795 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:50696'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:50696>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,796 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:37679
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:56464>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,796 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:56464'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:56464>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,796 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:39849
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:38856>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,798 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:42997
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:45722>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,797 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:38856'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:38856>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,798 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:45722'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:45722>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,799 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.
2025-12-02 05:23:59,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 403.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,799 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:44073
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:56456>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,799 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:33161
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:43740>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,800 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:56456'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:56456>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,800 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:43740'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:43740>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,801 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:35325
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:49070>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,802 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.5.29:36759, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-02 05:23:59,802 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.5.29:40353, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-02 05:23:59,800 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:40353 -> tcp://10.6.5.29:39417
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:40353 remote=tcp://10.6.5.29:39450>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-02 05:23:59,802 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:49070'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:49070>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,802 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:37761
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:41312>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,803 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:46031
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:38078>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,802 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:39450'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:40353 remote=tcp://10.6.5.29:39450>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-02 05:23:59,803 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:38078'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:38078>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,803 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:41312'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:41312>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,805 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:45297'. Reason: scheduler-remove-worker
2025-12-02 05:23:59,804 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:39849
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:39528>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-881155522756d4ef93b739e1f625ca93', 4, 7, 18))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-02 05:23:59,805 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:39528'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:39528>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,806 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-02 05:23:59,806 - distributed.worker - INFO - Removing Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-02 05:23:59,805 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:42275
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:43736>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,806 - distributed.worker - INFO - Removing Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-02 05:23:59,806 - distributed.worker - INFO - Removing Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-02 05:23:59,806 - distributed.worker - INFO - Removing Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-02 05:23:59,806 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:35355
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:50652>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,800 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:36759 -> tcp://10.6.5.29:39417
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:36759 remote=tcp://10.6.5.29:56566>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-02 05:23:59,807 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:50652'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:50652>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,806 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:43736'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:43736>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,808 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:35963
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:38540>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,809 - distributed.nanny - INFO - Worker closed
2025-12-02 05:23:59,808 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:38540'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:38540>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,808 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:42143
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:59914>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,809 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:59914'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:59914>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,809 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:34071
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:51130>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,807 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:56566'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:36759 remote=tcp://10.6.5.29:56566>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-02 05:23:59,810 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:51130'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:51130>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,810 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:36213
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:51986>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,811 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:46841
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:41160>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,811 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-02 05:23:59,811 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:51986'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:51986>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,812 - distributed.worker - INFO - Removing Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-02 05:23:59,812 - distributed.worker - INFO - Removing Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-02 05:23:59,812 - distributed.worker - INFO - Removing Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-02 05:23:59,812 - distributed.worker - INFO - Removing Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-02 05:23:59,811 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:41160'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:41160>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,812 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:41035
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:50328>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,813 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:34839
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:41246>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,813 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:50328'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:50328>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,813 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:41246'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:41246>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,809 - distributed.core - INFO - Connection to tcp://10.6.5.29:8789 has been closed.
2025-12-02 05:23:59,814 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:32849
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:40428>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,814 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:33335
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:45148>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,815 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:40428'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:40428>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,815 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:45148'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:45148>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,816 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:37761
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:36788>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,816 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:43679
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:44238>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,817 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:36788'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:36788>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,817 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:44238'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:44238>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,818 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:40585
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:38592>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,792 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:44929
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:46336>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,818 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:40353
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:43760>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,818 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:38592'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:38592>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,814 - distributed.semaphore - ERROR - Release failed for id=ee8fef2e4c984f1492af461e2107aa60, lease_id=b06d4848c1b340aaad5cb6904cd92b01, name=/g/data/xv83/users/at2708/bias_adjustment_acs_qme/qme_dev_hourly/test_hourly_correction/BOM/ACCESS-CM2/ssp370/r4i1p1f1/BARPA-R/v1-r1-ACS-QME-BARRAR2-1980-2022/1hr/wbgtAdjust/v20250311/wbgtAdjust_AUST-11i_ACCESS-CM2_ssp370_r4i1p1f1_BOM_BARPA-R_v1-r1-ACS-QME-BARRAR2-1980-2022_1hr_20310101-20311231.nc. Cluster network might be unstable?
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/semaphore.py", line 486, in _release
    await retry_operation(
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.semaphore_release local=tcp://10.6.5.29:57266 remote=tcp://10.6.5.29:8789>: Stream is closed
2025-12-02 05:23:59,819 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:46336'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:46336>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,819 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:43760'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:43760>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,819 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:35323
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:51696>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,820 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:51696'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:51696>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,820 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:34559
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:47040>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,820 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:46841
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:36280>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,821 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:47040'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:47040>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,821 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:36280'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:36280>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,821 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:39593
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:39542>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,821 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:39542'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:39542>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,822 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:35325
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:35858>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,822 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:41035
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:46284>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,822 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:35858'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:35858>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,823 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:41935
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:39964>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,823 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:46284'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:46284>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,823 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:39964'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:39964>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,823 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:40585
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:41320>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,824 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:36213
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:55946>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,824 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:41320'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:41320>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,824 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:35131 -> tcp://10.6.5.29:39063
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:50850>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,825 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:55946'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:55946>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,825 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:50850'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:35131 remote=tcp://10.6.5.29:50850>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,825 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:37679
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:52530>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,826 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:52530'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:52530>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,826 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:33161
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:55960>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,826 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.
2025-12-02 05:23:59,827 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:35131. Reason: worker-heartbeat-missing
2025-12-02 05:23:59,826 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:55960'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:55960>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,827 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:32849
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:33098>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 403.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,827 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:33098'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:33098>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,828 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:34071
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:48266>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,828 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:39063
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:44138>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,828 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:48266'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:48266>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,829 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:44138'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:44138>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,829 - distributed.core - INFO - Connection to tcp://10.6.5.29:8789 has been closed.
2025-12-02 05:23:59,830 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:35185
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:55988>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,830 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:34071
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:32958>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,830 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:55988'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:55988>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,831 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:32958'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:32958>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,831 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:42143
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:45402>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,832 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:39593
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:51988>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,832 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:45402'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:45402>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,832 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:51988'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:51988>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,833 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:41935
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:55942>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,833 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:40873
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:46286>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,834 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:55942'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:55942>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,834 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:46286'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:46286>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,830 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:33179 -> tcp://10.6.5.29:33793
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:38334>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,835 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:39063
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:46316>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,835 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:35323
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:36804>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,836 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:40949'. Reason: worker-heartbeat-missing
2025-12-02 05:23:59,835 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:46316'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:46316>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,835 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:36804'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:36804>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,837 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:35963
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:60780>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,837 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:46031
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:52420>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,837 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:60780'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:60780>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,835 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:38334'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:38334>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,838 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:42171 -> tcp://10.6.5.29:33793
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:36014>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,837 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:52420'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:52420>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,839 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:33179 -> tcp://10.6.5.29:32849
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:35346>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,839 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:36014'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:42171 remote=tcp://10.6.5.29:36014>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,839 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:35346'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:35346>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,840 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:34559
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:55916>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,840 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.
2025-12-02 05:23:59,840 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:42171. Reason: worker-heartbeat-missing
2025-12-02 05:23:59,840 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:55916'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:55916>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,840 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:33179 -> tcp://10.6.5.29:37679
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:52344>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 403.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,841 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:52344'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:52344>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,841 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:40585
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:48250>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,842 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:48250'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:48250>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,843 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:35325
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:57118>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,843 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:57118'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:57118>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,844 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:33335
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:47906>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,845 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:47906'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:47906>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,846 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:39593
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:42710>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,846 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:42710'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:42710>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,847 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:39281'. Reason: worker-heartbeat-missing
2025-12-02 05:23:59,847 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:32799 -> tcp://10.6.5.29:32849
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:35484>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,848 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:35484'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:32799 remote=tcp://10.6.5.29:35484>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,849 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-881155522756d4ef93b739e1f625ca93', 4, 12, 15))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-02 05:23:59,849 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-02 05:23:59,849 - distributed.worker - INFO - Removing Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-02 05:23:59,849 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.
2025-12-02 05:23:59,849 - distributed.worker - INFO - Removing Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-02 05:23:59,849 - distributed.worker - INFO - Removing Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-02 05:23:59,849 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:32799. Reason: worker-heartbeat-missing
2025-12-02 05:23:59,849 - distributed.worker - INFO - Removing Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-02 05:23:59,850 - distributed.core - INFO - Event loop was unresponsive in Worker for 403.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,855 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:37099. Reason: check-worker-ttl-1764613415.336112
2025-12-02 05:23:59,854 - distributed.semaphore - ERROR - Release failed for id=60604fc6a6a1418daae1cfcd08f78679, lease_id=a7b14fb8e0c746f78f087897c11b9337, name=/g/data/xv83/users/at2708/bias_adjustment_acs_qme/qme_dev_hourly/test_hourly_correction/BOM/ACCESS-CM2/ssp370/r4i1p1f1/BARPA-R/v1-r1-ACS-QME-BARRAR2-1980-2022/1hr/wbgtAdjust/v20250311/wbgtAdjust_AUST-11i_ACCESS-CM2_ssp370_r4i1p1f1_BOM_BARPA-R_v1-r1-ACS-QME-BARRAR2-1980-2022_1hr_20310101-20311231.nc. Cluster network might be unstable?
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/semaphore.py", line 486, in _release
    await retry_operation(
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.semaphore_release local=tcp://10.6.5.29:51384 remote=tcp://10.6.5.29:8789>: Stream is closed
2025-12-02 05:23:59,855 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:46109'. Reason: worker-heartbeat-missing
2025-12-02 05:23:59,858 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-881155522756d4ef93b739e1f625ca93', 4, 10, 13))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-02 05:23:59,859 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-02 05:23:59,859 - distributed.worker - INFO - Removing Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-02 05:23:59,859 - distributed.worker - INFO - Removing Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-02 05:23:59,859 - distributed.worker - INFO - Removing Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-02 05:23:59,859 - distributed.worker - INFO - Removing Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-02 05:23:59,863 - distributed.semaphore - ERROR - Release failed for id=d9b4b043f3e0459189f95bb7992d2e40, lease_id=863d071c74a149c1991ba8ac8b197faa, name=/g/data/xv83/users/at2708/bias_adjustment_acs_qme/qme_dev_hourly/test_hourly_correction/BOM/ACCESS-CM2/ssp370/r4i1p1f1/BARPA-R/v1-r1-ACS-QME-BARRAR2-1980-2022/1hr/wbgtAdjust/v20250311/wbgtAdjust_AUST-11i_ACCESS-CM2_ssp370_r4i1p1f1_BOM_BARPA-R_v1-r1-ACS-QME-BARRAR2-1980-2022_1hr_20310101-20311231.nc. Cluster network might be unstable?
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/semaphore.py", line 486, in _release
    await retry_operation(
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.semaphore_release local=tcp://10.6.5.29:57372 remote=tcp://10.6.5.29:8789>: Stream is closed
2025-12-02 05:23:59,865 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-881155522756d4ef93b739e1f625ca93', 3, 5, 20))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-02 05:23:59,866 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-02 05:23:59,867 - distributed.worker - INFO - Removing Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-02 05:23:59,867 - distributed.worker - INFO - Removing Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-02 05:23:59,867 - distributed.worker - INFO - Removing Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-02 05:23:59,867 - distributed.worker - INFO - Removing Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-02 05:23:59,872 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:39417
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:46468 remote=tcp://10.6.5.29:39417>: Stream is closed
2025-12-02 05:23:59,842 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:33179 -> tcp://10.6.5.29:33161
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:52416>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,877 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:52416'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:52416>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,872 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:37679 -> tcp://10.6.5.29:37099
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:37679 remote=tcp://10.6.5.29:43388>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-02 05:23:59,879 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:33179 -> tcp://10.6.5.29:42997
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:35354>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,880 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.5.29:37679, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-02 05:23:59,879 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:35354'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:35354>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,880 - distributed.nanny - INFO - Worker closed
2025-12-02 05:23:59,881 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:33179 -> tcp://10.6.5.29:44073
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:52400>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,879 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:39417
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:35608 remote=tcp://10.6.5.29:39417>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-02 05:23:59,881 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:52400'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:52400>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,882 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:33179 -> tcp://10.6.5.29:39849
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:33830>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,883 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:33830'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:33830>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,884 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:33179 -> tcp://10.6.5.29:36759
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:34738>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,885 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:34738'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:34738>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,885 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:39175. Reason: check-worker-ttl-1764613415.336112
2025-12-02 05:23:59,886 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:33179 -> tcp://10.6.5.29:35355
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:52320>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,886 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:52320'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:52320>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,887 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:39417
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 228, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 367, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:44424 remote=tcp://10.6.5.29:39417>: Stream is closed
2025-12-02 05:23:59,887 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:33179 -> tcp://10.6.5.29:39063
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:34108>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,888 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:34108'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:34108>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,889 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:33179 -> tcp://10.6.5.29:41035
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:34948>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,889 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:34948'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:34948>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,890 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:33179 -> tcp://10.6.5.29:36213
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:34774>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,891 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:34774'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:34774>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,892 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:33179 -> tcp://10.6.5.29:40873
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:52366>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,893 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:52366'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:52366>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,894 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:33179 -> tcp://10.6.5.29:35325
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:52388>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,894 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:52388'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:52388>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,895 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:33179 -> tcp://10.6.5.29:43679
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:47032>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,896 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:47032'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:47032>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,897 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:33179 -> tcp://10.6.5.29:43347
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:44160>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,898 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:44160'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:44160>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,899 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:33179 -> tcp://10.6.5.29:34071
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:60288>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,899 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:60288'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:60288>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,900 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:33179 -> tcp://10.6.5.29:41935
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:55298>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,901 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:55298'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:55298>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,902 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:33179 -> tcp://10.6.5.29:34559
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:47024>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,902 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:47024'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33179 remote=tcp://10.6.5.29:47024>: BrokenPipeError: [Errno 32] Broken pipe
2025-12-02 05:23:59,904 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.
2025-12-02 05:23:59,905 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:33179. Reason: worker-heartbeat-missing
2025-12-02 05:23:59,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 403.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:23:59,909 - distributed.nanny - INFO - Worker closed
2025-12-02 05:23:59,910 - distributed.core - INFO - Connection to tcp://10.6.5.29:8789 has been closed.
2025-12-02 05:23:59,918 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:37499. Reason: check-worker-ttl-1764613415.336112
2025-12-02 05:23:59,919 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-02 05:23:59,919 - distributed.worker - INFO - Removing Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-02 05:23:59,919 - distributed.worker - INFO - Removing Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-02 05:23:59,920 - distributed.worker - INFO - Removing Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-02 05:23:59,920 - distributed.worker - INFO - Removing Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-02 05:23:59,921 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:39105'. Reason: worker-heartbeat-missing
2025-12-02 05:23:59,904 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:43388'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:37679 remote=tcp://10.6.5.29:43388>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-02 05:23:59,922 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-store-map-881155522756d4ef93b739e1f625ca93', 4, 11, 22))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-02 05:23:59,923 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-02 05:23:59,923 - distributed.worker - INFO - Removing Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-02 05:23:59,923 - distributed.worker - INFO - Removing Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-02 05:23:59,923 - distributed.worker - INFO - Removing Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-02 05:23:59,923 - distributed.worker - INFO - Removing Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-02 05:23:59,926 - distributed.comm.tcp - INFO - Connection from tcp://10.6.5.29:60696 closed before handshake completed
2025-12-02 05:23:59,927 - distributed.semaphore - ERROR - Release failed for id=abdf6768da6345abb8ed48c6672b524f, lease_id=e9c3b9883ff04719bafa96e025d01ebd, name=/g/data/xv83/users/at2708/bias_adjustment_acs_qme/qme_dev_hourly/test_hourly_correction/BOM/ACCESS-CM2/ssp370/r4i1p1f1/BARPA-R/v1-r1-ACS-QME-BARRAR2-1980-2022/1hr/wbgtAdjust/v20250311/wbgtAdjust_AUST-11i_ACCESS-CM2_ssp370_r4i1p1f1_BOM_BARPA-R_v1-r1-ACS-QME-BARRAR2-1980-2022_1hr_20310101-20311231.nc. Cluster network might be unstable?
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/semaphore.py", line 486, in _release
    await retry_operation(
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.semaphore_release local=tcp://10.6.5.29:51520 remote=tcp://10.6.5.29:8789>: Stream is closed
2025-12-02 05:23:59,947 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:36759 -> tcp://10.6.5.29:37499
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:36759 remote=tcp://10.6.5.29:34776>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-02 05:23:59,949 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.5.29:33179, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-02 05:23:59,950 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.5.29:36759, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-02 05:23:59,950 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.5.29:33793, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-02 05:23:59,949 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:34776'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:36759 remote=tcp://10.6.5.29:34776>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-02 05:23:59,959 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-02 05:23:59,959 - distributed.worker - INFO - Removing Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-02 05:23:59,959 - distributed.worker - INFO - Removing Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-02 05:23:59,959 - distributed.worker - INFO - Removing Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-02 05:23:59,959 - distributed.worker - INFO - Removing Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-02 05:23:59,969 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('vectorize_factors-vectorize_factors_0-transpose-deafc3affecd712e59d8d2dd662f56bb', 11, 5, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-02 05:23:59,971 - distributed.core - INFO - Connection to tcp://10.6.5.29:8789 has been closed.
2025-12-02 05:23:59,973 - distributed.core - INFO - Connection to tcp://10.6.5.29:8789 has been closed.
2025-12-02 05:23:59,975 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:33093. Reason: check-worker-ttl-1764613415.336112
2025-12-02 05:24:00,016 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:36759 -> tcp://10.6.5.29:33093
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:36759 remote=tcp://10.6.5.29:58874>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-02 05:24:00,016 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:58874'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:36759 remote=tcp://10.6.5.29:58874>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-02 05:24:00,027 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.5.29:36759, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-02 05:24:00,026 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:37099
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 228, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 367, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:53690 remote=tcp://10.6.5.29:37099>: Stream is closed
2025-12-02 05:24:00,029 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('vectorize_factors-vectorize_factors_0-transpose-deafc3affecd712e59d8d2dd662f56bb', 14, 11, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-12-02 05:24:00,029 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:37099
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 228, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 367, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:56532 remote=tcp://10.6.5.29:37099>: Stream is closed
2025-12-02 05:24:00,029 - distributed.nanny - INFO - Worker closed
2025-12-02 05:24:00,027 - distributed.worker - ERROR - failed during get data with tcp://10.6.5.29:33793 -> tcp://10.6.5.29:37499
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33793 remote=tcp://10.6.5.29:45024>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-02 05:24:00,030 - distributed.core - INFO - Lost connection to 'tcp://10.6.5.29:45024'
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 834, in _handle_comm
    result = await result
             ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 981, in wrapper
    return await func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.5.29:33793 remote=tcp://10.6.5.29:45024>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-12-02 05:24:00,033 - distributed.nanny - INFO - Worker closed
2025-12-02 05:24:00,032 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:37499
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:49892 remote=tcp://10.6.5.29:37499>: Stream is closed
2025-12-02 05:24:00,036 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:37499
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1016, in send_recv
    await comm.write(msg, serializers=serializers, on_error="raise")
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:40760 remote=tcp://10.6.5.29:37499>: Stream is closed
2025-12-02 05:24:00,041 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-02 05:24:00,042 - distributed.worker - INFO - Removing Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-02 05:24:00,042 - distributed.worker - INFO - Removing Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-02 05:24:00,042 - distributed.worker - INFO - Removing Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-02 05:24:00,043 - distributed.worker - INFO - Removing Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-02 05:24:00,053 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:37099
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 228, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 367, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:56510 remote=tcp://10.6.5.29:37099>: Stream is closed
2025-12-02 05:24:00,071 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-12-02 05:24:00,071 - distributed.worker - INFO - Removing Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-02 05:24:00,071 - distributed.worker - INFO - Removing Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-02 05:24:00,072 - distributed.worker - INFO - Removing Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-02 05:24:00,044 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:37499
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 231, in read
    buffer = await read_bytes_rw(stream, buffer_nbytes)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 367, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:35578 remote=tcp://10.6.5.29:37499>: Stream is closed
2025-12-02 05:24:00,072 - distributed.worker - INFO - Removing Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-02 05:24:00,131 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:51692 remote=tcp://10.6.5.29:8789>: Stream is closed
2025-12-02 05:24:00,149 - distributed.core - INFO - Connection to tcp://10.6.5.29:8789 has been closed.
2025-12-02 05:24:00,139 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:37499
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 231, in read
    buffer = await read_bytes_rw(stream, buffer_nbytes)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 367, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.5.29:45398 remote=tcp://10.6.5.29:37499>: Stream is closed
2025-12-02 05:24:00,180 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:51554 remote=tcp://10.6.5.29:8789>: Stream is closed
2025-12-02 05:24:00,194 - distributed.core - INFO - Connection to tcp://10.6.5.29:8789 has been closed.
2025-12-02 05:24:00,580 - distributed.nanny - INFO - Worker closed
2025-12-02 05:24:00,581 - distributed.core - INFO - Connection to tcp://10.6.5.29:8789 has been closed.
2025-12-02 05:24:00,637 - distributed.nanny - INFO - Worker closed
2025-12-02 05:24:00,639 - distributed.nanny - INFO - Worker closed
2025-12-02 05:24:00,638 - distributed.core - INFO - Connection to tcp://10.6.5.29:8789 has been closed.
2025-12-02 05:24:00,639 - distributed.core - INFO - Connection to tcp://10.6.5.29:8789 has been closed.
2025-12-02 05:24:00,696 - distributed.nanny - INFO - Worker closed
2025-12-02 05:24:00,697 - distributed.core - INFO - Connection to tcp://10.6.5.29:8789 has been closed.
2025-12-02 05:24:00,706 - distributed.nanny - INFO - Worker closed
2025-12-02 05:24:00,707 - distributed.core - INFO - Connection to tcp://10.6.5.29:8789 has been closed.
2025-12-02 05:24:00,707 - distributed.nanny - INFO - Worker closed
2025-12-02 05:24:00,708 - distributed.core - INFO - Connection to tcp://10.6.5.29:8789 has been closed.
2025-12-02 05:24:00,711 - distributed.nanny - INFO - Worker closed
2025-12-02 05:24:00,712 - distributed.core - INFO - Connection to tcp://10.6.5.29:8789 has been closed.
2025-12-02 05:24:02,590 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-12-02 05:24:02,650 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-12-02 05:24:02,657 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-12-02 05:24:02,711 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-12-02 05:24:03,605 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2025-12-02 05:24:03,605 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2025-12-02 05:24:03,765 - distributed.nanny - INFO - Worker process 1354723 was killed by signal 9
2025-12-02 05:24:03,771 - distributed.nanny - INFO - Worker process 1354800 was killed by signal 9
2025-12-02 05:24:04,393 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:46109'. Reason: nanny-close-gracefully
2025-12-02 05:24:04,395 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:46109' closed.
2025-12-02 05:24:05,184 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:40949'. Reason: nanny-close-gracefully
2025-12-02 05:24:05,186 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:40949' closed.
2025-12-02 05:24:05,306 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:39105'. Reason: nanny-close-gracefully
2025-12-02 05:24:05,308 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:39105' closed.
2025-12-02 05:24:05,316 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:37683'. Reason: nanny-close-gracefully
2025-12-02 05:24:05,318 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:37683' closed.
2025-12-02 05:24:05,447 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:39281'. Reason: nanny-close-gracefully
2025-12-02 05:24:05,448 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:39281' closed.
2025-12-02 05:24:05,573 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:45297'. Reason: nanny-close-gracefully
2025-12-02 05:24:05,574 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:45297' closed.
2025-12-02 05:24:05,791 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:44179'. Reason: nanny-close-gracefully
2025-12-02 05:24:05,792 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:44179' closed.
2025-12-02 05:24:06,507 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:38311'. Reason: nanny-close-gracefully
2025-12-02 05:24:06,508 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:38311' closed.
2025-12-02 05:24:06,784 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:39417
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14b4176d6450>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2876, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2025-12-02 05:24:06,793 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:39417
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x154211bbabd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2876, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2025-12-02 05:24:06,800 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:37499
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1541fdbaa590>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2876, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2025-12-02 05:24:06,832 - distributed.nanny - WARNING - Restarting worker
2025-12-02 05:24:06,838 - distributed.nanny - WARNING - Restarting worker
2025-12-02 05:24:06,843 - distributed.nanny - WARNING - Restarting worker
2025-12-02 05:24:06,847 - distributed.nanny - WARNING - Restarting worker
2025-12-02 05:24:06,850 - distributed.nanny - WARNING - Restarting worker
2025-12-02 05:24:06,855 - distributed.nanny - WARNING - Restarting worker
2025-12-02 05:24:06,849 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:37499
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1529c4ea6c10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2876, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2025-12-02 05:24:06,854 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:37099
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x152028ea4990>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2876, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2025-12-02 05:24:06,858 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:39417
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14a9e7866990>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2876, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2025-12-02 05:24:06,863 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:37499
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14aa08262950>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2876, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2025-12-02 05:24:06,875 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.5.29:37499
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14efa83229d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2876, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2025-12-02 05:24:08,229 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:36893
2025-12-02 05:24:08,230 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:36893
2025-12-02 05:24:08,230 - distributed.worker - INFO -          dashboard at:            10.6.5.29:43705
2025-12-02 05:24:08,230 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-02 05:24:08,230 - distributed.worker - INFO - -------------------------------------------------
2025-12-02 05:24:08,230 - distributed.worker - INFO -               Threads:                          1
2025-12-02 05:24:08,230 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-02 05:24:08,230 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-hyq3zdc2
2025-12-02 05:24:08,230 - distributed.worker - INFO - -------------------------------------------------
2025-12-02 05:24:08,264 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:36529
2025-12-02 05:24:08,265 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:36529
2025-12-02 05:24:08,265 - distributed.worker - INFO -          dashboard at:            10.6.5.29:44985
2025-12-02 05:24:08,265 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-02 05:24:08,265 - distributed.worker - INFO - -------------------------------------------------
2025-12-02 05:24:08,265 - distributed.worker - INFO -               Threads:                          1
2025-12-02 05:24:08,265 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-02 05:24:08,265 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-xzkiwj71
2025-12-02 05:24:08,265 - distributed.worker - INFO - -------------------------------------------------
2025-12-02 05:24:08,273 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-02 05:24:08,274 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-02 05:24:08,275 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-02 05:24:08,278 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:38115
2025-12-02 05:24:08,278 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:38115
2025-12-02 05:24:08,278 - distributed.worker - INFO -          dashboard at:            10.6.5.29:41657
2025-12-02 05:24:08,278 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-02 05:24:08,278 - distributed.worker - INFO - -------------------------------------------------
2025-12-02 05:24:08,278 - distributed.worker - INFO -               Threads:                          1
2025-12-02 05:24:08,278 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-02 05:24:08,278 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-_9ru9qd5
2025-12-02 05:24:08,279 - distributed.worker - INFO - -------------------------------------------------
2025-12-02 05:24:08,284 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-02 05:24:08,284 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-02 05:24:08,285 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-02 05:24:08,290 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-02 05:24:08,290 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-02 05:24:08,291 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-02 05:24:08,294 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:38441
2025-12-02 05:24:08,295 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:38441
2025-12-02 05:24:08,295 - distributed.worker - INFO -          dashboard at:            10.6.5.29:34903
2025-12-02 05:24:08,295 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-02 05:24:08,295 - distributed.worker - INFO - -------------------------------------------------
2025-12-02 05:24:08,295 - distributed.worker - INFO -               Threads:                          1
2025-12-02 05:24:08,295 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-02 05:24:08,295 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-6x5up024
2025-12-02 05:24:08,295 - distributed.worker - INFO - -------------------------------------------------
2025-12-02 05:24:08,306 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-02 05:24:08,306 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-02 05:24:08,306 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-02 05:24:08,321 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:38101
2025-12-02 05:24:08,321 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:38101
2025-12-02 05:24:08,321 - distributed.worker - INFO -          dashboard at:            10.6.5.29:45145
2025-12-02 05:24:08,321 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-02 05:24:08,321 - distributed.worker - INFO - -------------------------------------------------
2025-12-02 05:24:08,321 - distributed.worker - INFO -               Threads:                          1
2025-12-02 05:24:08,321 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-02 05:24:08,321 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-msxukq5a
2025-12-02 05:24:08,322 - distributed.worker - INFO - -------------------------------------------------
2025-12-02 05:24:08,332 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-02 05:24:08,333 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-02 05:24:08,333 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-02 05:24:08,352 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:37579
2025-12-02 05:24:08,352 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:37579
2025-12-02 05:24:08,352 - distributed.worker - INFO -          dashboard at:            10.6.5.29:38649
2025-12-02 05:24:08,352 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.29:8789
2025-12-02 05:24:08,352 - distributed.worker - INFO - -------------------------------------------------
2025-12-02 05:24:08,352 - distributed.worker - INFO -               Threads:                          1
2025-12-02 05:24:08,352 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-12-02 05:24:08,352 - distributed.worker - INFO -       Local Directory: /jobfs/155618795.gadi-pbs/dask-scratch-space/worker-3h3k0wun
2025-12-02 05:24:08,352 - distributed.worker - INFO - -------------------------------------------------
2025-12-02 05:24:08,384 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-12-02 05:24:08,384 - distributed.worker - INFO - Starting Worker plugin qme_utils.py42f975b5-eb7b-4af4-a156-218c3986a51f
2025-12-02 05:24:08,385 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-12-02 05:24:09,490 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-02 05:24:09,491 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-02 05:24:09,492 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-02 05:24:09,492 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-02 05:24:09,493 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-02 05:24:09,495 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-02 05:24:09,498 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-02 05:24:09,498 - distributed.worker - INFO - -------------------------------------------------
2025-12-02 05:24:09,498 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-02 05:24:09,514 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-02 05:24:09,515 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-02 05:24:09,516 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-02 05:24:09,517 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-02 05:24:09,517 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-02 05:24:09,519 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-02 05:24:09,522 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-02 05:24:09,522 - distributed.worker - INFO - -------------------------------------------------
2025-12-02 05:24:09,523 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-02 05:24:09,523 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-02 05:24:09,524 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-02 05:24:09,525 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-02 05:24:09,525 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-02 05:24:09,526 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-02 05:24:09,528 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-02 05:24:09,530 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-02 05:24:09,530 - distributed.worker - INFO - -------------------------------------------------
2025-12-02 05:24:09,531 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-02 05:24:09,541 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-02 05:24:09,541 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-02 05:24:09,542 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-02 05:24:09,543 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-02 05:24:09,544 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-02 05:24:09,546 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-02 05:24:09,549 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-02 05:24:09,549 - distributed.worker - INFO - -------------------------------------------------
2025-12-02 05:24:09,549 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-02 05:24:09,615 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-02 05:24:09,616 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-02 05:24:09,617 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-02 05:24:09,618 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-02 05:24:09,618 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-02 05:24:09,620 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-02 05:24:09,623 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-02 05:24:09,623 - distributed.worker - INFO - -------------------------------------------------
2025-12-02 05:24:09,624 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-02 05:24:09,662 - distributed.worker - INFO - Starting Worker plugin qme_vars.py568c96d2-1c5f-40a9-8cb3-98ec6878012c
2025-12-02 05:24:09,662 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-12-02 05:24:09,663 - distributed.worker - INFO - Starting Worker plugin qme_train.py2c9f3c1a-3236-4338-89c3-0e7c71410d5c
2025-12-02 05:24:09,664 - distributed.worker - INFO - Starting Worker plugin qme_apply.py1765e7d6-499a-45e2-8502-ace35dfce8e9
2025-12-02 05:24:09,664 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-12-02 05:24:09,666 - distributed.utils - INFO - Reload module qme_train from .py file
2025-12-02 05:24:09,669 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.29:8789
2025-12-02 05:24:09,669 - distributed.worker - INFO - -------------------------------------------------
2025-12-02 05:24:09,670 - distributed.core - INFO - Starting established connection to tcp://10.6.5.29:8789
2025-12-02 05:24:17,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:25:33,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 83.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:25:33,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 83.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:25:33,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 83.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:25:33,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 83.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:25:33,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 81.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:25:33,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 83.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:25:33,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 83.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:25:33,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 81.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:25:33,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 83.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:25:33,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 80.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:26:36,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 60.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:26:36,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:26:49,446 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:26:49,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:26:49,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:26:49,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:26:49,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:26:49,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:27:21,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 42.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:27:21,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 42.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:27:21,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 42.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:27:21,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 42.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:27:21,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 42.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-12-02 05:27:21,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 42.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
