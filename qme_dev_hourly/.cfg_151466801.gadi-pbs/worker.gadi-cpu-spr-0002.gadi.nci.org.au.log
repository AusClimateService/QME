Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-10-02 16:11:25,621 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:33135'
2025-10-02 16:11:25,630 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:34515'
2025-10-02 16:11:25,634 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:39651'
2025-10-02 16:11:25,638 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:35543'
2025-10-02 16:11:25,642 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:40603'
2025-10-02 16:11:25,646 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:39155'
2025-10-02 16:11:25,650 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:34803'
2025-10-02 16:11:25,654 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:38219'
2025-10-02 16:11:25,658 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:44019'
2025-10-02 16:11:25,663 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:34115'
2025-10-02 16:11:25,667 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:39179'
2025-10-02 16:11:25,670 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:33515'
2025-10-02 16:11:25,675 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:37005'
2025-10-02 16:11:25,679 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:38707'
2025-10-02 16:11:25,683 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:35093'
2025-10-02 16:11:25,689 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:45871'
2025-10-02 16:11:25,693 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:33613'
2025-10-02 16:11:25,697 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:36319'
2025-10-02 16:11:25,702 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:39689'
2025-10-02 16:11:25,706 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:37453'
2025-10-02 16:11:25,782 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:38889'
2025-10-02 16:11:25,786 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:38231'
2025-10-02 16:11:25,790 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:45997'
2025-10-02 16:11:25,795 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:33957'
2025-10-02 16:11:25,799 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:42129'
2025-10-02 16:11:25,804 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:34023'
2025-10-02 16:11:25,808 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:39113'
2025-10-02 16:11:25,813 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:45859'
2025-10-02 16:11:25,817 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:39411'
2025-10-02 16:11:25,822 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:33963'
2025-10-02 16:11:25,827 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:43511'
2025-10-02 16:11:25,831 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:35529'
2025-10-02 16:11:25,836 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:44121'
2025-10-02 16:11:25,840 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:40929'
2025-10-02 16:11:25,845 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:33693'
2025-10-02 16:11:25,850 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:37061'
2025-10-02 16:11:25,854 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:37993'
2025-10-02 16:11:25,859 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:33163'
2025-10-02 16:11:25,864 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:37763'
2025-10-02 16:11:25,867 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:38727'
2025-10-02 16:11:25,872 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:46211'
2025-10-02 16:11:25,877 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:34217'
2025-10-02 16:11:25,882 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:45145'
2025-10-02 16:11:25,887 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:41115'
2025-10-02 16:11:25,891 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:41567'
2025-10-02 16:11:25,895 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:39227'
2025-10-02 16:11:25,899 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:35043'
2025-10-02 16:11:25,905 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:37729'
2025-10-02 16:11:25,911 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:33373'
2025-10-02 16:11:25,915 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:45137'
2025-10-02 16:11:25,919 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:36407'
2025-10-02 16:11:25,925 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.81.2:33421'
2025-10-02 16:11:26,904 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:41141
2025-10-02 16:11:26,904 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:46331
2025-10-02 16:11:26,904 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:34517
2025-10-02 16:11:26,904 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:45267
2025-10-02 16:11:26,904 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:46155
2025-10-02 16:11:26,904 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:39125
2025-10-02 16:11:26,904 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:34495
2025-10-02 16:11:26,904 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:44765
2025-10-02 16:11:26,904 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:40853
2025-10-02 16:11:26,904 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:45199
2025-10-02 16:11:26,904 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:33921
2025-10-02 16:11:26,904 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:41141
2025-10-02 16:11:26,904 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:44647
2025-10-02 16:11:26,904 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:42971
2025-10-02 16:11:26,904 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:44639
2025-10-02 16:11:26,904 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:46331
2025-10-02 16:11:26,904 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:42599
2025-10-02 16:11:26,904 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:34517
2025-10-02 16:11:26,904 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:45267
2025-10-02 16:11:26,904 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:46155
2025-10-02 16:11:26,904 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:39125
2025-10-02 16:11:26,904 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:34495
2025-10-02 16:11:26,904 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:44765
2025-10-02 16:11:26,904 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:40853
2025-10-02 16:11:26,904 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:45199
2025-10-02 16:11:26,904 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:33921
2025-10-02 16:11:26,904 - distributed.worker - INFO -          dashboard at:            10.6.81.2:33371
2025-10-02 16:11:26,904 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:44647
2025-10-02 16:11:26,904 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:42971
2025-10-02 16:11:26,904 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:44639
2025-10-02 16:11:26,904 - distributed.worker - INFO -          dashboard at:            10.6.81.2:37615
2025-10-02 16:11:26,904 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:42599
2025-10-02 16:11:26,904 - distributed.worker - INFO -          dashboard at:            10.6.81.2:35879
2025-10-02 16:11:26,904 - distributed.worker - INFO -          dashboard at:            10.6.81.2:37571
2025-10-02 16:11:26,904 - distributed.worker - INFO -          dashboard at:            10.6.81.2:40771
2025-10-02 16:11:26,904 - distributed.worker - INFO -          dashboard at:            10.6.81.2:33423
2025-10-02 16:11:26,904 - distributed.worker - INFO -          dashboard at:            10.6.81.2:39415
2025-10-02 16:11:26,904 - distributed.worker - INFO -          dashboard at:            10.6.81.2:41775
2025-10-02 16:11:26,904 - distributed.worker - INFO -          dashboard at:            10.6.81.2:38125
2025-10-02 16:11:26,904 - distributed.worker - INFO -          dashboard at:            10.6.81.2:35539
2025-10-02 16:11:26,904 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,904 - distributed.worker - INFO -          dashboard at:            10.6.81.2:41287
2025-10-02 16:11:26,904 - distributed.worker - INFO -          dashboard at:            10.6.81.2:33753
2025-10-02 16:11:26,904 - distributed.worker - INFO -          dashboard at:            10.6.81.2:45439
2025-10-02 16:11:26,904 - distributed.worker - INFO -          dashboard at:            10.6.81.2:37505
2025-10-02 16:11:26,904 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,904 - distributed.worker - INFO -          dashboard at:            10.6.81.2:43115
2025-10-02 16:11:26,904 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,904 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,904 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,904 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,904 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,904 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,904 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,904 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,904 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,904 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,904 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,904 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,904 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,904 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,904 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,904 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,904 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,904 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,904 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,904 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,904 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,904 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,904 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:26,904 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:26,904 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,904 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,904 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,904 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,904 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:26,904 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,904 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:26,904 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,904 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:26,904 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:26,905 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:26,905 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:26,905 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:26,905 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:26,905 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:26,905 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:26,905 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:26,905 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:26,905 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:26,905 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:26,905 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:26,905 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:26,905 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:26,905 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:26,905 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:26,905 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:26,905 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:26,905 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:26,905 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:26,905 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-rhwsamo4
2025-10-02 16:11:26,905 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:26,905 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-0xybviys
2025-10-02 16:11:26,905 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:26,905 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:26,905 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:26,905 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-8o2r650n
2025-10-02 16:11:26,905 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-u1vs6n9m
2025-10-02 16:11:26,905 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:26,905 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-qhelcfo2
2025-10-02 16:11:26,905 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-5gm1vi7_
2025-10-02 16:11:26,905 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-bekqc73y
2025-10-02 16:11:26,905 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-7vyaf83h
2025-10-02 16:11:26,905 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-0h2klcqt
2025-10-02 16:11:26,905 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-j9u6aq6_
2025-10-02 16:11:26,905 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,905 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-rz1agcko
2025-10-02 16:11:26,905 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,905 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-ed70xkxm
2025-10-02 16:11:26,905 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-dz8seb99
2025-10-02 16:11:26,905 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-memi7j4g
2025-10-02 16:11:26,905 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,905 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,905 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-9nvrtok_
2025-10-02 16:11:26,905 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,905 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,905 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,905 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,905 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,905 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,905 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,905 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,905 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,905 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,905 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,905 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:40983
2025-10-02 16:11:26,905 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:40983
2025-10-02 16:11:26,905 - distributed.worker - INFO -          dashboard at:            10.6.81.2:40133
2025-10-02 16:11:26,905 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,905 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,905 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:26,905 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:26,905 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-_9knzfne
2025-10-02 16:11:26,906 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,906 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:36853
2025-10-02 16:11:26,906 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:36853
2025-10-02 16:11:26,906 - distributed.worker - INFO -          dashboard at:            10.6.81.2:35141
2025-10-02 16:11:26,906 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,906 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,906 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:26,906 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:26,906 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-ph_wjg9a
2025-10-02 16:11:26,906 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,907 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:32939
2025-10-02 16:11:26,907 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:32939
2025-10-02 16:11:26,907 - distributed.worker - INFO -          dashboard at:            10.6.81.2:40041
2025-10-02 16:11:26,907 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,907 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,907 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:26,907 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:26,907 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-j076oavc
2025-10-02 16:11:26,907 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,909 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:37693
2025-10-02 16:11:26,910 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:37693
2025-10-02 16:11:26,910 - distributed.worker - INFO -          dashboard at:            10.6.81.2:38747
2025-10-02 16:11:26,910 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,910 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,910 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:26,910 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:26,910 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-tqfua_vg
2025-10-02 16:11:26,910 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,911 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:37537
2025-10-02 16:11:26,912 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:37537
2025-10-02 16:11:26,912 - distributed.worker - INFO -          dashboard at:            10.6.81.2:35905
2025-10-02 16:11:26,912 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,912 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,912 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:26,912 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:26,912 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-ur6om1jo
2025-10-02 16:11:26,912 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,924 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:26,925 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,925 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,925 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:26,926 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:26,926 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,926 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,927 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:26,927 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,927 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,927 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:26,928 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:26,928 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:26,928 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,928 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,929 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:26,930 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:26,931 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,931 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,932 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:26,933 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:26,933 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:26,933 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,933 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,934 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,934 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:26,934 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,934 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:26,934 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:26,934 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,935 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,935 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:26,935 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,935 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,935 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:26,935 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:26,936 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,936 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,936 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:26,936 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:26,937 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,937 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:26,937 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,937 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,937 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:26,937 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,937 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:26,938 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,938 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,938 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:26,939 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:26,939 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:26,939 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:26,940 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:26,940 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,940 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,940 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:26,940 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,940 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,941 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:26,941 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,941 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,941 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:26,941 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:26,942 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,942 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,942 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:26,942 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:26,942 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,943 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,943 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:26,943 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,943 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:26,943 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,943 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:26,944 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:26,944 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:26,944 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:26,945 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:26,946 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,090 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:45245
2025-10-02 16:11:27,090 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:45245
2025-10-02 16:11:27,090 - distributed.worker - INFO -          dashboard at:            10.6.81.2:46747
2025-10-02 16:11:27,090 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,090 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,090 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,090 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,090 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-i5yc80ob
2025-10-02 16:11:27,090 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,097 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:35079
2025-10-02 16:11:27,098 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:35079
2025-10-02 16:11:27,098 - distributed.worker - INFO -          dashboard at:            10.6.81.2:46749
2025-10-02 16:11:27,098 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,098 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,098 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,098 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,098 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-4x3ol7bn
2025-10-02 16:11:27,098 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,108 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:39353
2025-10-02 16:11:27,108 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:39353
2025-10-02 16:11:27,108 - distributed.worker - INFO -          dashboard at:            10.6.81.2:36229
2025-10-02 16:11:27,108 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,108 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,108 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,108 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,108 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-0yk60ame
2025-10-02 16:11:27,108 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,112 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,113 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,113 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,114 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,117 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,118 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,118 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,120 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,126 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:41457
2025-10-02 16:11:27,126 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:41457
2025-10-02 16:11:27,126 - distributed.worker - INFO -          dashboard at:            10.6.81.2:40577
2025-10-02 16:11:27,126 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,126 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,126 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,126 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,126 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-twx_8kwu
2025-10-02 16:11:27,126 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,130 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,131 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,131 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,133 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,144 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:33979
2025-10-02 16:11:27,144 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:33979
2025-10-02 16:11:27,144 - distributed.worker - INFO -          dashboard at:            10.6.81.2:38233
2025-10-02 16:11:27,144 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,144 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,144 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,144 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,144 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-6pyyo1gu
2025-10-02 16:11:27,145 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,147 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,148 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,148 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,150 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,150 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:43803
2025-10-02 16:11:27,150 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:43803
2025-10-02 16:11:27,150 - distributed.worker - INFO -          dashboard at:            10.6.81.2:34733
2025-10-02 16:11:27,150 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,150 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,151 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,151 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,151 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-gzv7qvhv
2025-10-02 16:11:27,151 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,169 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,170 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,170 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,171 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,172 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,173 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,173 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,174 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,174 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:40487
2025-10-02 16:11:27,175 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:40487
2025-10-02 16:11:27,175 - distributed.worker - INFO -          dashboard at:            10.6.81.2:40977
2025-10-02 16:11:27,175 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,175 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,175 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,175 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,175 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-3uidynq2
2025-10-02 16:11:27,175 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,183 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,184 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,184 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,184 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:35397
2025-10-02 16:11:27,184 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:35397
2025-10-02 16:11:27,184 - distributed.worker - INFO -          dashboard at:            10.6.81.2:33467
2025-10-02 16:11:27,184 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,184 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,184 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,184 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,184 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-ss04aesx
2025-10-02 16:11:27,184 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,184 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,194 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:45869
2025-10-02 16:11:27,194 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:45869
2025-10-02 16:11:27,194 - distributed.worker - INFO -          dashboard at:            10.6.81.2:39621
2025-10-02 16:11:27,195 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,195 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,195 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,195 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,195 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-uhbb5zeu
2025-10-02 16:11:27,195 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,203 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,204 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,204 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,205 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,206 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,206 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,206 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,207 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,247 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:35061
2025-10-02 16:11:27,248 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:35061
2025-10-02 16:11:27,248 - distributed.worker - INFO -          dashboard at:            10.6.81.2:40495
2025-10-02 16:11:27,248 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,248 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,248 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,248 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,248 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-fbmijf7l
2025-10-02 16:11:27,248 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,258 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,258 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,258 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,259 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,284 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:37267
2025-10-02 16:11:27,284 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:37267
2025-10-02 16:11:27,284 - distributed.worker - INFO -          dashboard at:            10.6.81.2:34549
2025-10-02 16:11:27,284 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,284 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,284 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,284 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,284 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-zotpedqy
2025-10-02 16:11:27,284 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,325 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,326 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,327 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,328 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,355 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:38777
2025-10-02 16:11:27,355 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:38777
2025-10-02 16:11:27,355 - distributed.worker - INFO -          dashboard at:            10.6.81.2:32931
2025-10-02 16:11:27,355 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,355 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,355 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,355 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,355 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-7j6thjia
2025-10-02 16:11:27,355 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,377 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,378 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,378 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,380 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,400 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:42567
2025-10-02 16:11:27,400 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:42567
2025-10-02 16:11:27,400 - distributed.worker - INFO -          dashboard at:            10.6.81.2:38133
2025-10-02 16:11:27,400 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,400 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,400 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,400 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,400 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-1zy4hsln
2025-10-02 16:11:27,400 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,410 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,411 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:45519
2025-10-02 16:11:27,411 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:45519
2025-10-02 16:11:27,411 - distributed.worker - INFO -          dashboard at:            10.6.81.2:33797
2025-10-02 16:11:27,411 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,411 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,411 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,411 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,411 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,411 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,411 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-mkwjscwx
2025-10-02 16:11:27,411 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,411 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,416 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:39493
2025-10-02 16:11:27,416 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:39493
2025-10-02 16:11:27,416 - distributed.worker - INFO -          dashboard at:            10.6.81.2:35549
2025-10-02 16:11:27,416 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,416 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,416 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,416 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,416 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-bt46_n_c
2025-10-02 16:11:27,416 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,426 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:39357
2025-10-02 16:11:27,426 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:39357
2025-10-02 16:11:27,426 - distributed.worker - INFO -          dashboard at:            10.6.81.2:38413
2025-10-02 16:11:27,426 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,426 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,426 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,426 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,426 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-t93wmrrc
2025-10-02 16:11:27,426 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,430 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,431 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,431 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,433 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,433 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:41543
2025-10-02 16:11:27,433 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:41543
2025-10-02 16:11:27,433 - distributed.worker - INFO -          dashboard at:            10.6.81.2:42733
2025-10-02 16:11:27,433 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,433 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,433 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,433 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,433 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-26647k0l
2025-10-02 16:11:27,433 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,437 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,438 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:43373
2025-10-02 16:11:27,438 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:43373
2025-10-02 16:11:27,438 - distributed.worker - INFO -          dashboard at:            10.6.81.2:38989
2025-10-02 16:11:27,438 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,438 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,438 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,438 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,438 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-pd_g9e4x
2025-10-02 16:11:27,438 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,438 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,438 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,439 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,439 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:41389
2025-10-02 16:11:27,439 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:37217
2025-10-02 16:11:27,439 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:41389
2025-10-02 16:11:27,439 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:37217
2025-10-02 16:11:27,439 - distributed.worker - INFO -          dashboard at:            10.6.81.2:34307
2025-10-02 16:11:27,440 - distributed.worker - INFO -          dashboard at:            10.6.81.2:37807
2025-10-02 16:11:27,440 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,440 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,440 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,440 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,440 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,440 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,440 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,440 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-hro4lssy
2025-10-02 16:11:27,440 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,440 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,440 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-i6m8v_bq
2025-10-02 16:11:27,440 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,442 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:42031
2025-10-02 16:11:27,442 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:42031
2025-10-02 16:11:27,442 - distributed.worker - INFO -          dashboard at:            10.6.81.2:45393
2025-10-02 16:11:27,442 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,442 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,442 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,442 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,442 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-r6ebg7a9
2025-10-02 16:11:27,442 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,443 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:34349
2025-10-02 16:11:27,443 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:34349
2025-10-02 16:11:27,443 - distributed.worker - INFO -          dashboard at:            10.6.81.2:41641
2025-10-02 16:11:27,443 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,443 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,443 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,443 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,443 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-aezob42a
2025-10-02 16:11:27,443 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,445 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:35023
2025-10-02 16:11:27,445 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:35023
2025-10-02 16:11:27,445 - distributed.worker - INFO -          dashboard at:            10.6.81.2:36837
2025-10-02 16:11:27,445 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,446 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,446 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,446 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,446 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-gqrd7jmk
2025-10-02 16:11:27,446 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,447 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,448 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,448 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,448 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:34331
2025-10-02 16:11:27,449 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:34331
2025-10-02 16:11:27,449 - distributed.worker - INFO -          dashboard at:            10.6.81.2:34731
2025-10-02 16:11:27,449 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,449 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,449 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,449 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,449 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-cspd9qeh
2025-10-02 16:11:27,449 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,449 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,451 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:43231
2025-10-02 16:11:27,451 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:43231
2025-10-02 16:11:27,451 - distributed.worker - INFO -          dashboard at:            10.6.81.2:43475
2025-10-02 16:11:27,451 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,451 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,451 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,451 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,451 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-k7bhveey
2025-10-02 16:11:27,451 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,452 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,453 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,453 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,453 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:35245
2025-10-02 16:11:27,454 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:35245
2025-10-02 16:11:27,454 - distributed.worker - INFO -          dashboard at:            10.6.81.2:44879
2025-10-02 16:11:27,454 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,454 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,454 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,454 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,454 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-mnkzl80f
2025-10-02 16:11:27,454 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,454 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,458 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,458 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,459 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,459 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,459 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,459 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,459 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:37641
2025-10-02 16:11:27,459 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:37641
2025-10-02 16:11:27,459 - distributed.worker - INFO -          dashboard at:            10.6.81.2:33867
2025-10-02 16:11:27,459 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,459 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,459 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,459 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,459 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-2sj4u9nh
2025-10-02 16:11:27,459 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,459 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,460 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,460 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,461 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,461 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,462 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,463 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,463 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:43987
2025-10-02 16:11:27,463 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:43987
2025-10-02 16:11:27,463 - distributed.worker - INFO -          dashboard at:            10.6.81.2:46623
2025-10-02 16:11:27,463 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,463 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,463 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,463 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,463 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-u_14cexa
2025-10-02 16:11:27,463 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,463 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,463 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,463 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,464 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,464 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,465 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,465 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,466 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,466 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:40687
2025-10-02 16:11:27,466 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:40687
2025-10-02 16:11:27,466 - distributed.worker - INFO -          dashboard at:            10.6.81.2:34659
2025-10-02 16:11:27,466 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,466 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,466 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,466 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,466 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-v6emg5hf
2025-10-02 16:11:27,466 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,467 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,467 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,467 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,468 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,468 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,468 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,470 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,470 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,471 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,471 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,471 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,472 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,472 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,472 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,472 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,472 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,472 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,473 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,474 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,475 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:46537
2025-10-02 16:11:27,475 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:46537
2025-10-02 16:11:27,475 - distributed.worker - INFO -          dashboard at:            10.6.81.2:44625
2025-10-02 16:11:27,475 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,475 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,475 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,475 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,475 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-yxxvsxbz
2025-10-02 16:11:27,475 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,477 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,477 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,477 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,478 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,480 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:34849
2025-10-02 16:11:27,480 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:34849
2025-10-02 16:11:27,480 - distributed.worker - INFO -          dashboard at:            10.6.81.2:35019
2025-10-02 16:11:27,480 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,480 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,481 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,481 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,481 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-dj04ron4
2025-10-02 16:11:27,481 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,483 - distributed.worker - INFO -       Start worker at:      tcp://10.6.81.2:35391
2025-10-02 16:11:27,483 - distributed.worker - INFO -          Listening to:      tcp://10.6.81.2:35391
2025-10-02 16:11:27,483 - distributed.worker - INFO -          dashboard at:            10.6.81.2:41109
2025-10-02 16:11:27,483 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,483 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,483 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:11:27,483 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:11:27,483 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-bmga7szq
2025-10-02 16:11:27,483 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,486 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,487 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,487 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,487 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,488 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,488 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,488 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,489 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,496 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,496 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,496 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,497 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:11:27,499 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:11:27,500 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:11:27,501 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:11:27,502 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:22,418 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,418 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,419 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,419 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,419 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,419 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,420 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,420 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,420 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,420 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,420 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,420 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,421 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,421 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,421 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,420 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,421 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,421 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,421 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,421 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,421 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,422 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,422 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,422 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,422 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,422 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,422 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,422 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,422 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,422 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,423 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,423 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,423 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,423 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,423 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,423 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,423 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,423 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,423 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,421 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,424 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,424 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,424 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,424 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,424 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,424 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,424 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,424 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,424 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,422 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,424 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,424 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,425 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,425 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,425 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,425 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,425 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,423 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,423 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,425 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,424 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,426 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,426 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,426 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,426 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,424 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,427 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,427 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,427 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,425 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,425 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,425 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,428 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,426 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,427 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,427 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,427 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,427 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,427 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,428 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,428 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,428 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,429 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,429 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,432 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,434 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,435 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,436 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,436 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,436 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,437 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,437 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,437 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,438 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,438 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,438 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,438 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,438 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,439 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,440 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,441 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,445 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,446 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,447 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:26,848 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,848 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,848 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,848 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,849 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,849 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,849 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,849 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,849 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,849 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,849 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,849 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,850 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,850 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,850 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,850 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,850 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,850 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,850 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,850 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,851 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,851 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,851 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,851 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,851 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,852 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,851 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,851 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,851 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,850 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,852 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,852 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,853 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,851 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,852 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,852 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,852 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,852 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,852 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,852 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,852 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,852 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,852 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,852 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,852 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,852 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,852 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,852 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,852 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,852 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,853 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,853 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,853 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,853 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,853 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,853 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,854 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,854 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,853 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,854 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,854 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,853 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,854 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,853 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,854 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,854 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,854 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,854 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,854 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,854 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,854 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,854 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,855 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,855 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,855 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,855 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,855 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,854 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,856 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,856 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,856 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,856 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,856 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,856 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,856 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,856 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,856 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,856 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,853 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,856 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,857 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,857 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,857 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,857 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,857 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,857 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,854 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:26,858 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,858 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,858 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,858 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,858 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,860 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:26,861 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:28,298 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,298 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,298 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,298 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,298 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,298 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,298 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,298 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,299 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,299 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,299 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,299 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,299 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,299 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,299 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,299 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,299 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,300 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,300 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,300 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,300 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,300 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,300 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,301 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,301 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,301 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,301 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,301 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,301 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,301 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,301 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,301 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,301 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,301 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,302 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,302 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,302 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,302 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,302 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,302 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,302 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,302 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,302 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,302 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,302 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,303 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,303 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,303 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,303 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,303 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,303 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,303 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,303 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,303 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,303 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,302 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,303 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,304 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,304 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,304 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,304 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,304 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,304 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,304 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,304 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,304 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,304 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,305 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,305 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,305 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,305 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,305 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,305 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,305 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,305 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,306 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,306 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,306 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,306 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,305 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,306 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,306 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,306 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,305 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,306 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,306 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,306 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,307 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,307 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,307 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,307 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,307 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,307 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,307 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,307 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,307 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,307 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,308 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,308 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,308 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,309 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,312 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,314 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,316 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:29,726 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,726 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,726 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,726 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,726 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,726 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,726 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,726 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,726 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,727 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,727 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,727 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,727 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,727 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,727 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,727 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,728 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,728 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,728 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,728 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,728 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,728 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,728 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,728 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,728 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,728 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,728 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,728 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,728 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,728 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,728 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,729 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,729 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,728 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,729 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,729 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,729 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,729 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,729 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,729 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,729 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,729 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,729 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,729 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,729 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,729 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,729 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,729 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,729 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,730 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,730 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,730 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,730 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,730 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,730 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,730 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,730 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,730 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,730 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,730 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,731 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,731 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,731 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,731 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,731 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,731 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,731 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,731 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,731 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,731 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,731 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,731 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,731 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,731 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,731 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,732 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,731 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,732 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,732 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,732 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,732 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,731 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:29,732 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,732 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,732 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,732 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,732 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,733 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,733 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,733 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,733 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,733 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,733 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,733 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,733 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,734 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,734 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,734 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,734 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,734 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,734 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,734 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,735 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:29,739 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:49,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:49,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:49,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:49,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:49,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:49,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:49,698 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:49,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:49,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:49,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:49,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:49,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:49,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 21:12:57,975 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,975 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,975 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,975 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,975 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,975 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,975 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,975 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,975 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,975 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,977 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:46537. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,977 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:34849. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,977 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:35023. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,977 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:41141. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,980 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:41457. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,977 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:35245. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,977 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:44639. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,977 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:43373. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,975 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,977 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:34495. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,975 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,977 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:39493. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,977 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:44647. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,978 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:34331. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,978 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:43987. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,978 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:41389. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,978 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:46155. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,978 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:40983. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,978 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:35061. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,978 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:37217. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,978 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:42599. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,978 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:43231. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,979 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:46331. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,979 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:37693. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,979 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:45519. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,979 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:37537. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,979 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:39125. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,979 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:37267. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,979 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:35397. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,979 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:33921. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,979 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:40487. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,979 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:39357. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,980 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:43803. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,980 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:38777. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,980 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:45245. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,980 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:35079. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,976 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,980 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:33979. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,980 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:42971. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,980 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:42567. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,980 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:45869. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,981 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:34349. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,981 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:37641. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,981 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:42031. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,981 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:34517. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,981 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:39353. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,982 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:45199. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,983 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.81.2:42082 remote=tcp://10.6.81.1:8752>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 21:12:57,983 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.81.2:42042 remote=tcp://10.6.81.1:8752>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 21:12:57,983 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.81.2:42044 remote=tcp://10.6.81.1:8752>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 21:12:57,983 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.81.2:42068 remote=tcp://10.6.81.1:8752>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 21:12:57,983 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.81.2:42060 remote=tcp://10.6.81.1:8752>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 21:12:57,983 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.81.2:42058 remote=tcp://10.6.81.1:8752>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 21:12:57,983 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.81.2:42088 remote=tcp://10.6.81.1:8752>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 21:12:57,984 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.81.2:42108 remote=tcp://10.6.81.1:8752>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 21:12:57,983 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.81.2:42064 remote=tcp://10.6.81.1:8752>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 21:12:57,990 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,992 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:46211'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,993 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,994 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,994 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,994 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,994 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,998 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:35391. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,999 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,002 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:37729'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,003 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:42129'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,003 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:33957'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,003 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:34217'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,003 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:34515'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,003 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,003 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:36407'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,003 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,003 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,003 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:39689'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,003 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,004 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,004 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:45871'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,004 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,004 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,004 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,004 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,004 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:41115'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,004 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,004 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:38727'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,004 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,004 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,004 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,004 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,004 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,004 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:35543'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,004 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,004 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,004 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,004 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,004 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,004 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,004 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:45145'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,004 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,004 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,004 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,004 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,005 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:34023'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,004 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,005 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:33163'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,005 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:37005'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,005 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:37763'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,005 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:41567'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,005 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:33515'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,005 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:37453'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,005 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,006 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,006 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,006 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:34115'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,006 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,006 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,006 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,006 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,006 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,006 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,006 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:38707'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,006 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,006 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,006 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:39651'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,006 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,006 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,006 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,006 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,006 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:35043'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,006 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,006 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,006 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,006 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,006 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,006 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,007 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:39155'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,007 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,006 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,007 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:39113'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,007 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:33693'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,007 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:45859'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,007 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:37993'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,007 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,008 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:44121'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,008 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:45137'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,008 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:35093'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,008 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:39411'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,008 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:38889'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,008 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,009 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:43511'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,009 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:38231'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,009 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:45997'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,009 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:40929'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,009 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:39179'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,009 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,010 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,010 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:40603'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,010 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,010 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,010 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,010 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,010 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,010 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,010 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:33373'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,010 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,010 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,010 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,010 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,010 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,010 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,010 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,010 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,010 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:34803'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,010 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,010 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,010 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,010 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,010 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,010 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,010 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,010 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,010 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,010 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:33963'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,010 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,011 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,011 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,011 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,011 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,011 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,011 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,011 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,011 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:33421'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,011 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,011 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,011 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,011 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,011 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,011 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,011 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,011 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,011 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,011 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,011 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,011 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,011 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,011 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,011 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,011 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,011 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,011 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,011 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,012 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,012 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,012 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,012 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,012 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,012 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,012 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,012 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,012 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,012 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,012 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,012 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,012 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,012 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,012 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,012 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,012 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,013 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,013 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,013 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,013 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,013 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,013 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,013 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,013 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,013 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,014 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,007 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 21:12:58,014 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,014 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,014 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,014 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,015 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,015 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,015 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,016 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,016 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,016 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,016 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,017 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,011 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 21:12:58,017 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,017 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,017 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,017 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,018 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,018 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,018 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,018 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,020 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,020 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,013 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14bcb28f03d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 21:12:58,022 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,017 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14ec20862d50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 21:12:58,026 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,031 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:58,031 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:58,039 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:40687. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,239 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:58,247 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:41543. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,249 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:58,256 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:40853. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,260 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:58,268 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:32939. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,286 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:58,294 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:36853. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,303 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:58,310 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:45267. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,332 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:58,339 - distributed.worker - INFO - Stopping worker at tcp://10.6.81.2:44765. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,344 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:37061'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,358 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:35529'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,359 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 7, 22, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 21:12:58,394 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 6, 9, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 21:12:58,400 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,401 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,405 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,406 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,411 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,411 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,415 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:36319'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,416 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,417 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,421 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,422 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,426 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:33613'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,427 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:39227'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,431 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 0, 20, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 21:12:58,442 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 1, 19, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 21:12:58,444 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:33135'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,470 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 2, 1, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 21:12:58,484 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:44019'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,498 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,503 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,509 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,514 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,514 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,519 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,520 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,522 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.81.2:38219'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,525 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,530 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,535 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,772 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,778 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,782 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,782 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,782 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,790 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 9, 16, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 21:12:58,796 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 0, 16, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 21:12:58,825 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 1, 3, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 21:12:58,934 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,939 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,944 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,946 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,950 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,951 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,955 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,956 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,962 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,967 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
