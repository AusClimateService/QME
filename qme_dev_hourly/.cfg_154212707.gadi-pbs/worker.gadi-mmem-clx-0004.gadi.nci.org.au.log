Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-11-10 11:41:41,554 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:36459'
2025-11-10 11:41:41,561 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:39027'
2025-11-10 11:41:41,565 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:35667'
2025-11-10 11:41:41,568 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:39769'
2025-11-10 11:41:41,573 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:46567'
2025-11-10 11:41:41,577 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:38345'
2025-11-10 11:41:41,583 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:46495'
2025-11-10 11:41:41,589 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:38935'
2025-11-10 11:41:41,594 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:42029'
2025-11-10 11:41:41,597 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:42111'
2025-11-10 11:41:41,601 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:44699'
2025-11-10 11:41:41,605 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:36699'
2025-11-10 11:41:41,609 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:41203'
2025-11-10 11:41:41,613 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:38037'
2025-11-10 11:41:41,617 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:38613'
2025-11-10 11:41:41,620 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:46073'
2025-11-10 11:41:41,624 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:42679'
2025-11-10 11:41:41,626 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:33161'
2025-11-10 11:41:41,631 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:35665'
2025-11-10 11:41:41,635 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:37219'
2025-11-10 11:41:41,639 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:43451'
2025-11-10 11:41:41,748 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:38255'
2025-11-10 11:41:41,753 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:33367'
2025-11-10 11:41:41,757 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:35697'
2025-11-10 11:41:41,761 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:45963'
2025-11-10 11:41:41,765 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:38025'
2025-11-10 11:41:41,769 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:45405'
2025-11-10 11:41:41,773 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:45361'
2025-11-10 11:41:41,777 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:33595'
2025-11-10 11:41:41,782 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:38181'
2025-11-10 11:41:41,785 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:35571'
2025-11-10 11:41:41,791 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:34559'
2025-11-10 11:41:41,795 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:38233'
2025-11-10 11:41:41,799 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:38315'
2025-11-10 11:41:41,803 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:44161'
2025-11-10 11:41:41,807 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:41919'
2025-11-10 11:41:41,812 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:42825'
2025-11-10 11:41:41,816 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:39693'
2025-11-10 11:41:41,820 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:36057'
2025-11-10 11:41:41,824 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:41495'
2025-11-10 11:41:41,828 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:42739'
2025-11-10 11:41:41,833 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:36589'
2025-11-10 11:41:41,835 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:38881'
2025-11-10 11:41:41,837 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:39459'
2025-11-10 11:41:41,842 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:40171'
2025-11-10 11:41:41,846 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:44465'
2025-11-10 11:41:41,851 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:39601'
2025-11-10 11:41:41,854 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.31:37617'
2025-11-10 11:41:42,976 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:41515
2025-11-10 11:41:42,976 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:46769
2025-11-10 11:41:42,976 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:38491
2025-11-10 11:41:42,977 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:41515
2025-11-10 11:41:42,977 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:46769
2025-11-10 11:41:42,977 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:38491
2025-11-10 11:41:42,977 - distributed.worker - INFO -          dashboard at:            10.6.5.31:38823
2025-11-10 11:41:42,977 - distributed.worker - INFO -          dashboard at:            10.6.5.31:32953
2025-11-10 11:41:42,977 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:42,977 - distributed.worker - INFO -          dashboard at:            10.6.5.31:38855
2025-11-10 11:41:42,977 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:42,977 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:42,977 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:42,977 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:42,977 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:42,977 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:42,977 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:42,977 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:42,977 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:42,977 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:42,977 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-1dalg81w
2025-11-10 11:41:42,977 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:42,977 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-vsjbbfyp
2025-11-10 11:41:42,977 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:42,977 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-_viz9rlo
2025-11-10 11:41:42,977 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:42,977 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:42,982 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:35319
2025-11-10 11:41:42,983 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:35319
2025-11-10 11:41:42,983 - distributed.worker - INFO -          dashboard at:            10.6.5.31:36235
2025-11-10 11:41:42,983 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:42,983 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:42,983 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:42,983 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:42,983 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-hjg5tdla
2025-11-10 11:41:42,983 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:42,983 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:36001
2025-11-10 11:41:42,983 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:36001
2025-11-10 11:41:42,983 - distributed.worker - INFO -          dashboard at:            10.6.5.31:33127
2025-11-10 11:41:42,983 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:42,983 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:42,983 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:42,984 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:42,984 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-i00vskfe
2025-11-10 11:41:42,984 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:42,989 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:45981
2025-11-10 11:41:42,989 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:45981
2025-11-10 11:41:42,989 - distributed.worker - INFO -          dashboard at:            10.6.5.31:37951
2025-11-10 11:41:42,989 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:42,989 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:42,989 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:42,989 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:42,989 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-dhgfid17
2025-11-10 11:41:42,989 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:42,989 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:36573
2025-11-10 11:41:42,990 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:36573
2025-11-10 11:41:42,990 - distributed.worker - INFO -          dashboard at:            10.6.5.31:35853
2025-11-10 11:41:42,990 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:42,990 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:42,990 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:42,990 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:42,990 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-gf5bvemq
2025-11-10 11:41:42,990 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:42,996 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:42,996 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:42,997 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:42,997 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:42,998 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:42,998 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:42,999 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:42,999 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:42,999 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,000 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,000 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,001 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,002 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,003 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,003 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,003 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,004 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,004 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,004 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,005 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,006 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,006 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,006 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,007 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,008 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,008 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,008 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,009 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,014 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:43647
2025-11-10 11:41:43,014 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:43647
2025-11-10 11:41:43,014 - distributed.worker - INFO -          dashboard at:            10.6.5.31:42049
2025-11-10 11:41:43,014 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,014 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,014 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,014 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,014 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-r3wazvip
2025-11-10 11:41:43,014 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,025 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,026 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,026 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,027 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,109 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:36393
2025-11-10 11:41:43,109 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:36393
2025-11-10 11:41:43,109 - distributed.worker - INFO -          dashboard at:            10.6.5.31:33423
2025-11-10 11:41:43,109 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,109 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,109 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:37223
2025-11-10 11:41:43,109 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,109 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,109 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:37223
2025-11-10 11:41:43,109 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-4bzz3vrk
2025-11-10 11:41:43,109 - distributed.worker - INFO -          dashboard at:            10.6.5.31:38441
2025-11-10 11:41:43,109 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,109 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,109 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,109 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,109 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,109 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-avgu8txs
2025-11-10 11:41:43,110 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,116 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:34021
2025-11-10 11:41:43,116 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:34021
2025-11-10 11:41:43,116 - distributed.worker - INFO -          dashboard at:            10.6.5.31:37715
2025-11-10 11:41:43,116 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,116 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,116 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,116 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,116 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-vbsdlko2
2025-11-10 11:41:43,116 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,120 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:43173
2025-11-10 11:41:43,120 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:43173
2025-11-10 11:41:43,120 - distributed.worker - INFO -          dashboard at:            10.6.5.31:43195
2025-11-10 11:41:43,120 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,120 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,120 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,120 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,120 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-3fx1b83y
2025-11-10 11:41:43,120 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,120 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,121 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,121 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,122 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,127 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,127 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,127 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,128 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,129 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:35725
2025-11-10 11:41:43,129 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:35725
2025-11-10 11:41:43,129 - distributed.worker - INFO -          dashboard at:            10.6.5.31:43409
2025-11-10 11:41:43,129 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,129 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,129 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,129 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,129 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,130 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-_vl7yfa9
2025-11-10 11:41:43,130 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,130 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,130 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,132 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,132 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,132 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,132 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,133 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,140 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:43767
2025-11-10 11:41:43,140 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:43767
2025-11-10 11:41:43,140 - distributed.worker - INFO -          dashboard at:            10.6.5.31:42197
2025-11-10 11:41:43,140 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,141 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,141 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,141 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,141 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-qlfykycl
2025-11-10 11:41:43,141 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,143 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,143 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,143 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,144 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,145 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:35055
2025-11-10 11:41:43,146 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:35055
2025-11-10 11:41:43,146 - distributed.worker - INFO -          dashboard at:            10.6.5.31:43875
2025-11-10 11:41:43,146 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,146 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,146 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,146 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,146 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-7iamoqhb
2025-11-10 11:41:43,146 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,146 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:41301
2025-11-10 11:41:43,147 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:41301
2025-11-10 11:41:43,147 - distributed.worker - INFO -          dashboard at:            10.6.5.31:37467
2025-11-10 11:41:43,147 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,147 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,147 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,147 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,147 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-abaz61mw
2025-11-10 11:41:43,147 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,151 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:43365
2025-11-10 11:41:43,151 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:43365
2025-11-10 11:41:43,151 - distributed.worker - INFO -          dashboard at:            10.6.5.31:43793
2025-11-10 11:41:43,151 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,151 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,151 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,151 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,151 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-kuqulhtx
2025-11-10 11:41:43,151 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,155 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:43957
2025-11-10 11:41:43,155 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:43957
2025-11-10 11:41:43,155 - distributed.worker - INFO -          dashboard at:            10.6.5.31:45433
2025-11-10 11:41:43,155 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,155 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,155 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,155 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,155 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-dhq4fmtx
2025-11-10 11:41:43,155 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,158 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,158 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,159 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,159 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,160 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,161 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,161 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,162 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,163 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,164 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,164 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,166 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,166 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,166 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,166 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,167 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:34645
2025-11-10 11:41:43,167 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:34645
2025-11-10 11:41:43,167 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,167 - distributed.worker - INFO -          dashboard at:            10.6.5.31:36675
2025-11-10 11:41:43,167 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,167 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,167 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,167 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,167 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-oh2om5s2
2025-11-10 11:41:43,168 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,170 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:43453
2025-11-10 11:41:43,171 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:43453
2025-11-10 11:41:43,171 - distributed.worker - INFO -          dashboard at:            10.6.5.31:43605
2025-11-10 11:41:43,171 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,171 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,171 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,171 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,171 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-54t401uz
2025-11-10 11:41:43,171 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,177 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,178 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,178 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,178 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,179 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,179 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,179 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,179 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,182 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,182 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,182 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,183 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,207 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:35733
2025-11-10 11:41:43,207 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:35733
2025-11-10 11:41:43,207 - distributed.worker - INFO -          dashboard at:            10.6.5.31:35971
2025-11-10 11:41:43,207 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,207 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,207 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,207 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,207 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-7kspgu08
2025-11-10 11:41:43,207 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,223 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:40645
2025-11-10 11:41:43,223 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:40645
2025-11-10 11:41:43,223 - distributed.worker - INFO -          dashboard at:            10.6.5.31:41989
2025-11-10 11:41:43,223 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,223 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,223 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,223 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,223 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-ktqrh43u
2025-11-10 11:41:43,223 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,224 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,225 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,225 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,227 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,235 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,235 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,235 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,236 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,248 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:37613
2025-11-10 11:41:43,248 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:37613
2025-11-10 11:41:43,248 - distributed.worker - INFO -          dashboard at:            10.6.5.31:44833
2025-11-10 11:41:43,248 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,248 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,248 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,248 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,248 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-lkclfgtf
2025-11-10 11:41:43,249 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,255 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:44557
2025-11-10 11:41:43,255 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:44557
2025-11-10 11:41:43,255 - distributed.worker - INFO -          dashboard at:            10.6.5.31:33693
2025-11-10 11:41:43,255 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,255 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,255 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,255 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,256 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-5sanxzox
2025-11-10 11:41:43,256 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,264 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:39831
2025-11-10 11:41:43,264 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:39831
2025-11-10 11:41:43,265 - distributed.worker - INFO -          dashboard at:            10.6.5.31:36543
2025-11-10 11:41:43,265 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,265 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,265 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,265 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,265 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-z8l49esh
2025-11-10 11:41:43,265 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,269 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,269 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,270 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,270 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,271 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,271 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,271 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,272 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,278 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:46677
2025-11-10 11:41:43,278 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:46677
2025-11-10 11:41:43,278 - distributed.worker - INFO -          dashboard at:            10.6.5.31:40289
2025-11-10 11:41:43,278 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,278 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,278 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,278 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,278 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-8s6vcpo8
2025-11-10 11:41:43,278 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,283 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,284 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,284 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,285 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,297 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,297 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,298 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,299 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,311 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:44053
2025-11-10 11:41:43,311 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:44053
2025-11-10 11:41:43,311 - distributed.worker - INFO -          dashboard at:            10.6.5.31:36839
2025-11-10 11:41:43,311 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,311 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,311 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,311 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,311 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-j8fpokyt
2025-11-10 11:41:43,311 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,331 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,332 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,332 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,333 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,365 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:45165
2025-11-10 11:41:43,365 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:45165
2025-11-10 11:41:43,365 - distributed.worker - INFO -          dashboard at:            10.6.5.31:43497
2025-11-10 11:41:43,365 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,365 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,365 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,365 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,366 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-3ptt73ra
2025-11-10 11:41:43,366 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,377 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,377 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,377 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,378 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,392 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:37463
2025-11-10 11:41:43,392 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:37463
2025-11-10 11:41:43,392 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:35001
2025-11-10 11:41:43,393 - distributed.worker - INFO -          dashboard at:            10.6.5.31:37779
2025-11-10 11:41:43,393 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:35001
2025-11-10 11:41:43,393 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,393 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,393 - distributed.worker - INFO -          dashboard at:            10.6.5.31:38549
2025-11-10 11:41:43,393 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,393 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,393 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,393 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:42137
2025-11-10 11:41:43,393 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,393 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-_5ddl8oo
2025-11-10 11:41:43,393 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,393 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:42137
2025-11-10 11:41:43,393 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,393 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,393 - distributed.worker - INFO -          dashboard at:            10.6.5.31:33551
2025-11-10 11:41:43,393 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-mzktc2rr
2025-11-10 11:41:43,393 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,393 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,393 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,393 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,393 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,393 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-_o15ks4x
2025-11-10 11:41:43,393 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,396 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:43175
2025-11-10 11:41:43,396 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:43175
2025-11-10 11:41:43,396 - distributed.worker - INFO -          dashboard at:            10.6.5.31:35503
2025-11-10 11:41:43,396 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,396 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,396 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:45563
2025-11-10 11:41:43,396 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,396 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,396 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:45563
2025-11-10 11:41:43,396 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-on8s2m8e
2025-11-10 11:41:43,396 - distributed.worker - INFO -          dashboard at:            10.6.5.31:44977
2025-11-10 11:41:43,396 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,396 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,396 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,396 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,397 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,397 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-vwlbkrdp
2025-11-10 11:41:43,397 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,398 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:45989
2025-11-10 11:41:43,398 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:45989
2025-11-10 11:41:43,398 - distributed.worker - INFO -          dashboard at:            10.6.5.31:46825
2025-11-10 11:41:43,398 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,398 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,398 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,398 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,398 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-202jyxiy
2025-11-10 11:41:43,398 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,400 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:33545
2025-11-10 11:41:43,400 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:33545
2025-11-10 11:41:43,400 - distributed.worker - INFO -          dashboard at:            10.6.5.31:38279
2025-11-10 11:41:43,400 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,400 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,400 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,400 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,400 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-slo5gtxl
2025-11-10 11:41:43,400 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,400 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:44875
2025-11-10 11:41:43,400 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:44875
2025-11-10 11:41:43,400 - distributed.worker - INFO -          dashboard at:            10.6.5.31:46255
2025-11-10 11:41:43,400 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,400 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,400 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,400 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,400 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-3sbkafvw
2025-11-10 11:41:43,401 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,402 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:41323
2025-11-10 11:41:43,402 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:41323
2025-11-10 11:41:43,402 - distributed.worker - INFO -          dashboard at:            10.6.5.31:38573
2025-11-10 11:41:43,402 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,402 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,402 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,402 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,402 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-hla5tq96
2025-11-10 11:41:43,403 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,407 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,407 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,407 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,408 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,409 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,409 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,409 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,410 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,411 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,411 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,411 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,412 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,414 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,414 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,414 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,415 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:44931
2025-11-10 11:41:43,415 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:44931
2025-11-10 11:41:43,415 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,415 - distributed.worker - INFO -          dashboard at:            10.6.5.31:42427
2025-11-10 11:41:43,415 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,415 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,415 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,415 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,415 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-6vsae825
2025-11-10 11:41:43,415 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,415 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,415 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,415 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,416 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,417 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,418 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,418 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,419 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,419 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,420 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,420 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,421 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,421 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,422 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,422 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,423 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,424 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,424 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,424 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,424 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:42799
2025-11-10 11:41:43,424 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:42799
2025-11-10 11:41:43,424 - distributed.worker - INFO -          dashboard at:            10.6.5.31:44501
2025-11-10 11:41:43,424 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,424 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,425 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,425 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,425 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-dd3jmd2m
2025-11-10 11:41:43,425 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,425 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,426 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:46551
2025-11-10 11:41:43,426 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:35491
2025-11-10 11:41:43,426 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:46551
2025-11-10 11:41:43,426 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:35491
2025-11-10 11:41:43,426 - distributed.worker - INFO -          dashboard at:            10.6.5.31:33939
2025-11-10 11:41:43,426 - distributed.worker - INFO -          dashboard at:            10.6.5.31:38703
2025-11-10 11:41:43,426 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,426 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,426 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,426 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,426 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,426 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,426 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,426 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,426 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-rp10e7s6
2025-11-10 11:41:43,426 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-v_vj5op1
2025-11-10 11:41:43,426 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,426 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,428 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:33187
2025-11-10 11:41:43,428 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:33187
2025-11-10 11:41:43,428 - distributed.worker - INFO -          dashboard at:            10.6.5.31:40415
2025-11-10 11:41:43,428 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,428 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,428 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,428 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,428 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-y_1t0sk1
2025-11-10 11:41:43,428 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,431 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:41051
2025-11-10 11:41:43,432 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:41051
2025-11-10 11:41:43,432 - distributed.worker - INFO -          dashboard at:            10.6.5.31:36079
2025-11-10 11:41:43,432 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,432 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,432 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,432 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,432 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-dsasmsvq
2025-11-10 11:41:43,432 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,435 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:34715
2025-11-10 11:41:43,435 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:34715
2025-11-10 11:41:43,435 - distributed.worker - INFO -          dashboard at:            10.6.5.31:37413
2025-11-10 11:41:43,435 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,435 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,435 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,435 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,436 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-z0dipiua
2025-11-10 11:41:43,436 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,436 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:46771
2025-11-10 11:41:43,436 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:46771
2025-11-10 11:41:43,436 - distributed.worker - INFO -          dashboard at:            10.6.5.31:32983
2025-11-10 11:41:43,436 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,436 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,436 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,436 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,436 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-b9bxv1bu
2025-11-10 11:41:43,436 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,437 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,437 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:36325
2025-11-10 11:41:43,437 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,437 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,437 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:36325
2025-11-10 11:41:43,438 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,438 - distributed.worker - INFO -          dashboard at:            10.6.5.31:46323
2025-11-10 11:41:43,438 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,438 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,438 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,438 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,438 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-rh05axca
2025-11-10 11:41:43,438 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,438 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,438 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,439 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,439 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,439 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:35265
2025-11-10 11:41:43,439 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:35265
2025-11-10 11:41:43,439 - distributed.worker - INFO -          dashboard at:            10.6.5.31:37595
2025-11-10 11:41:43,439 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,439 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,439 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,440 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,440 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-r3qvlb92
2025-11-10 11:41:43,440 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,440 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,440 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,440 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,441 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,441 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,441 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,441 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,442 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,445 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,445 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,446 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,447 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,451 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,451 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,452 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,453 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,454 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,455 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,455 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,455 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,456 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,456 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,456 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,457 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,457 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,458 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,458 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,460 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,460 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,460 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.31:33267
2025-11-10 11:41:43,461 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.31:33267
2025-11-10 11:41:43,461 - distributed.worker - INFO -          dashboard at:            10.6.5.31:42673
2025-11-10 11:41:43,461 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,461 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,461 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,461 - distributed.worker - INFO -               Threads:                          1
2025-11-10 11:41:43,461 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,461 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-11-10 11:41:43,461 - distributed.worker - INFO -       Local Directory: /jobfs/154212707.gadi-pbs/dask-scratch-space/worker-llqim7c8
2025-11-10 11:41:43,461 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,462 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:43,480 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-11-10 11:41:43,481 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.30:8767
2025-11-10 11:41:43,481 - distributed.worker - INFO - -------------------------------------------------
2025-11-10 11:41:43,482 - distributed.core - INFO - Starting established connection to tcp://10.6.5.30:8767
2025-11-10 11:41:49,887 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,888 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,888 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,888 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,888 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,888 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,888 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,889 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,889 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,889 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,889 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,889 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,889 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,890 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,890 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,890 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,890 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,890 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,890 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,890 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,890 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,891 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,891 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,891 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,891 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,891 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,891 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,891 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,891 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,891 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,892 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,892 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,892 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,892 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,892 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,892 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,892 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,892 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,892 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,892 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,893 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,893 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,893 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,891 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,893 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,893 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,892 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,893 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,893 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,893 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,894 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,894 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,894 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,894 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,894 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,894 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,894 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,894 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,894 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,894 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,895 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,895 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,895 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,895 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,895 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,895 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,895 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,895 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,895 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,895 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,895 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,895 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,896 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,896 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,896 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,896 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,897 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,897 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,897 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,897 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,897 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,897 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,897 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,897 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,897 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,898 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,898 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,898 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,898 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,898 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,898 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,898 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,898 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,902 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:49,909 - distributed.worker - INFO - Starting Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 11:41:49,913 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-11-10 11:41:52,503 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,503 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,504 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,504 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,504 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,504 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,504 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,505 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,505 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,505 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,505 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,505 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,506 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,505 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,506 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,506 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,505 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,505 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,506 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,506 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,506 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,506 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,506 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,506 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,507 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,507 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,507 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,507 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,507 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,507 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,507 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,507 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,507 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,507 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,508 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,508 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,508 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,508 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,508 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,508 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,508 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,509 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,508 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,509 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,507 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,509 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,509 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,509 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,509 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,509 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,510 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,510 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,510 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,510 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,511 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,510 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,510 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,508 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,511 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,511 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,511 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,511 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,510 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,509 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,511 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,511 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,511 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,511 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,511 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,509 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,512 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,512 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,512 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,510 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,511 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,512 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,512 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,512 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,513 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,512 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,513 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,513 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,513 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,513 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,512 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,513 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,513 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,513 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,514 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,514 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,514 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,514 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,514 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,514 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,513 - distributed.worker - INFO - Starting Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 11:41:52,516 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-11-10 11:41:52,591 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,591 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,592 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,592 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,592 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,592 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,592 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,593 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,593 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,593 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,593 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,594 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,594 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,594 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,594 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,594 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,594 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,594 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,594 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,594 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,594 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,594 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,595 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,595 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,595 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,595 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,595 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,595 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,595 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,595 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,596 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,596 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,596 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,596 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,596 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,596 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,596 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,596 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,596 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,596 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,596 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,597 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,597 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,597 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,597 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,597 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,597 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,597 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,598 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,598 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,598 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,598 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,598 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,598 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,598 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,598 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,598 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,598 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,599 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,599 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,599 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,599 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,599 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,599 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,599 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,600 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,600 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,600 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,600 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,600 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,600 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,600 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,600 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,600 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,600 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,601 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,601 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,601 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,601 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,601 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,601 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,601 - distributed.worker - INFO - Starting Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 11:41:52,601 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,602 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,602 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,602 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,602 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,602 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,602 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,603 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,603 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,603 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,603 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,603 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,604 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,604 - distributed.utils - INFO - Reload module qme_train from .py file
2025-11-10 11:41:52,691 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,691 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,692 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,692 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,692 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,692 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,692 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,693 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,693 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,693 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,693 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,693 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,693 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,693 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,693 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,693 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,693 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,693 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,694 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,694 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,694 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,694 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,694 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,694 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,694 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,694 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,694 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,695 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,695 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,695 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,695 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,695 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,695 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,695 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,695 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,695 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,695 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,696 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,696 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,696 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,696 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,696 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,696 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,696 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,696 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,696 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,696 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,697 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,697 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,697 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,697 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,697 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,697 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,697 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,697 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,697 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,697 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,697 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,697 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,697 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,698 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,698 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,698 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,698 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,698 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,698 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,698 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,699 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,699 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,699 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,699 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,699 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,699 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,699 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,699 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,699 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,699 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,699 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,699 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,700 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,700 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,700 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,700 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,700 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,700 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,700 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,700 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,700 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,700 - distributed.worker - INFO - Starting Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 11:41:52,701 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,701 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,701 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,701 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,701 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,701 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:41:52,702 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-11-10 11:49:28,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 75.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 75.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 75.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 75.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 75.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 75.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 75.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 75.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 75.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 75.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 75.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 75.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 75.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 75.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 75.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 56.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 56.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 56.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 56.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:28,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 56.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:29,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 56.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:29,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 56.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:29,081 - distributed.core - INFO - Event loop was unresponsive in Worker for 56.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:29,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 56.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:29,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:29,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 56.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:29,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:29,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 76.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:29,092 - distributed.core - INFO - Event loop was unresponsive in Worker for 56.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:36,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:36,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 84.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:36,995 - distributed.core - INFO - Event loop was unresponsive in Worker for 84.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:49:36,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:50:10,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 98.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:50:10,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 98.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:50:10,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 98.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:30,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:30,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:30,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:30,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:30,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 68.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:30,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 68.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:33,603 - distributed.core - INFO - Event loop was unresponsive in Worker for 71.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:33,603 - distributed.core - INFO - Event loop was unresponsive in Worker for 71.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:33,604 - distributed.core - INFO - Event loop was unresponsive in Worker for 71.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:33,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 67.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:33,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 71.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:33,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 71.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:33,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 67.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:39,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 77.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:39,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 77.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:39,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 73.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:39,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 77.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:42,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 80.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:42,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 80.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:42,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 80.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:42,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 77.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:42,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 80.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:42,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 80.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:42,908 - distributed.core - INFO - Event loop was unresponsive in Worker for 80.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:42,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 77.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:42,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 80.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:42,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 80.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:42,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 80.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:42,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 80.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:42,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 80.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:42,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 80.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:42,953 - distributed.core - INFO - Event loop was unresponsive in Worker for 80.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:42,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 80.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:42,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 80.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:51:43,035 - distributed.core - INFO - Event loop was unresponsive in Worker for 80.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 111.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 109.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 109.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 111.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 116.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 109.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 111.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 112.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 116.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 109.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 111.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 109.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 112.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 109.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 116.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 112.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 112.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 111.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 116.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 109.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 116.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 109.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 109.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 112.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 116.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 112.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 112.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 112.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,738 - distributed.core - INFO - Event loop was unresponsive in Worker for 112.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 112.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 112.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,929 - distributed.core - INFO - Event loop was unresponsive in Worker for 112.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 112.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:42,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 112.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:51,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 121.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:53:53,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 119.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:54:05,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:54:05,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:54:05,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:54:05,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:54:05,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:54:05,883 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:54:05,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:54:05,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:54:06,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 108.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 108.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 108.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,527 - distributed.core - INFO - Event loop was unresponsive in Worker for 108.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 108.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 108.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 108.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 105.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 105.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 105.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 108.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 108.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 105.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 108.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 105.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 108.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 108.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 108.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,660 - distributed.core - INFO - Event loop was unresponsive in Worker for 108.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 108.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 108.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 105.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 108.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 108.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 105.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 108.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 108.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 105.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:42,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 105.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:55:57,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 120.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:06,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 68.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:06,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 68.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:17,868 - distributed.core - INFO - Event loop was unresponsive in Worker for 79.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:17,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 79.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:17,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 79.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:17,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 79.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:17,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 79.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:17,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 79.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:17,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 79.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:17,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 79.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:17,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 79.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:17,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 79.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:24,430 - distributed.core - INFO - Event loop was unresponsive in Worker for 86.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:24,430 - distributed.core - INFO - Event loop was unresponsive in Worker for 86.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:24,431 - distributed.core - INFO - Event loop was unresponsive in Worker for 86.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:24,431 - distributed.core - INFO - Event loop was unresponsive in Worker for 86.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:24,431 - distributed.core - INFO - Event loop was unresponsive in Worker for 86.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:24,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 86.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:24,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 86.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:26,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 88.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:26,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 88.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:26,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 88.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:26,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 88.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:26,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 88.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:26,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 88.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:26,492 - distributed.core - INFO - Event loop was unresponsive in Worker for 88.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:26,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 88.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:26,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 88.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:26,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 88.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:26,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 88.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:26,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 88.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:26,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 88.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:26,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 88.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:26,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 88.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:26,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 88.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:31,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 93.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:31,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 93.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:31,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 93.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:31,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 93.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:31,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 93.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:31,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 93.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:31,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 93.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:31,426 - distributed.core - INFO - Event loop was unresponsive in Worker for 93.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:31,426 - distributed.core - INFO - Event loop was unresponsive in Worker for 93.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:31,426 - distributed.core - INFO - Event loop was unresponsive in Worker for 93.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:31,426 - distributed.core - INFO - Event loop was unresponsive in Worker for 93.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:31,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 93.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:31,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 93.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:40,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:40,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:40,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:40,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:40,655 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:40,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:40,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:57:40,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 77.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 77.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 77.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 77.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 77.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 77.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,038 - distributed.core - INFO - Event loop was unresponsive in Worker for 77.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,038 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 77.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 77.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 77.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 77.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 75.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,044 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:50,044 - distributed.core - INFO - Event loop was unresponsive in Worker for 77.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:55,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 67.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:58:55,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 80.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:59:02,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 74.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 11:59:02,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 74.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 103.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 103.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 100.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 103.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 100.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,430 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 100.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 100.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,453 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 100.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 100.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,458 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 100.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 100.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 100.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 100.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 100.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,492 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 100.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:00:48,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 104.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:01:04,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:01:04,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:01:04,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:01:04,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:01:06,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:01:59,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 67.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:01:59,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 67.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:01:59,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:01:59,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 67.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:01:59,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 67.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:01:59,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:01:59,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 67.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:01:59,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 67.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:01:59,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 67.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:01:59,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:01:59,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 67.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:01:59,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:01:59,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:02:04,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 72.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:02:04,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 72.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:02:04,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 72.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:02:04,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:02:10,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 56.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:02:10,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 56.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:02:16,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:02:16,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:02:16,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:02:16,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:02:16,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:02:16,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 62.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,499 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:42799. Reason: scheduler-close
2025-11-10 12:04:30,500 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:35319. Reason: scheduler-close
2025-11-10 12:04:30,500 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:33545. Reason: scheduler-close
2025-11-10 12:04:30,500 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:35265. Reason: scheduler-close
2025-11-10 12:04:30,500 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:41515. Reason: scheduler-close
2025-11-10 12:04:30,500 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:45165. Reason: scheduler-close
2025-11-10 12:04:30,501 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:43453. Reason: scheduler-close
2025-11-10 12:04:30,501 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:37463. Reason: scheduler-close
2025-11-10 12:04:30,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 128.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 128.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,502 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:43957. Reason: scheduler-close
2025-11-10 12:04:30,502 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:46677. Reason: scheduler-close
2025-11-10 12:04:30,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,505 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48088 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48088 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,505 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47762 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47762 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,505 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48048 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48048 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,506 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47882 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47882 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,506 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47960 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47960 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,507 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48012 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48012 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,511 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:43647. Reason: scheduler-close
2025-11-10 12:04:30,511 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:36325. Reason: scheduler-close
2025-11-10 12:04:30,507 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47930 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47930 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,511 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:43175. Reason: scheduler-close
2025-11-10 12:04:30,511 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:45981. Reason: scheduler-close
2025-11-10 12:04:30,511 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:38491. Reason: scheduler-close
2025-11-10 12:04:30,511 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:41051. Reason: scheduler-close
2025-11-10 12:04:30,507 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47954 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47954 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,509 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47860 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47860 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,506 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47734 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47734 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,515 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47778 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47778 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,516 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47788 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47788 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,516 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48086 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48086 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,516 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47986 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47986 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,517 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47754 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47754 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,517 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48066 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48066 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,522 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:36589'. Reason: scheduler-close
2025-11-10 12:04:30,524 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:39027'. Reason: scheduler-close
2025-11-10 12:04:30,524 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 8, 18))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,524 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,524 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:39601'. Reason: scheduler-close
2025-11-10 12:04:30,525 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,525 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,525 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,525 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,525 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:38025'. Reason: scheduler-close
2025-11-10 12:04:30,525 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:38037'. Reason: scheduler-close
2025-11-10 12:04:30,525 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 7, 4))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,526 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,526 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 8, 3))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,526 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:42825'. Reason: scheduler-close
2025-11-10 12:04:30,526 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,526 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:41323. Reason: scheduler-close
2025-11-10 12:04:30,526 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,526 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,526 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 7, 17))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,526 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,526 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,526 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,526 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 7, 11))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,526 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,526 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,526 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,526 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,526 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,526 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,526 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,527 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,527 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 7, 13))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,527 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:42137. Reason: scheduler-close
2025-11-10 12:04:30,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,527 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:33187. Reason: scheduler-close
2025-11-10 12:04:30,528 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:44931. Reason: scheduler-close
2025-11-10 12:04:30,528 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,528 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,528 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:37617'. Reason: scheduler-close
2025-11-10 12:04:30,529 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,529 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,529 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,529 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:41495'. Reason: scheduler-close
2025-11-10 12:04:30,529 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,529 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,529 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:38613'. Reason: scheduler-close
2025-11-10 12:04:30,529 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,529 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,529 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 128.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,529 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,529 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 7, 19))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,529 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:36393. Reason: scheduler-close
2025-11-10 12:04:30,530 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,530 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,530 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,530 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,530 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,530 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,530 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,530 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:36001. Reason: scheduler-close
2025-11-10 12:04:30,530 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,530 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,531 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 7, 15))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,531 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,532 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,532 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,532 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,532 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,532 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,532 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:42029'. Reason: scheduler-close
2025-11-10 12:04:30,529 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14bfc7bf1610>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,533 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:36057'. Reason: scheduler-close
2025-11-10 12:04:30,533 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:35667'. Reason: scheduler-close
2025-11-10 12:04:30,533 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 6, 3))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,533 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,533 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:39459'. Reason: scheduler-close
2025-11-10 12:04:30,533 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 6, 9))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,533 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,533 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,533 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,533 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:38345'. Reason: scheduler-close
2025-11-10 12:04:30,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 128.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,534 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,534 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,527 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x148a1101c850>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,534 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:41919'. Reason: scheduler-close
2025-11-10 12:04:30,531 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14a4a523c150>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,534 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 6, 7))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,531 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x154784df0f90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,534 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 128.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,528 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1463eef36990>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,534 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,534 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:36459'. Reason: scheduler-close
2025-11-10 12:04:30,534 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,534 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,534 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,535 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,535 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,535 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,535 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,535 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,526 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15384fd9f790>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,535 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,535 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:45989. Reason: scheduler-close
2025-11-10 12:04:30,535 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,535 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,534 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,532 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48038 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48038 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,536 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:35491. Reason: scheduler-close
2025-11-10 12:04:30,536 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:44557. Reason: scheduler-close
2025-11-10 12:04:30,536 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 6, 19))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,536 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:43173. Reason: scheduler-close
2025-11-10 12:04:30,532 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14daed586e50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,531 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14d5f4fdb310>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,537 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,537 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 6, 4))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 128.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,537 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,537 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,537 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,537 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,537 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,537 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,537 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:46769. Reason: scheduler-close
2025-11-10 12:04:30,538 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,538 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,538 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:35055. Reason: scheduler-close
2025-11-10 12:04:30,538 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,534 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47970 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47970 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,538 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:40645. Reason: scheduler-close
2025-11-10 12:04:30,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,538 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:37613. Reason: scheduler-close
2025-11-10 12:04:30,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 128.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,539 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:46551. Reason: scheduler-close
2025-11-10 12:04:30,533 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1457f92a6810>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,536 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x148a50fe0a90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,537 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x151e22aa7d90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,541 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:35001. Reason: scheduler-close
2025-11-10 12:04:30,541 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:39831. Reason: scheduler-close
2025-11-10 12:04:30,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 128.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,541 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:45563. Reason: scheduler-close
2025-11-10 12:04:30,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,543 - distributed.core - INFO - Event loop was unresponsive in Worker for 128.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,543 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:44053. Reason: scheduler-close
2025-11-10 12:04:30,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,545 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:33267. Reason: scheduler-close
2025-11-10 12:04:30,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,545 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 7, 1))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,546 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,546 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:34559'. Reason: scheduler-close
2025-11-10 12:04:30,545 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:35725. Reason: scheduler-close
2025-11-10 12:04:30,546 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:44161'. Reason: scheduler-close
2025-11-10 12:04:30,542 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48002 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48002 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,546 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,547 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,546 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:43365. Reason: scheduler-close
2025-11-10 12:04:30,547 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,543 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47742 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47742 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,547 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,547 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:34021. Reason: scheduler-close
2025-11-10 12:04:30,543 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47908 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47908 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,548 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,548 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,548 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:46771. Reason: scheduler-close
2025-11-10 12:04:30,542 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48044 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48044 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,543 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47826 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47826 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,550 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:33367'. Reason: scheduler-close
2025-11-10 12:04:30,546 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47806 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47806 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,550 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:46495'. Reason: scheduler-close
2025-11-10 12:04:30,550 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:34715. Reason: scheduler-close
2025-11-10 12:04:30,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 128.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,550 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 4, 18))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,545 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48054 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48054 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,551 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:38181'. Reason: scheduler-close
2025-11-10 12:04:30,551 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,551 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,551 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,550 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:44875. Reason: scheduler-close
2025-11-10 12:04:30,551 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,551 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,551 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,551 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,551 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:41301. Reason: scheduler-close
2025-11-10 12:04:30,552 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:45361'. Reason: scheduler-close
2025-11-10 12:04:30,542 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47790 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47790 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,553 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:36699'. Reason: scheduler-close
2025-11-10 12:04:30,553 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:45405'. Reason: scheduler-close
2025-11-10 12:04:30,553 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 5, 17))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,553 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 3, 7))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,553 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,553 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,553 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,553 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:38935'. Reason: scheduler-close
2025-11-10 12:04:30,554 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,554 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,554 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,553 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:36573. Reason: scheduler-close
2025-11-10 12:04:30,554 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 4, 14))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,554 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,554 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 128.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,554 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 3, 5))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,554 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,554 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,554 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,554 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,555 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,555 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,552 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,555 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,555 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,555 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,539 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14806a9db8d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,555 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,555 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 4, 19))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,555 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:37223. Reason: scheduler-close
2025-11-10 12:04:30,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,556 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:35733. Reason: scheduler-close
2025-11-10 12:04:30,556 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:43767. Reason: scheduler-close
2025-11-10 12:04:30,538 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x153c33f651d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,553 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48094 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48094 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,556 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,556 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,557 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,557 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,557 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,557 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 5, 15))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,557 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,557 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,554 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47852 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47852 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,558 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,558 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,558 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,555 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1474c0418fd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,556 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1518ac585b10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,559 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:39693'. Reason: scheduler-close
2025-11-10 12:04:30,546 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47898 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47898 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,560 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:42111'. Reason: scheduler-close
2025-11-10 12:04:30,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,560 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:41203'. Reason: scheduler-close
2025-11-10 12:04:30,558 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14c3df2af590>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,561 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:33595'. Reason: scheduler-close
2025-11-10 12:04:30,557 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47804 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47804 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,561 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 5, 3))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,561 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 4, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,558 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,562 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,562 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,562 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,562 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,562 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,562 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 3, 16))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,562 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,557 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47938 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47938 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,562 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,562 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,562 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,562 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,562 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,563 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,563 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:46073'. Reason: scheduler-close
2025-11-10 12:04:30,563 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,563 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,563 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,564 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 3, 17))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,564 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:43451'. Reason: scheduler-close
2025-11-10 12:04:30,564 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,564 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,564 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,564 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,564 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,565 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 3, 1))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,565 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,566 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,566 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,566 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,566 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,564 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 128.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,562 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47886 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47886 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,565 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x148d509dba50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,566 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x152d7055f4d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,566 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1526fb9fe3d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,565 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47998 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47998 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,562 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47768 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47768 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,570 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:37219'. Reason: scheduler-close
2025-11-10 12:04:30,564 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x146873a0fbd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,567 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15056c0ccb90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,566 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14ca9edd2d50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,536 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14fe20c08350>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,571 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:44465'. Reason: scheduler-close
2025-11-10 12:04:30,571 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 3, 9))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,572 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 3, 11))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,572 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,566 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48074 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48074 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,572 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,572 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,572 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,572 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,567 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47808 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47808 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,572 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,572 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,572 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,572 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,572 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,568 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x149ce5376050>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,574 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,574 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,574 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:42679'. Reason: scheduler-close
2025-11-10 12:04:30,570 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47822 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47822 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,575 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:38315'. Reason: scheduler-close
2025-11-10 12:04:30,576 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 2, 4))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,576 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 3, 2))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,570 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48076 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48076 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,576 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,576 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,551 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47918 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47918 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,576 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,576 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,576 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:39769'. Reason: scheduler-close
2025-11-10 12:04:30,576 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,576 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,576 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,576 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,576 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,576 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,576 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 2, 17))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,577 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,577 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 5, 16))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,577 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,577 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,577 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,577 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,577 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,577 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,577 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,578 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,578 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,578 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 5, 7))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,578 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,573 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47966 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47966 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,577 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,578 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:42739'. Reason: scheduler-close
2025-11-10 12:04:30,578 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,578 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,579 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,579 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,579 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,579 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 2, 15))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,579 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:35571'. Reason: scheduler-close
2025-11-10 12:04:30,579 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,579 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 2, 2))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,579 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,580 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,580 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,580 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,577 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,580 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,580 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,580 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,580 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,580 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,580 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.31:34645. Reason: scheduler-close
2025-11-10 12:04:30,580 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:38881'. Reason: scheduler-close
2025-11-10 12:04:30,566 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48058 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48058 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,574 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47796 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47796 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,576 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48050 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48050 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,581 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 3, 12))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,578 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14610ef86910>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,582 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,582 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,582 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,582 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,582 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,582 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 3, 6))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,582 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,582 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,582 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:33161'. Reason: scheduler-close
2025-11-10 12:04:30,583 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,583 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,583 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,580 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14b07e15bdd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,548 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47888 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47888 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,583 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:40171'. Reason: scheduler-close
2025-11-10 12:04:30,580 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1503aea13010>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,584 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 2, 6))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,584 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,581 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x147138260a50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,584 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,584 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,584 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,585 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,585 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 3, 15))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,585 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,585 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,585 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,582 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15334188ff50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,585 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,585 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,584 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14a820026c50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,587 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,589 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,588 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:45963'. Reason: scheduler-close
2025-11-10 12:04:30,587 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14a59e0a9210>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,587 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,590 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 5, 6))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,590 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,591 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,591 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,591 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,591 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,591 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,592 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,559 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47840 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47840 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,594 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1504838ad350>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,594 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1539b22a1190>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 132.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-11-10 12:04:30,597 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,599 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,594 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x146662a28950>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,588 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48024 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:48024 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,603 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:35697'. Reason: scheduler-close
2025-11-10 12:04:30,603 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,603 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,604 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,604 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,562 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47786 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47786 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,582 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15490b84e690>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,605 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 4, 3))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,606 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,587 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x148885815e90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,606 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,606 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,607 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,607 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,606 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x146a321e4550>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,607 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,611 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,611 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,612 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,613 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,615 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,615 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,617 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,613 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1471b802df90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,616 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,618 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,618 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,619 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,618 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,619 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,615 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,620 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,619 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,621 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,621 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,622 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,621 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,623 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,623 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,623 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,623 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,624 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,626 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:35665'. Reason: scheduler-close
2025-11-10 12:04:30,627 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,627 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,627 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,627 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,627 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,627 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:38233'. Reason: scheduler-close
2025-11-10 12:04:30,628 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:44699'. Reason: scheduler-close
2025-11-10 12:04:30,628 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:46567'. Reason: scheduler-close
2025-11-10 12:04:30,624 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47872 remote=tcp://10.6.5.30:8767>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.5.31:47872 remote=tcp://10.6.5.30:8767>: Stream is closed
2025-11-10 12:04:30,629 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 2, 5))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,629 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 2, 3))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,629 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,629 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,629 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,629 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,629 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,630 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,629 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,630 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,630 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,630 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,630 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,630 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,631 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.31:38255'. Reason: scheduler-close
2025-11-10 12:04:30,629 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,631 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,632 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,632 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 2, 14))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,632 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('broadcast_to-mean_agg-aggregate-mean_chunk-concatenate-fcd94a954d0c0d670d14a33f73a5e389', 90, 3, 4))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-11-10 12:04:30,633 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,633 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,633 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-11-10 12:04:30,633 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,633 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,633 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,633 - distributed.worker - INFO - Removing Worker plugin qme_utils.py695dd508-a396-4c4f-b78d-f0898b4f0f0b
2025-11-10 12:04:30,633 - distributed.worker - INFO - Removing Worker plugin qme_vars.py66b5b055-57be-4fd2-9311-4496e433ca89
2025-11-10 12:04:30,633 - distributed.worker - INFO - Removing Worker plugin qme_train.py9798154c-4f36-4282-a8d3-c974e1b40a37
2025-11-10 12:04:30,633 - distributed.worker - INFO - Removing Worker plugin qme_apply.py7dfcb86e-4cb8-4db4-9920-e6ea00d09397
2025-11-10 12:04:30,631 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14f99a841bd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,633 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14b751a80bd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,634 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14b2d3787650>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,637 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,637 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,638 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,638 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,636 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x154d1154f910>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-11-10 12:04:30,640 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,641 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,643 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,643 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,644 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,645 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,645 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,648 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,650 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,650 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,652 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,652 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,652 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,653 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,654 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,654 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,654 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,654 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,655 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,655 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,662 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,661 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,662 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,663 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,662 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,661 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,663 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,663 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,664 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,665 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,665 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,666 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,666 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,667 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,666 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,667 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,671 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,672 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,673 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,675 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,680 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,682 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,681 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,682 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,697 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,698 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,699 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,700 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:30,700 - distributed.core - INFO - Received 'close-stream' from tcp://10.6.5.30:8767; closing.
2025-11-10 12:04:30,703 - distributed.nanny - INFO - Worker closed
2025-11-10 12:04:32,592 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-11-10 12:04:32,608 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-11-10 12:04:32,623 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-11-10 12:04:32,627 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-11-10 12:04:32,627 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-11-10 12:04:32,628 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-11-10 12:04:32,629 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-11-10 12:04:32,630 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-11-10 12:04:32,634 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-11-10 12:04:32,637 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-11-10 12:04:32,645 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-11-10 12:04:32,646 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-11-10 12:04:32,649 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-11-10 12:04:32,658 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-11-10 12:04:32,666 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-11-10 12:04:32,667 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-11-10 12:04:32,667 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-11-10 12:04:32,667 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-11-10 12:04:32,670 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-11-10 12:04:32,675 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-11-10 12:04:32,678 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-11-10 12:04:34,762 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:38037'. Reason: nanny-close-gracefully
2025-11-10 12:04:34,764 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:38037' closed.
2025-11-10 12:04:34,780 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:42029'. Reason: nanny-close-gracefully
2025-11-10 12:04:34,781 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:42029' closed.
2025-11-10 12:04:34,784 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:42825'. Reason: nanny-close-gracefully
2025-11-10 12:04:34,785 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:42825' closed.
2025-11-10 12:04:34,796 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:38181'. Reason: nanny-close-gracefully
2025-11-10 12:04:34,797 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:38181' closed.
2025-11-10 12:04:34,802 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:38935'. Reason: nanny-close-gracefully
2025-11-10 12:04:34,802 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:38935' closed.
2025-11-10 12:04:34,836 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:38025'. Reason: nanny-close-gracefully
2025-11-10 12:04:34,837 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:38025' closed.
2025-11-10 12:04:34,853 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:39601'. Reason: nanny-close-gracefully
2025-11-10 12:04:34,856 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:33367'. Reason: nanny-close-gracefully
2025-11-10 12:04:34,857 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:39601' closed.
2025-11-10 12:04:34,858 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:33367' closed.
2025-11-10 12:04:34,873 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:41919'. Reason: nanny-close-gracefully
2025-11-10 12:04:34,874 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:41919' closed.
2025-11-10 12:04:34,876 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:36057'. Reason: nanny-close-gracefully
2025-11-10 12:04:34,877 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:36057' closed.
2025-11-10 12:04:34,879 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:36699'. Reason: nanny-close-gracefully
2025-11-10 12:04:34,880 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:36699' closed.
2025-11-10 12:04:34,890 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:34559'. Reason: nanny-close-gracefully
2025-11-10 12:04:34,891 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:34559' closed.
2025-11-10 12:04:34,925 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:41203'. Reason: nanny-close-gracefully
2025-11-10 12:04:34,927 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:41203' closed.
2025-11-10 12:04:34,967 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:37617'. Reason: nanny-close-gracefully
2025-11-10 12:04:34,968 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:37617' closed.
2025-11-10 12:04:34,990 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:46495'. Reason: nanny-close-gracefully
2025-11-10 12:04:34,991 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:46495' closed.
2025-11-10 12:04:35,003 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:39027'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,007 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:39027' closed.
2025-11-10 12:04:35,017 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:43451'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,018 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:43451' closed.
2025-11-10 12:04:35,028 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:39693'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,032 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:38881'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,033 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:39693' closed.
2025-11-10 12:04:35,033 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:38255'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,036 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:38881' closed.
2025-11-10 12:04:35,039 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:38255' closed.
2025-11-10 12:04:35,044 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:46073'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,045 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:46073' closed.
2025-11-10 12:04:35,067 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:36459'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,068 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:36459' closed.
2025-11-10 12:04:35,083 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:44465'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,084 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:44465' closed.
2025-11-10 12:04:35,085 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:35665'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,086 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:35665' closed.
2025-11-10 12:04:35,091 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:35571'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,091 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:35571' closed.
2025-11-10 12:04:35,097 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:40171'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,099 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:40171' closed.
2025-11-10 12:04:35,099 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:42739'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,100 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:42739' closed.
2025-11-10 12:04:35,108 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:38613'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,109 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:38613' closed.
2025-11-10 12:04:35,113 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:44699'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,114 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:44699' closed.
2025-11-10 12:04:35,119 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:42679'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,120 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:42679' closed.
2025-11-10 12:04:35,131 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:44161'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,132 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:44161' closed.
2025-11-10 12:04:35,152 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:37219'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,154 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:37219' closed.
2025-11-10 12:04:35,212 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:33161'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,213 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:33161' closed.
2025-11-10 12:04:35,220 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:45963'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,220 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:45963' closed.
2025-11-10 12:04:35,222 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:45361'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,223 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:45361' closed.
2025-11-10 12:04:35,226 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:38315'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,227 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:38315' closed.
2025-11-10 12:04:35,252 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:45405'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,253 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:45405' closed.
2025-11-10 12:04:35,594 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:35697'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,595 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:35697' closed.
2025-11-10 12:04:35,684 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:35667'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,685 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:35667' closed.
2025-11-10 12:04:35,751 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:38233'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,752 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:38233' closed.
2025-11-10 12:04:35,811 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:41495'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,812 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:41495' closed.
2025-11-10 12:04:35,827 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:46567'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,828 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:46567' closed.
2025-11-10 12:04:35,855 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:38345'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,855 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:38345' closed.
2025-11-10 12:04:35,870 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:39459'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,870 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:39459' closed.
2025-11-10 12:04:35,890 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:42111'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,891 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:42111' closed.
2025-11-10 12:04:35,934 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:39769'. Reason: nanny-close-gracefully
2025-11-10 12:04:35,935 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:39769' closed.
2025-11-10 12:04:36,032 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:36589'. Reason: nanny-close-gracefully
2025-11-10 12:04:36,033 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:36589' closed.
2025-11-10 12:04:36,184 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.31:33595'. Reason: nanny-close-gracefully
2025-11-10 12:04:36,184 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.31:33595' closed.
2025-11-10 12:04:36,187 - distributed.dask_worker - INFO - End worker
