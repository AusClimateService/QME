Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 10:32:19,353 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.5:38563'
2025-09-03 10:32:19,363 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.5:33399'
2025-09-03 10:32:19,368 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.5:46801'
2025-09-03 10:32:19,373 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.5:43307'
2025-09-03 10:32:19,378 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.5:36787'
2025-09-03 10:32:19,383 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.5:46537'
2025-09-03 10:32:19,387 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.5:37645'
2025-09-03 10:32:19,392 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.5:33897'
2025-09-03 10:32:19,398 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.5:43995'
2025-09-03 10:32:19,403 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.5:42459'
2025-09-03 10:32:20,191 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.5:35887
2025-09-03 10:32:20,191 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.5:36663
2025-09-03 10:32:20,191 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.5:35651
2025-09-03 10:32:20,191 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.5:41195
2025-09-03 10:32:20,191 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.5:46765
2025-09-03 10:32:20,191 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.5:41853
2025-09-03 10:32:20,191 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.5:42581
2025-09-03 10:32:20,191 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.5:40459
2025-09-03 10:32:20,192 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.5:33185
2025-09-03 10:32:20,192 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.5:35887
2025-09-03 10:32:20,192 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.5:36663
2025-09-03 10:32:20,192 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.5:35651
2025-09-03 10:32:20,192 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.5:41195
2025-09-03 10:32:20,192 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.5:46765
2025-09-03 10:32:20,192 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.5:41853
2025-09-03 10:32:20,192 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.5:42581
2025-09-03 10:32:20,192 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.5:40459
2025-09-03 10:32:20,192 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.5:33185
2025-09-03 10:32:20,192 - distributed.worker - INFO -          dashboard at:           10.6.105.5:36331
2025-09-03 10:32:20,192 - distributed.worker - INFO -          dashboard at:           10.6.105.5:40981
2025-09-03 10:32:20,192 - distributed.worker - INFO -          dashboard at:           10.6.105.5:40661
2025-09-03 10:32:20,192 - distributed.worker - INFO -          dashboard at:           10.6.105.5:45529
2025-09-03 10:32:20,192 - distributed.worker - INFO -          dashboard at:           10.6.105.5:44031
2025-09-03 10:32:20,192 - distributed.worker - INFO -          dashboard at:           10.6.105.5:38791
2025-09-03 10:32:20,192 - distributed.worker - INFO -          dashboard at:           10.6.105.5:34317
2025-09-03 10:32:20,192 - distributed.worker - INFO -          dashboard at:           10.6.105.5:41335
2025-09-03 10:32:20,192 - distributed.worker - INFO -          dashboard at:           10.6.105.5:33293
2025-09-03 10:32:20,192 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:20,192 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:20,192 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:20,192 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:20,192 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:20,192 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:20,192 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:20,192 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:20,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:20,192 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:20,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:20,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:20,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:20,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:20,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:20,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:20,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:20,192 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:20,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:20,192 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:20,192 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:20,192 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:20,192 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:20,192 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:20,192 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:20,192 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:20,192 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:20,192 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:20,192 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:20,192 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:20,192 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:20,192 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-5lpnvgxp
2025-09-03 10:32:20,192 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:20,192 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:20,192 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:20,192 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:20,192 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:20,192 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-c6hahzc0
2025-09-03 10:32:20,192 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hnpor3nn
2025-09-03 10:32:20,192 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-iv376ufa
2025-09-03 10:32:20,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:20,192 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-k2dc9_s1
2025-09-03 10:32:20,192 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-626dbgvz
2025-09-03 10:32:20,192 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-lvhljopq
2025-09-03 10:32:20,192 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-5ug49sbn
2025-09-03 10:32:20,192 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hykmxern
2025-09-03 10:32:20,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:20,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:20,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:20,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:20,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:20,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:20,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:20,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:20,198 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.5:35221
2025-09-03 10:32:20,198 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.5:35221
2025-09-03 10:32:20,198 - distributed.worker - INFO -          dashboard at:           10.6.105.5:33855
2025-09-03 10:32:20,198 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:20,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:20,198 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:20,198 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:20,198 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-cbsq0zhn
2025-09-03 10:32:20,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:24,453 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:24,454 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:24,454 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:24,455 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:24,469 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:24,470 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:24,470 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:24,472 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:24,486 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:24,487 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:24,487 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:24,488 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:24,502 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:24,504 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:24,504 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:24,506 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:24,519 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:24,520 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:24,520 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:24,522 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:24,535 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:24,536 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:24,537 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:24,538 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:24,552 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:24,553 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:24,553 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:24,554 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:24,568 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:24,569 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:24,569 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:24,571 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:24,584 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:24,585 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:24,586 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:24,587 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:24,601 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:24,602 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:24,602 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:24,604 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:41,705 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:37903'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,706 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:37903' closed.
2025-09-03 10:32:41,706 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:33781'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,707 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:33781' closed.
2025-09-03 10:32:41,707 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:38241'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,708 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:38241' closed.
2025-09-03 10:32:41,708 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:35919'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,709 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:35919' closed.
2025-09-03 10:32:41,709 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:41899'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,710 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:41899' closed.
2025-09-03 10:32:41,710 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:39267'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,710 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:39267' closed.
2025-09-03 10:32:41,710 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:40771'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,711 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:40771' closed.
2025-09-03 10:32:41,711 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:44103'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,711 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:44103' closed.
2025-09-03 10:32:41,712 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:39035'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,712 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:39035' closed.
2025-09-03 10:32:41,712 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:34821'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,713 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:34821' closed.
2025-09-03 10:32:41,713 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:41501'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,713 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:41501' closed.
2025-09-03 10:32:41,714 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:34051'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,714 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:34051' closed.
2025-09-03 10:32:41,715 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:44869'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,715 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:44869' closed.
2025-09-03 10:32:41,715 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:34487'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,716 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:34487' closed.
2025-09-03 10:32:41,716 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:40621'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,716 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:40621' closed.
2025-09-03 10:32:41,716 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:41627'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,717 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:41627' closed.
2025-09-03 10:32:41,717 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:45485'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,717 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:45485' closed.
2025-09-03 10:32:41,717 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:37697'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,718 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:37697' closed.
2025-09-03 10:32:41,718 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:37673'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,718 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:37673' closed.
2025-09-03 10:32:41,719 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:36871'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,719 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:36871' closed.
2025-09-03 10:32:41,720 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:40679'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,720 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:40679' closed.
2025-09-03 10:32:41,720 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:42953'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,720 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:42953' closed.
2025-09-03 10:32:41,721 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:36393'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,721 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:36393' closed.
2025-09-03 10:32:41,721 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:45241'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,721 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:45241' closed.
2025-09-03 10:32:41,722 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:34845'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,722 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:34845' closed.
2025-09-03 10:32:41,722 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:37151'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,722 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:37151' closed.
2025-09-03 10:32:41,723 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.5:34473'. Reason: failure-to-start-<class 'OSError'>
2025-09-03 10:32:41,723 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.5:34473' closed.
2025-09-03 10:32:41,728 - distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 528, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/nanny.py", line 358, in start_unsafe
    comm = await self.rpc.connect(saddr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/bin/dask", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/click/core.py", line 1442, in __call__
    return self.main(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/click/core.py", line 1363, in main
    rv = self.invoke(ctx)
         ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/click/core.py", line 1830, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/click/core.py", line 1226, in invoke
    return ctx.invoke(self.callback, **ctx.params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/click/core.py", line 794, in invoke
    return callback(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/cli/dask_worker.py", line 452, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/compatibility.py", line 204, in asyncio_run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/cli/dask_worker.py", line 449, in run
    [task.result() for task in done]
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/cli/dask_worker.py", line 449, in <listcomp>
    [task.result() for task in done]
     ^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/cli/dask_worker.py", line 422, in wait_for_nannies_to_finish
    await asyncio.gather(*nannies)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 694, in _wrap_awaitable
    return (yield from awaitable.__await__())
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 536, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2025-09-03 10:32:41,739 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=377213 parent=377163 started daemon>
2025-09-03 10:32:41,739 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=377208 parent=377163 started daemon>
2025-09-03 10:32:41,740 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=377205 parent=377163 started daemon>
2025-09-03 10:32:41,741 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=377201 parent=377163 started daemon>
2025-09-03 10:32:41,741 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=377196 parent=377163 started daemon>
2025-09-03 10:32:41,741 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=377192 parent=377163 started daemon>
2025-09-03 10:32:41,741 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=377189 parent=377163 started daemon>
2025-09-03 10:32:41,742 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=377185 parent=377163 started daemon>
2025-09-03 10:32:41,742 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=377181 parent=377163 started daemon>
2025-09-03 10:32:41,742 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=377178 parent=377163 started daemon>
2025-09-03 10:32:41,768 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 377178 exit status was already read will report exitcode 255
2025-09-03 10:32:41,773 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 377189 exit status was already read will report exitcode 255
