Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 10:31:55,975 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:33961'
2025-09-03 10:31:55,986 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:38527'
2025-09-03 10:31:55,993 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:46701'
2025-09-03 10:31:55,997 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:38193'
2025-09-03 10:31:56,002 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:36699'
2025-09-03 10:31:56,007 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:41561'
2025-09-03 10:31:56,013 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:37525'
2025-09-03 10:31:56,017 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:41237'
2025-09-03 10:31:56,023 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:38585'
2025-09-03 10:31:56,029 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:39653'
2025-09-03 10:31:56,035 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:42207'
2025-09-03 10:31:56,039 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:43233'
2025-09-03 10:31:56,043 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:36581'
2025-09-03 10:31:56,048 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:39677'
2025-09-03 10:31:56,052 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:34365'
2025-09-03 10:31:56,056 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:43317'
2025-09-03 10:31:56,061 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:39591'
2025-09-03 10:31:56,065 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:33067'
2025-09-03 10:31:56,070 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:41003'
2025-09-03 10:31:56,074 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:41615'
2025-09-03 10:31:56,078 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:42151'
2025-09-03 10:31:56,083 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:37229'
2025-09-03 10:31:56,088 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:44477'
2025-09-03 10:31:56,092 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:46773'
2025-09-03 10:31:56,097 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:46335'
2025-09-03 10:31:56,128 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:34881'
2025-09-03 10:31:56,131 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:41803'
2025-09-03 10:31:56,251 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:36157'
2025-09-03 10:31:56,257 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:46027'
2025-09-03 10:31:56,261 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:33625'
2025-09-03 10:31:56,265 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:35135'
2025-09-03 10:31:56,270 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:39293'
2025-09-03 10:31:56,274 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:40721'
2025-09-03 10:31:56,279 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:46249'
2025-09-03 10:31:56,284 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:35993'
2025-09-03 10:31:56,289 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:44799'
2025-09-03 10:31:56,294 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:33593'
2025-09-03 10:31:56,298 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:43917'
2025-09-03 10:31:56,303 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:41549'
2025-09-03 10:31:56,306 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:42009'
2025-09-03 10:31:56,311 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:32777'
2025-09-03 10:31:56,316 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:33823'
2025-09-03 10:31:56,321 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:41393'
2025-09-03 10:31:56,326 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:41467'
2025-09-03 10:31:56,330 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:43709'
2025-09-03 10:31:56,335 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:38125'
2025-09-03 10:31:56,339 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:34657'
2025-09-03 10:31:56,344 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:40521'
2025-09-03 10:31:56,347 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:33359'
2025-09-03 10:31:56,353 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:38973'
2025-09-03 10:31:56,357 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:33177'
2025-09-03 10:31:56,361 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:41859'
2025-09-03 10:31:56,363 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:41189'
2025-09-03 10:31:56,367 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:40429'
2025-09-03 10:31:56,372 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:41437'
2025-09-03 10:31:56,375 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:35989'
2025-09-03 10:31:56,381 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:32967'
2025-09-03 10:31:56,385 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:42299'
2025-09-03 10:31:56,390 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:33909'
2025-09-03 10:31:56,395 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:41541'
2025-09-03 10:31:56,415 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:36311'
2025-09-03 10:31:56,420 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:33205'
2025-09-03 10:31:56,423 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:44545'
2025-09-03 10:31:56,427 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:44255'
2025-09-03 10:31:56,431 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:34819'
2025-09-03 10:31:56,436 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:33223'
2025-09-03 10:31:56,440 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:39567'
2025-09-03 10:31:56,445 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:33347'
2025-09-03 10:31:56,449 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:37077'
2025-09-03 10:31:56,455 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:44519'
2025-09-03 10:31:56,460 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:36191'
2025-09-03 10:31:56,466 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:34805'
2025-09-03 10:31:56,470 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:36457'
2025-09-03 10:31:56,477 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:46141'
2025-09-03 10:31:56,481 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:35103'
2025-09-03 10:31:56,486 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:45945'
2025-09-03 10:31:56,490 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:42975'
2025-09-03 10:31:56,497 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:34281'
2025-09-03 10:31:56,502 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:44729'
2025-09-03 10:31:56,507 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:43961'
2025-09-03 10:31:56,513 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:38155'
2025-09-03 10:31:56,517 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:37441'
2025-09-03 10:31:56,521 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:35309'
2025-09-03 10:31:56,526 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:43815'
2025-09-03 10:31:56,531 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:37299'
2025-09-03 10:31:56,535 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:42059'
2025-09-03 10:31:56,540 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:36575'
2025-09-03 10:31:56,544 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:40671'
2025-09-03 10:31:56,549 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:34319'
2025-09-03 10:31:56,553 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:38425'
2025-09-03 10:31:56,558 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:41075'
2025-09-03 10:31:56,563 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:33045'
2025-09-03 10:31:56,567 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:34927'
2025-09-03 10:31:56,572 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:45973'
2025-09-03 10:31:56,575 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:45673'
2025-09-03 10:31:56,581 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:33131'
2025-09-03 10:31:56,585 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:44461'
2025-09-03 10:31:56,591 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:38311'
2025-09-03 10:31:56,596 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:42025'
2025-09-03 10:31:56,709 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:40591'
2025-09-03 10:31:56,718 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:35261'
2025-09-03 10:31:56,723 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:39615'
2025-09-03 10:31:56,726 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:37349'
2025-09-03 10:31:56,731 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.29:33333'
2025-09-03 10:31:57,920 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:35503
2025-09-03 10:31:57,920 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:45019
2025-09-03 10:31:57,920 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:38399
2025-09-03 10:31:57,920 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:40551
2025-09-03 10:31:57,920 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:44919
2025-09-03 10:31:57,920 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:36633
2025-09-03 10:31:57,920 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:33843
2025-09-03 10:31:57,920 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:40523
2025-09-03 10:31:57,920 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:39183
2025-09-03 10:31:57,920 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:37851
2025-09-03 10:31:57,920 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:45199
2025-09-03 10:31:57,920 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:35503
2025-09-03 10:31:57,920 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:43341
2025-09-03 10:31:57,920 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:46091
2025-09-03 10:31:57,920 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:42787
2025-09-03 10:31:57,920 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:45019
2025-09-03 10:31:57,921 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:38399
2025-09-03 10:31:57,920 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:41687
2025-09-03 10:31:57,921 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:40551
2025-09-03 10:31:57,921 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:43615
2025-09-03 10:31:57,921 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:44919
2025-09-03 10:31:57,921 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:33831
2025-09-03 10:31:57,921 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:36633
2025-09-03 10:31:57,921 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:33843
2025-09-03 10:31:57,921 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:34953
2025-09-03 10:31:57,921 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:40523
2025-09-03 10:31:57,921 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:39183
2025-09-03 10:31:57,921 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:37851
2025-09-03 10:31:57,921 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:45199
2025-09-03 10:31:57,921 - distributed.worker - INFO -          dashboard at:          10.6.102.29:37039
2025-09-03 10:31:57,921 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:43341
2025-09-03 10:31:57,921 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:46091
2025-09-03 10:31:57,921 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:42787
2025-09-03 10:31:57,921 - distributed.worker - INFO -          dashboard at:          10.6.102.29:38319
2025-09-03 10:31:57,921 - distributed.worker - INFO -          dashboard at:          10.6.102.29:38909
2025-09-03 10:31:57,921 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:41687
2025-09-03 10:31:57,921 - distributed.worker - INFO -          dashboard at:          10.6.102.29:37031
2025-09-03 10:31:57,921 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:43615
2025-09-03 10:31:57,921 - distributed.worker - INFO -          dashboard at:          10.6.102.29:33479
2025-09-03 10:31:57,921 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:33831
2025-09-03 10:31:57,921 - distributed.worker - INFO -          dashboard at:          10.6.102.29:40047
2025-09-03 10:31:57,921 - distributed.worker - INFO -          dashboard at:          10.6.102.29:43587
2025-09-03 10:31:57,921 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:34953
2025-09-03 10:31:57,921 - distributed.worker - INFO -          dashboard at:          10.6.102.29:45377
2025-09-03 10:31:57,921 - distributed.worker - INFO -          dashboard at:          10.6.102.29:38469
2025-09-03 10:31:57,921 - distributed.worker - INFO -          dashboard at:          10.6.102.29:46275
2025-09-03 10:31:57,921 - distributed.worker - INFO -          dashboard at:          10.6.102.29:42135
2025-09-03 10:31:57,921 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,921 - distributed.worker - INFO -          dashboard at:          10.6.102.29:36873
2025-09-03 10:31:57,921 - distributed.worker - INFO -          dashboard at:          10.6.102.29:41983
2025-09-03 10:31:57,921 - distributed.worker - INFO -          dashboard at:          10.6.102.29:39417
2025-09-03 10:31:57,921 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,921 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,921 - distributed.worker - INFO -          dashboard at:          10.6.102.29:36277
2025-09-03 10:31:57,921 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,921 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,921 - distributed.worker - INFO -          dashboard at:          10.6.102.29:36809
2025-09-03 10:31:57,921 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,921 - distributed.worker - INFO -          dashboard at:          10.6.102.29:33003
2025-09-03 10:31:57,921 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,921 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,921 - distributed.worker - INFO -          dashboard at:          10.6.102.29:40201
2025-09-03 10:31:57,921 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,921 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,921 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,921 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,921 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,921 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,921 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,921 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,921 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,921 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,921 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,921 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,921 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,921 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,921 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,921 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,921 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,921 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,921 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,921 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,921 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,921 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,921 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,921 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,921 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,921 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,921 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,922 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,922 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,922 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,922 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,922 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,922 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,922 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,922 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,922 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4z9hnhq3
2025-09-03 10:31:57,922 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,922 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,922 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,922 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,922 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-uehinoks
2025-09-03 10:31:57,922 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,922 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,922 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,922 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3y_2offp
2025-09-03 10:31:57,922 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-lykslbq2
2025-09-03 10:31:57,922 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,922 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-17pqaaxv
2025-09-03 10:31:57,922 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,922 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-9j18xjwh
2025-09-03 10:31:57,922 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-za4f02ah
2025-09-03 10:31:57,922 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,922 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-yy8x2bm4
2025-09-03 10:31:57,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,922 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,922 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-suh0phuu
2025-09-03 10:31:57,922 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-31rqdwm9
2025-09-03 10:31:57,922 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-zbzijcpt
2025-09-03 10:31:57,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,922 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-05375c9y
2025-09-03 10:31:57,922 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:45009
2025-09-03 10:31:57,922 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ea6asb3p
2025-09-03 10:31:57,922 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-vkb0q4vp
2025-09-03 10:31:57,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,922 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-q6__iao1
2025-09-03 10:31:57,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,922 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2zzqnxez
2025-09-03 10:31:57,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,922 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-aaaualhm
2025-09-03 10:31:57,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,922 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-vh2dwzr8
2025-09-03 10:31:57,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,922 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:45009
2025-09-03 10:31:57,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,922 - distributed.worker - INFO -          dashboard at:          10.6.102.29:35415
2025-09-03 10:31:57,922 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,922 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,922 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,922 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-mawnubzn
2025-09-03 10:31:57,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,957 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:42539
2025-09-03 10:31:57,957 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:42539
2025-09-03 10:31:57,957 - distributed.worker - INFO -          dashboard at:          10.6.102.29:40011
2025-09-03 10:31:57,957 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,958 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,958 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,958 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,958 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7fq1xak2
2025-09-03 10:31:57,958 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,967 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:37703
2025-09-03 10:31:57,967 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:37703
2025-09-03 10:31:57,967 - distributed.worker - INFO -          dashboard at:          10.6.102.29:34627
2025-09-03 10:31:57,967 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,967 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,967 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,967 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,967 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-xzcnqjtr
2025-09-03 10:31:57,967 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,988 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:46671
2025-09-03 10:31:57,988 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:46671
2025-09-03 10:31:57,988 - distributed.worker - INFO -          dashboard at:          10.6.102.29:42057
2025-09-03 10:31:57,988 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,988 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,988 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,988 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,988 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-s2kyilgm
2025-09-03 10:31:57,988 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,004 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:46695
2025-09-03 10:31:58,004 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:46695
2025-09-03 10:31:58,004 - distributed.worker - INFO -          dashboard at:          10.6.102.29:39753
2025-09-03 10:31:58,005 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,005 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,005 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,005 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,005 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-sxeg6cso
2025-09-03 10:31:58,005 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,011 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:32891
2025-09-03 10:31:58,011 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:32891
2025-09-03 10:31:58,011 - distributed.worker - INFO -          dashboard at:          10.6.102.29:39355
2025-09-03 10:31:58,011 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,011 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,011 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,011 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,011 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-h9mp8o78
2025-09-03 10:31:58,011 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,031 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:40225
2025-09-03 10:31:58,031 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:40225
2025-09-03 10:31:58,031 - distributed.worker - INFO -          dashboard at:          10.6.102.29:34877
2025-09-03 10:31:58,031 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,031 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,031 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,031 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,031 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ohddy6mv
2025-09-03 10:31:58,031 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,110 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:43545
2025-09-03 10:31:58,111 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:43545
2025-09-03 10:31:58,111 - distributed.worker - INFO -          dashboard at:          10.6.102.29:42827
2025-09-03 10:31:58,111 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,111 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,111 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,111 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,111 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-b3y9lqpm
2025-09-03 10:31:58,111 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,136 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:40869
2025-09-03 10:31:58,136 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:40869
2025-09-03 10:31:58,136 - distributed.worker - INFO -          dashboard at:          10.6.102.29:42285
2025-09-03 10:31:58,136 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,136 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,136 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,136 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,136 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-08uhfenc
2025-09-03 10:31:58,136 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,327 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:40627
2025-09-03 10:31:58,327 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:40627
2025-09-03 10:31:58,327 - distributed.worker - INFO -          dashboard at:          10.6.102.29:36413
2025-09-03 10:31:58,327 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,327 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,327 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,327 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,327 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hpkhkrfc
2025-09-03 10:31:58,327 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,399 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:41999
2025-09-03 10:31:58,399 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:41999
2025-09-03 10:31:58,399 - distributed.worker - INFO -          dashboard at:          10.6.102.29:42659
2025-09-03 10:31:58,400 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,400 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,400 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,400 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,400 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-s7uhz11e
2025-09-03 10:31:58,400 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,405 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:37757
2025-09-03 10:31:58,405 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:37757
2025-09-03 10:31:58,405 - distributed.worker - INFO -          dashboard at:          10.6.102.29:33697
2025-09-03 10:31:58,405 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,405 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,405 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,405 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,405 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-g6xdw3gs
2025-09-03 10:31:58,405 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,421 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:41849
2025-09-03 10:31:58,421 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:41849
2025-09-03 10:31:58,421 - distributed.worker - INFO -          dashboard at:          10.6.102.29:40581
2025-09-03 10:31:58,421 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,421 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,421 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,421 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,421 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-mfyis0js
2025-09-03 10:31:58,422 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,447 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:41481
2025-09-03 10:31:58,447 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:41481
2025-09-03 10:31:58,447 - distributed.worker - INFO -          dashboard at:          10.6.102.29:37335
2025-09-03 10:31:58,447 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,447 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,447 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,447 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,447 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6b9eh8hn
2025-09-03 10:31:58,447 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,471 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:46747
2025-09-03 10:31:58,471 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:46747
2025-09-03 10:31:58,471 - distributed.worker - INFO -          dashboard at:          10.6.102.29:40577
2025-09-03 10:31:58,471 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,471 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,471 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,471 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,471 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-aju4kway
2025-09-03 10:31:58,471 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,479 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:39539
2025-09-03 10:31:58,479 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:39539
2025-09-03 10:31:58,479 - distributed.worker - INFO -          dashboard at:          10.6.102.29:36503
2025-09-03 10:31:58,479 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,479 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,479 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,479 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,479 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2495peuo
2025-09-03 10:31:58,479 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,518 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:43269
2025-09-03 10:31:58,518 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:43269
2025-09-03 10:31:58,518 - distributed.worker - INFO -          dashboard at:          10.6.102.29:39235
2025-09-03 10:31:58,518 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,518 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,518 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,518 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,518 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3oq2wc2e
2025-09-03 10:31:58,518 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,636 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:45825
2025-09-03 10:31:58,636 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:45825
2025-09-03 10:31:58,636 - distributed.worker - INFO -          dashboard at:          10.6.102.29:43519
2025-09-03 10:31:58,636 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,636 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,636 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,636 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,636 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3qxr5e9x
2025-09-03 10:31:58,636 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,792 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:46231
2025-09-03 10:31:58,792 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:46231
2025-09-03 10:31:58,792 - distributed.worker - INFO -          dashboard at:          10.6.102.29:46267
2025-09-03 10:31:58,792 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,792 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,792 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,792 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,792 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4vf512jd
2025-09-03 10:31:58,792 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,797 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:39755
2025-09-03 10:31:58,797 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:39755
2025-09-03 10:31:58,797 - distributed.worker - INFO -          dashboard at:          10.6.102.29:37389
2025-09-03 10:31:58,797 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,797 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,797 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,797 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,797 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-qtihy6xf
2025-09-03 10:31:58,797 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,817 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:43171
2025-09-03 10:31:58,818 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:43171
2025-09-03 10:31:58,818 - distributed.worker - INFO -          dashboard at:          10.6.102.29:44463
2025-09-03 10:31:58,818 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,818 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,818 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,818 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,818 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-dro01xux
2025-09-03 10:31:58,818 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,919 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:35235
2025-09-03 10:31:58,919 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:35235
2025-09-03 10:31:58,919 - distributed.worker - INFO -          dashboard at:          10.6.102.29:41921
2025-09-03 10:31:58,919 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,919 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,919 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,919 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,919 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3n785tsy
2025-09-03 10:31:58,919 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,015 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:44133
2025-09-03 10:31:59,015 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:44133
2025-09-03 10:31:59,015 - distributed.worker - INFO -          dashboard at:          10.6.102.29:34367
2025-09-03 10:31:59,015 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,015 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,015 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,015 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,015 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-8w7n0ip8
2025-09-03 10:31:59,015 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,035 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:39707
2025-09-03 10:31:59,035 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:39707
2025-09-03 10:31:59,035 - distributed.worker - INFO -          dashboard at:          10.6.102.29:34501
2025-09-03 10:31:59,035 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,035 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,035 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,036 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,036 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3p8nkjxy
2025-09-03 10:31:59,036 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,052 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:46725
2025-09-03 10:31:59,052 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:46725
2025-09-03 10:31:59,052 - distributed.worker - INFO -          dashboard at:          10.6.102.29:45753
2025-09-03 10:31:59,052 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,052 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,052 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,052 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,053 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3al4s0im
2025-09-03 10:31:59,053 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,071 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:37249
2025-09-03 10:31:59,071 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:37249
2025-09-03 10:31:59,071 - distributed.worker - INFO -          dashboard at:          10.6.102.29:35453
2025-09-03 10:31:59,071 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,071 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,071 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,071 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,071 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hmuj02p3
2025-09-03 10:31:59,071 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,107 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:45879
2025-09-03 10:31:59,107 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:45879
2025-09-03 10:31:59,107 - distributed.worker - INFO -          dashboard at:          10.6.102.29:45671
2025-09-03 10:31:59,107 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,107 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,107 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,107 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,107 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2qvvyrmj
2025-09-03 10:31:59,107 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,160 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:39269
2025-09-03 10:31:59,161 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:39269
2025-09-03 10:31:59,161 - distributed.worker - INFO -          dashboard at:          10.6.102.29:44053
2025-09-03 10:31:59,161 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,161 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,161 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,161 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,161 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-tx86j_3i
2025-09-03 10:31:59,161 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,163 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:38515
2025-09-03 10:31:59,163 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:38515
2025-09-03 10:31:59,163 - distributed.worker - INFO -          dashboard at:          10.6.102.29:44689
2025-09-03 10:31:59,163 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,163 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,163 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,163 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,163 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6ag1p45g
2025-09-03 10:31:59,163 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,210 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:46311
2025-09-03 10:31:59,210 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:46311
2025-09-03 10:31:59,210 - distributed.worker - INFO -          dashboard at:          10.6.102.29:46273
2025-09-03 10:31:59,210 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,210 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,210 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,210 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,210 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-57ouc17v
2025-09-03 10:31:59,210 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,224 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:40629
2025-09-03 10:31:59,224 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:40629
2025-09-03 10:31:59,224 - distributed.worker - INFO -          dashboard at:          10.6.102.29:33007
2025-09-03 10:31:59,224 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,224 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,224 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,224 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,224 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-bvdg1sn6
2025-09-03 10:31:59,224 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,226 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:44485
2025-09-03 10:31:59,226 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:44485
2025-09-03 10:31:59,227 - distributed.worker - INFO -          dashboard at:          10.6.102.29:43439
2025-09-03 10:31:59,227 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,227 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,227 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,227 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,227 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7ttlu6pr
2025-09-03 10:31:59,227 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,229 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:41747
2025-09-03 10:31:59,229 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:41747
2025-09-03 10:31:59,229 - distributed.worker - INFO -          dashboard at:          10.6.102.29:45993
2025-09-03 10:31:59,229 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,229 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,229 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,229 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,229 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-pqa42dii
2025-09-03 10:31:59,229 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,240 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:46657
2025-09-03 10:31:59,241 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:46657
2025-09-03 10:31:59,241 - distributed.worker - INFO -          dashboard at:          10.6.102.29:37683
2025-09-03 10:31:59,241 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,241 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,241 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,241 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-pixo3fwl
2025-09-03 10:31:59,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,246 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:59,248 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,248 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,250 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:59,299 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:45389
2025-09-03 10:31:59,299 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:45389
2025-09-03 10:31:59,299 - distributed.worker - INFO -          dashboard at:          10.6.102.29:35845
2025-09-03 10:31:59,299 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,299 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,299 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,299 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,299 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-21xmsi2t
2025-09-03 10:31:59,299 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,314 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:34419
2025-09-03 10:31:59,314 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:34419
2025-09-03 10:31:59,314 - distributed.worker - INFO -          dashboard at:          10.6.102.29:40859
2025-09-03 10:31:59,314 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,314 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,314 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,314 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,314 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-f9dtn1v3
2025-09-03 10:31:59,314 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,317 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:40709
2025-09-03 10:31:59,317 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:40709
2025-09-03 10:31:59,317 - distributed.worker - INFO -          dashboard at:          10.6.102.29:41719
2025-09-03 10:31:59,317 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,317 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,317 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,317 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,317 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ys225zzb
2025-09-03 10:31:59,317 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,322 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:39203
2025-09-03 10:31:59,322 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:39203
2025-09-03 10:31:59,322 - distributed.worker - INFO -          dashboard at:          10.6.102.29:44327
2025-09-03 10:31:59,322 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,322 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,322 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,322 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,322 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3f0wn6b1
2025-09-03 10:31:59,322 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,339 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:43387
2025-09-03 10:31:59,339 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:43387
2025-09-03 10:31:59,339 - distributed.worker - INFO -          dashboard at:          10.6.102.29:46661
2025-09-03 10:31:59,339 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,339 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,339 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,339 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,339 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-spepdkru
2025-09-03 10:31:59,339 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,342 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:43205
2025-09-03 10:31:59,342 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:43205
2025-09-03 10:31:59,343 - distributed.worker - INFO -          dashboard at:          10.6.102.29:36875
2025-09-03 10:31:59,343 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,343 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,343 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,343 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,343 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-n751pxjp
2025-09-03 10:31:59,343 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,351 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:39249
2025-09-03 10:31:59,351 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:39249
2025-09-03 10:31:59,351 - distributed.worker - INFO -          dashboard at:          10.6.102.29:40625
2025-09-03 10:31:59,351 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,351 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,351 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,351 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,351 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ewqae02l
2025-09-03 10:31:59,351 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,516 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:40301
2025-09-03 10:31:59,516 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:40301
2025-09-03 10:31:59,516 - distributed.worker - INFO -          dashboard at:          10.6.102.29:36099
2025-09-03 10:31:59,516 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,516 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,516 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,516 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,516 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_5lfm2r6
2025-09-03 10:31:59,516 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,584 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:37907
2025-09-03 10:31:59,584 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:37907
2025-09-03 10:31:59,584 - distributed.worker - INFO -          dashboard at:          10.6.102.29:38589
2025-09-03 10:31:59,584 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,584 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,584 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,584 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,584 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-w2br5xoq
2025-09-03 10:31:59,584 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:46871
2025-09-03 10:31:59,584 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,584 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:46871
2025-09-03 10:31:59,584 - distributed.worker - INFO -          dashboard at:          10.6.102.29:45507
2025-09-03 10:31:59,584 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,584 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,585 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,585 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,585 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-naf0h03s
2025-09-03 10:31:59,585 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,623 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:45549
2025-09-03 10:31:59,623 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:45549
2025-09-03 10:31:59,623 - distributed.worker - INFO -          dashboard at:          10.6.102.29:46743
2025-09-03 10:31:59,623 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,623 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,623 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,623 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,623 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-cmnu_1ec
2025-09-03 10:31:59,623 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,629 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:37289
2025-09-03 10:31:59,629 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:37289
2025-09-03 10:31:59,629 - distributed.worker - INFO -          dashboard at:          10.6.102.29:44525
2025-09-03 10:31:59,629 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,629 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,629 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,629 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,629 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7nwt0ydt
2025-09-03 10:31:59,629 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,638 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:34875
2025-09-03 10:31:59,638 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:34875
2025-09-03 10:31:59,638 - distributed.worker - INFO -          dashboard at:          10.6.102.29:42041
2025-09-03 10:31:59,638 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,638 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,638 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,638 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,638 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-t0x81ufm
2025-09-03 10:31:59,638 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,652 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:45341
2025-09-03 10:31:59,653 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:45341
2025-09-03 10:31:59,653 - distributed.worker - INFO -          dashboard at:          10.6.102.29:39937
2025-09-03 10:31:59,653 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,653 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,653 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,653 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,653 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-17346f33
2025-09-03 10:31:59,653 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,688 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:45169
2025-09-03 10:31:59,688 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:45169
2025-09-03 10:31:59,688 - distributed.worker - INFO -          dashboard at:          10.6.102.29:45089
2025-09-03 10:31:59,688 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,688 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,688 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,688 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,688 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_z4vinob
2025-09-03 10:31:59,688 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,727 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:36213
2025-09-03 10:31:59,727 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:36213
2025-09-03 10:31:59,727 - distributed.worker - INFO -          dashboard at:          10.6.102.29:37313
2025-09-03 10:31:59,727 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,727 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,727 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,727 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,727 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-tu3kuwe_
2025-09-03 10:31:59,727 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,728 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:45913
2025-09-03 10:31:59,728 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:45913
2025-09-03 10:31:59,728 - distributed.worker - INFO -          dashboard at:          10.6.102.29:39617
2025-09-03 10:31:59,728 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,728 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,728 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,728 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,728 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-y9tpi3p8
2025-09-03 10:31:59,728 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,731 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:39519
2025-09-03 10:31:59,731 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:39519
2025-09-03 10:31:59,731 - distributed.worker - INFO -          dashboard at:          10.6.102.29:44315
2025-09-03 10:31:59,732 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,732 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,732 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,732 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,732 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-gbkej1rk
2025-09-03 10:31:59,732 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,736 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:35051
2025-09-03 10:31:59,736 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:35051
2025-09-03 10:31:59,736 - distributed.worker - INFO -          dashboard at:          10.6.102.29:38069
2025-09-03 10:31:59,736 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,736 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,736 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,736 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,736 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-eev2wfq5
2025-09-03 10:31:59,736 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,737 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:42727
2025-09-03 10:31:59,737 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:42727
2025-09-03 10:31:59,737 - distributed.worker - INFO -          dashboard at:          10.6.102.29:34333
2025-09-03 10:31:59,737 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,737 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,737 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,737 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,737 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-aqlmf4dv
2025-09-03 10:31:59,737 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,739 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:34747
2025-09-03 10:31:59,739 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:34747
2025-09-03 10:31:59,739 - distributed.worker - INFO -          dashboard at:          10.6.102.29:35089
2025-09-03 10:31:59,740 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,740 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,740 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,740 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,740 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ut99i0si
2025-09-03 10:31:59,740 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,745 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:36167
2025-09-03 10:31:59,746 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:36167
2025-09-03 10:31:59,746 - distributed.worker - INFO -          dashboard at:          10.6.102.29:34863
2025-09-03 10:31:59,746 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,746 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,746 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,746 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,746 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-plutqgjj
2025-09-03 10:31:59,746 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,760 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:41051
2025-09-03 10:31:59,760 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:41051
2025-09-03 10:31:59,760 - distributed.worker - INFO -          dashboard at:          10.6.102.29:33497
2025-09-03 10:31:59,760 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,760 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,760 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,760 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,760 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-soa41noq
2025-09-03 10:31:59,760 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,763 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:45855
2025-09-03 10:31:59,763 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:45855
2025-09-03 10:31:59,763 - distributed.worker - INFO -          dashboard at:          10.6.102.29:37899
2025-09-03 10:31:59,763 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,763 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,763 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,763 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,763 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7lcpap_a
2025-09-03 10:31:59,763 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,766 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:33517
2025-09-03 10:31:59,767 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:33517
2025-09-03 10:31:59,767 - distributed.worker - INFO -          dashboard at:          10.6.102.29:38987
2025-09-03 10:31:59,767 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,767 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,767 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,767 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,767 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7oqgo7_k
2025-09-03 10:31:59,767 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,786 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:59,787 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,787 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,789 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:59,827 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:43467
2025-09-03 10:31:59,827 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:43467
2025-09-03 10:31:59,827 - distributed.worker - INFO -          dashboard at:          10.6.102.29:34541
2025-09-03 10:31:59,827 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,827 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,827 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,827 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,827 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-15dqkqbe
2025-09-03 10:31:59,827 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,837 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:40773
2025-09-03 10:31:59,837 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:40773
2025-09-03 10:31:59,837 - distributed.worker - INFO -          dashboard at:          10.6.102.29:39133
2025-09-03 10:31:59,837 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,837 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,837 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,837 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,837 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-m5lhovpn
2025-09-03 10:31:59,837 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,867 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:44755
2025-09-03 10:31:59,867 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:44755
2025-09-03 10:31:59,867 - distributed.worker - INFO -          dashboard at:          10.6.102.29:39921
2025-09-03 10:31:59,867 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,867 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,867 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,867 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,867 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ppvvc3ie
2025-09-03 10:31:59,867 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,995 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:38739
2025-09-03 10:31:59,995 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:38739
2025-09-03 10:31:59,995 - distributed.worker - INFO -          dashboard at:          10.6.102.29:35497
2025-09-03 10:31:59,995 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,995 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,995 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,995 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,995 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-k5t2s497
2025-09-03 10:31:59,995 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,997 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:40501
2025-09-03 10:31:59,997 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:40501
2025-09-03 10:31:59,997 - distributed.worker - INFO -          dashboard at:          10.6.102.29:43143
2025-09-03 10:31:59,997 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,997 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,997 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,997 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,997 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-pj4oczt0
2025-09-03 10:31:59,997 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,011 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:34137
2025-09-03 10:32:00,011 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:34137
2025-09-03 10:32:00,011 - distributed.worker - INFO -          dashboard at:          10.6.102.29:41935
2025-09-03 10:32:00,012 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,012 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,012 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,012 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,012 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ol6avx1r
2025-09-03 10:32:00,012 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,019 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:45833
2025-09-03 10:32:00,019 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:45833
2025-09-03 10:32:00,019 - distributed.worker - INFO -          dashboard at:          10.6.102.29:35555
2025-09-03 10:32:00,019 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,019 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,019 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,019 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,019 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-8gr5ufu1
2025-09-03 10:32:00,019 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,027 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:37639
2025-09-03 10:32:00,027 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:37639
2025-09-03 10:32:00,027 - distributed.worker - INFO -          dashboard at:          10.6.102.29:40735
2025-09-03 10:32:00,027 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,027 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,027 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,027 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,028 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7s07l3py
2025-09-03 10:32:00,028 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,097 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,098 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,098 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,100 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:00,222 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:43487
2025-09-03 10:32:00,223 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:43487
2025-09-03 10:32:00,223 - distributed.worker - INFO -          dashboard at:          10.6.102.29:36101
2025-09-03 10:32:00,223 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,223 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,223 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,223 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,223 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-bbam9g05
2025-09-03 10:32:00,223 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,223 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:42723
2025-09-03 10:32:00,223 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:42723
2025-09-03 10:32:00,223 - distributed.worker - INFO -          dashboard at:          10.6.102.29:40933
2025-09-03 10:32:00,223 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,223 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,223 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,223 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,223 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-u0k6k0dk
2025-09-03 10:32:00,223 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,231 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:34369
2025-09-03 10:32:00,232 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:34369
2025-09-03 10:32:00,232 - distributed.worker - INFO -          dashboard at:          10.6.102.29:43339
2025-09-03 10:32:00,232 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,232 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,232 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,232 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,232 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-oztvg2nw
2025-09-03 10:32:00,232 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,234 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:35163
2025-09-03 10:32:00,234 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:35163
2025-09-03 10:32:00,234 - distributed.worker - INFO -          dashboard at:          10.6.102.29:34763
2025-09-03 10:32:00,234 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,234 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,234 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,234 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,234 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-i8pbv7m_
2025-09-03 10:32:00,234 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,234 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:32859
2025-09-03 10:32:00,234 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:32859
2025-09-03 10:32:00,234 - distributed.worker - INFO -          dashboard at:          10.6.102.29:37425
2025-09-03 10:32:00,234 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,234 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,234 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,234 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,234 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-mbmp6s4a
2025-09-03 10:32:00,235 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:33493
2025-09-03 10:32:00,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:33493
2025-09-03 10:32:00,236 - distributed.worker - INFO -          dashboard at:          10.6.102.29:33705
2025-09-03 10:32:00,236 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,236 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,236 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,236 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,236 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3hypfyml
2025-09-03 10:32:00,236 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,244 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:36557
2025-09-03 10:32:00,244 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:36557
2025-09-03 10:32:00,244 - distributed.worker - INFO -          dashboard at:          10.6.102.29:37709
2025-09-03 10:32:00,244 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,244 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,244 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,244 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,244 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-xmt9072d
2025-09-03 10:32:00,244 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,248 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:35737
2025-09-03 10:32:00,248 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:35737
2025-09-03 10:32:00,248 - distributed.worker - INFO -          dashboard at:          10.6.102.29:43003
2025-09-03 10:32:00,248 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,248 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,248 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,248 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,248 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-zb7i0wer
2025-09-03 10:32:00,248 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,249 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:42801
2025-09-03 10:32:00,249 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:42801
2025-09-03 10:32:00,249 - distributed.worker - INFO -          dashboard at:          10.6.102.29:35587
2025-09-03 10:32:00,249 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,249 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,249 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,249 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,249 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-j9z4q4zk
2025-09-03 10:32:00,249 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,252 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:45249
2025-09-03 10:32:00,252 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:45249
2025-09-03 10:32:00,252 - distributed.worker - INFO -          dashboard at:          10.6.102.29:38411
2025-09-03 10:32:00,252 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,252 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,252 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,252 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,252 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-8awahjma
2025-09-03 10:32:00,252 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,253 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:38591
2025-09-03 10:32:00,254 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:38591
2025-09-03 10:32:00,254 - distributed.worker - INFO -          dashboard at:          10.6.102.29:34705
2025-09-03 10:32:00,254 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,254 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,254 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,254 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,254 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-wlmcck_2
2025-09-03 10:32:00,254 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,260 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:46019
2025-09-03 10:32:00,260 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:46019
2025-09-03 10:32:00,260 - distributed.worker - INFO -          dashboard at:          10.6.102.29:32799
2025-09-03 10:32:00,260 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,260 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,260 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,260 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,260 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ztfwj3b1
2025-09-03 10:32:00,260 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,261 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:39451
2025-09-03 10:32:00,261 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:39451
2025-09-03 10:32:00,261 - distributed.worker - INFO -          dashboard at:          10.6.102.29:46759
2025-09-03 10:32:00,261 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,261 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,261 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,261 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,261 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-lrrwnf8e
2025-09-03 10:32:00,261 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,262 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:40491
2025-09-03 10:32:00,262 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:40491
2025-09-03 10:32:00,262 - distributed.worker - INFO -          dashboard at:          10.6.102.29:46379
2025-09-03 10:32:00,262 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,262 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,262 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,262 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,262 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-f26mocrq
2025-09-03 10:32:00,262 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,264 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:40137
2025-09-03 10:32:00,264 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:40137
2025-09-03 10:32:00,264 - distributed.worker - INFO -          dashboard at:          10.6.102.29:37033
2025-09-03 10:32:00,264 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,264 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,264 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,264 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,264 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-czj6yesf
2025-09-03 10:32:00,264 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,267 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:36427
2025-09-03 10:32:00,267 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:36427
2025-09-03 10:32:00,267 - distributed.worker - INFO -          dashboard at:          10.6.102.29:45787
2025-09-03 10:32:00,267 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,267 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,267 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,267 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,267 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7c7ihuoe
2025-09-03 10:32:00,268 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,272 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:40749
2025-09-03 10:32:00,272 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:40749
2025-09-03 10:32:00,272 - distributed.worker - INFO -          dashboard at:          10.6.102.29:37613
2025-09-03 10:32:00,272 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,272 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,272 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,272 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,272 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-kf3ed_9m
2025-09-03 10:32:00,272 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,274 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:42729
2025-09-03 10:32:00,275 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:42729
2025-09-03 10:32:00,275 - distributed.worker - INFO -          dashboard at:          10.6.102.29:37449
2025-09-03 10:32:00,275 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,275 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,275 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,275 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,275 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-0_pdhy2f
2025-09-03 10:32:00,275 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,278 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.29:39051
2025-09-03 10:32:00,278 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.29:39051
2025-09-03 10:32:00,278 - distributed.worker - INFO -          dashboard at:          10.6.102.29:36513
2025-09-03 10:32:00,278 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,278 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,279 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,279 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,279 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-es818z01
2025-09-03 10:32:00,279 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,525 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,526 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,528 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:00,674 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,675 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,675 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,677 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:00,688 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,689 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,689 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,691 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:00,703 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,704 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,705 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,706 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:00,718 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,719 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,719 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,720 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:00,733 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,734 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,734 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,736 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,019 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,021 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,021 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,023 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,082 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,084 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,084 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,086 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,097 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,099 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,099 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,101 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,112 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,113 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,113 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,115 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,126 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,127 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,127 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,129 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,142 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,144 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,144 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,146 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,157 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,158 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,159 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,161 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,172 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,173 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,174 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,176 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,187 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,188 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,188 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,190 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,201 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,202 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,203 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,204 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,216 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,217 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,218 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,219 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,231 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,232 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,233 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,234 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,246 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,247 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,248 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,250 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,261 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,263 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,263 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,265 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,276 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,277 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,277 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,279 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,291 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,292 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,293 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,295 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,306 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,307 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,307 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,309 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,321 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,322 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,322 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,324 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,336 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,337 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,337 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,339 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,351 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,352 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,352 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,354 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,366 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,367 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,367 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,369 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,382 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,383 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,383 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,385 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,396 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,397 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,397 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,399 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,412 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,415 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,415 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,419 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,427 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,428 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,428 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,430 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,441 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,443 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,443 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,445 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,461 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,466 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,466 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,472 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:03,178 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:03,179 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:03,179 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:03,181 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:03,193 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:03,195 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:03,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:03,197 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:03,209 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:03,210 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:03,211 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:03,212 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:03,224 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:03,226 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:03,226 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:03,228 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,213 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,214 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,214 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,216 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,228 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,229 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,229 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,231 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,243 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,244 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,244 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,246 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,258 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,260 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,260 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,262 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,273 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,274 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,275 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,277 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,288 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,289 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,289 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,291 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,304 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,305 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,305 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,307 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,365 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,366 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,366 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,369 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,425 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,426 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,426 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,439 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,441 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,442 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,443 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,445 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,456 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,458 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,458 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,460 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,471 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,473 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,473 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,475 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:05,613 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:05,615 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:05,615 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:05,616 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:05,660 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:05,662 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:05,662 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:05,663 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:07,787 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:07,788 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:07,788 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:07,790 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:07,833 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:07,834 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:07,835 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:07,836 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:07,852 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:07,853 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:07,853 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:07,855 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:08,575 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:08,576 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:08,577 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:08,578 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:08,591 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:08,593 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:08,593 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:08,595 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:08,606 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:08,608 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:08,608 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:08,610 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:08,622 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:08,623 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:08,624 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:08,625 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:08,638 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:08,639 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:08,640 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:08,641 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:08,895 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:08,896 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:08,896 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:08,898 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,467 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,469 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,469 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,471 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,483 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,484 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,484 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,486 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,499 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,500 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,501 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,502 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:21,750 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:21,751 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:21,751 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:21,753 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:21,766 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:21,768 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:21,768 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:21,769 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:21,782 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:21,784 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:21,784 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:21,785 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:21,798 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:21,799 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:21,800 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:21,801 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:21,815 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:21,817 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:21,817 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:21,819 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:28,112 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,137 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,328 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,401 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,406 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,423 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,448 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,300 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,315 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,318 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,323 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,341 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,344 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,586 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,224 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,225 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,233 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,235 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,236 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,237 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,246 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,249 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,250 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,254 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,255 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,253 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:30,262 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,262 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,263 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,265 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,269 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,274 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,276 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,280 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:32,203 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:32,205 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:32,205 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:32,207 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:32,821 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:32,823 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:32,823 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:32,825 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:33,102 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:33,118 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:33,178 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:33,193 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:33,208 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:33,221 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:33,236 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:33,256 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:33,267 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:33,281 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:33,297 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:33,311 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:33,325 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:33,340 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:33,357 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:33,372 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:33,388 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:33,404 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:34,229 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:35,218 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:35,233 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:35,247 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:35,432 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:35,448 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:35,461 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:35,644 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:35,645 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:35,645 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:35,647 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:35,660 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:35,661 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:35,661 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:35,663 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:36,620 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:37,791 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:37,793 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:37,793 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:37,795 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:39,582 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:39,597 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:39,612 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:39,628 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:39,643 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:39,830 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:39,832 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:39,832 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:39,834 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:39,849 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:39,851 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:39,851 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:39,853 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:39,867 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:39,868 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:39,868 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:39,869 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:39,884 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:39,885 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:39,885 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:39,886 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:39,901 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:39,903 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:39,903 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:39,904 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:39,918 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:39,920 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:39,920 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:39,922 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:39,936 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:39,937 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:39,937 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:39,939 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:39,953 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:39,955 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:39,955 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:39,957 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:39,970 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:39,971 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:39,972 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:39,973 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:39,987 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:39,988 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:39,989 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:39,990 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:40,004 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:40,005 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:40,005 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:40,007 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:40,021 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:40,022 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:40,022 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:40,024 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:40,039 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:40,041 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:40,041 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:40,043 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:40,057 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:40,058 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:40,059 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:40,060 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:40,074 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:40,075 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:40,075 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:40,077 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:40,091 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:40,092 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:40,092 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:40,094 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:40,108 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:40,109 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:40,109 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:40,111 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:40,125 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:40,126 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:40,127 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:40,128 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:40,142 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:40,143 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:40,143 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:40,145 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:40,939 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:40,941 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:40,941 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:40,943 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:52,787 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:52,803 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:52,821 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:58,503 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:58,508 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:58,525 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:58,907 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:58,908 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:58,908 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:58,910 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:58,943 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:58,944 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:58,944 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:58,946 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:58,961 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:58,962 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:58,962 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:58,964 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:59,402 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:59,417 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:59,419 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:59,425 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:59,592 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:59,593 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:59,593 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:59,595 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:59,616 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:59,617 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:59,618 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:59,619 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:59,639 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:59,640 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:59,640 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:59,642 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:59,663 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:59,665 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:59,665 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:59,666 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:59,688 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:59,841 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:59,842 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:59,842 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:59,844 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:35:51,478 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,483 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,529 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,532 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,764 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,765 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,828 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,834 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,942 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,943 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,083 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,091 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,112 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,113 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,118 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,120 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,123 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,125 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,782 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,784 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,884 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,886 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,462 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,464 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,491 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,493 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,494 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,501 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,754 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,759 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,040 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,042 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,075 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,079 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,242 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,242 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,243 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,244 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,245 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,247 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,246 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,254 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,254 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,255 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,301 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,303 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,305 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,305 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,306 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,311 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,311 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,312 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,339 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,340 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,343 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,344 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,353 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,357 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,449 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,450 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,469 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,471 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,829 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,831 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,869 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,870 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,871 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,872 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,962 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,964 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,021 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,022 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,041 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,044 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,111 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,113 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,113 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,113 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,115 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,118 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,119 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,120 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,145 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,147 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,155 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,159 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,160 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,161 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,163 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,164 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,227 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,233 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,269 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,270 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,271 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,276 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,319 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,321 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,397 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,403 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,416 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,418 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,716 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,723 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,731 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,732 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,769 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,770 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,788 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,791 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,884 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,890 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,001 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,006 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,081 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,082 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,090 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,095 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,135 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,136 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,250 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,255 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,260 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,266 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,369 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,372 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,545 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,547 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,616 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,617 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,618 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,619 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,647 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,651 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,702 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,707 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,749 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,755 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,876 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,877 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,965 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,967 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,995 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,999 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,033 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,036 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,049 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,055 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,094 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,096 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,141 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,147 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,308 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,314 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,487 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,492 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,591 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,595 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,608 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,613 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,625 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,631 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,658 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,663 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,681 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,682 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,797 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,799 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,941 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,942 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,219 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,219 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,221 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,221 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,644 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,651 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,881 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,886 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,942 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,948 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,419 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,420 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,495 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,496 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,837 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,843 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,416 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,422 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,548 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,610 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,655 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,657 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,579 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,580 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:03,055 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:03,062 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:03,761 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:03,762 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:04,227 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:04,229 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:04,521 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:04,523 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:04,572 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:04,578 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:05,938 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:05,940 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:11,912 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:11,918 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:22,891 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,893 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,905 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,907 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,913 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,915 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,917 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,919 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,921 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,922 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,923 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,924 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,925 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,927 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,931 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,932 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,934 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,939 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,942 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,944 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,947 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,951 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,951 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,953 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,954 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,956 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,958 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,960 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,410 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,410 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,412 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,412 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,500 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,504 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,505 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,506 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,508 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,508 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,510 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,510 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,514 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,516 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,531 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,533 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,535 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,536 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,537 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,538 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,550 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,551 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,552 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,553 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,560 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,565 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,565 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,568 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,569 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,569 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,570 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,571 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,784 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,786 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,791 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,793 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,802 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,804 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,806 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,813 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,830 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,832 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,832 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,835 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,836 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,838 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,840 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,843 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,852 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,853 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,855 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,855 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,856 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,858 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,871 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,872 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,872 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,873 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,874 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,874 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,882 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,882 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,884 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,885 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,886 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,887 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,887 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,889 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,889 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,891 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,892 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,894 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,020 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,022 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,064 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,068 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,071 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,073 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,084 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,086 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,091 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,093 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,116 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,121 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,121 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,123 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,123 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,125 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,227 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,228 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,229 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,229 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,238 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,243 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,331 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,334 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,348 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,348 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,350 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,350 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,391 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,394 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,400 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,409 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,411 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,433 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,435 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,437 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,439 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,442 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,444 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,446 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,450 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,452 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,454 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,500 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,503 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,506 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,508 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,512 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,515 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,586 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,588 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,627 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,629 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,630 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,632 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,897 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,899 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,902 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,904 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,903 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,908 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,945 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,947 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,970 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,972 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,984 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,986 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,986 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,988 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,014 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,015 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,016 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,017 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,016 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,018 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,038 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,040 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,040 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,042 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,121 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,123 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,326 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,328 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,351 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,353 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,514 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,516 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,527 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,529 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,542 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,544 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,894 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,896 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,013 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,015 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,077 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,079 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,086 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,091 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,544 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,547 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,553 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,555 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,817 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,820 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,841 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,843 - distributed.utils - INFO - Reload module qme_vars from .py file
