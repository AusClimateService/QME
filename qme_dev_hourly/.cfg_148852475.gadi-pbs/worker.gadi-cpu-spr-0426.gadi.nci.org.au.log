Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-05 09:11:52,776 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:37297'
2025-09-05 09:11:52,785 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:42755'
2025-09-05 09:11:52,788 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:45889'
2025-09-05 09:11:52,793 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:46581'
2025-09-05 09:11:52,798 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:44815'
2025-09-05 09:11:52,803 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:33655'
2025-09-05 09:11:52,807 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:37569'
2025-09-05 09:11:52,812 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:44729'
2025-09-05 09:11:52,817 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:34825'
2025-09-05 09:11:52,822 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:37191'
2025-09-05 09:11:52,829 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:45239'
2025-09-05 09:11:52,835 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:45191'
2025-09-05 09:11:52,841 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:43557'
2025-09-05 09:11:52,846 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:42077'
2025-09-05 09:11:52,855 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:41673'
2025-09-05 09:11:52,857 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:42637'
2025-09-05 09:11:52,862 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:39343'
2025-09-05 09:11:52,867 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:39997'
2025-09-05 09:11:52,872 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:36031'
2025-09-05 09:11:52,876 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:45637'
2025-09-05 09:11:52,988 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:46177'
2025-09-05 09:11:52,992 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:46807'
2025-09-05 09:11:52,997 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:45343'
2025-09-05 09:11:53,001 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:38619'
2025-09-05 09:11:53,006 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:40279'
2025-09-05 09:11:53,012 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:35079'
2025-09-05 09:11:53,017 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:32985'
2025-09-05 09:11:53,021 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:37771'
2025-09-05 09:11:53,026 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:33461'
2025-09-05 09:11:53,031 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:36759'
2025-09-05 09:11:53,035 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:37577'
2025-09-05 09:11:53,040 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:32871'
2025-09-05 09:11:53,044 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:41409'
2025-09-05 09:11:53,049 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:45921'
2025-09-05 09:11:53,053 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:36099'
2025-09-05 09:11:53,058 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:44385'
2025-09-05 09:11:53,062 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:38371'
2025-09-05 09:11:53,066 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:36361'
2025-09-05 09:11:53,070 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:40987'
2025-09-05 09:11:53,072 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:45193'
2025-09-05 09:11:53,078 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:45571'
2025-09-05 09:11:53,081 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:35659'
2025-09-05 09:11:53,086 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:45687'
2025-09-05 09:11:53,090 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:44167'
2025-09-05 09:11:53,095 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:41609'
2025-09-05 09:11:53,100 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:41587'
2025-09-05 09:11:53,104 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:41347'
2025-09-05 09:11:53,109 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:38951'
2025-09-05 09:11:53,114 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:43923'
2025-09-05 09:11:53,117 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:40363'
2025-09-05 09:11:53,121 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:39453'
2025-09-05 09:11:53,126 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.66:35651'
2025-09-05 09:11:54,171 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:39163
2025-09-05 09:11:54,171 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:41271
2025-09-05 09:11:54,171 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:41269
2025-09-05 09:11:54,171 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:42119
2025-09-05 09:11:54,171 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:45055
2025-09-05 09:11:54,171 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:43555
2025-09-05 09:11:54,172 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:45055
2025-09-05 09:11:54,172 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:39163
2025-09-05 09:11:54,172 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:41271
2025-09-05 09:11:54,172 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:44931
2025-09-05 09:11:54,172 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:41269
2025-09-05 09:11:54,172 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:46253
2025-09-05 09:11:54,172 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:39711
2025-09-05 09:11:54,172 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:35035
2025-09-05 09:11:54,172 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:44887
2025-09-05 09:11:54,172 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:37547
2025-09-05 09:11:54,172 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:46071
2025-09-05 09:11:54,172 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:42119
2025-09-05 09:11:54,172 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:44251
2025-09-05 09:11:54,172 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:43577
2025-09-05 09:11:54,172 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:43555
2025-09-05 09:11:54,172 - distributed.worker - INFO -          dashboard at:          10.6.101.66:35413
2025-09-05 09:11:54,172 - distributed.worker - INFO -          dashboard at:          10.6.101.66:37945
2025-09-05 09:11:54,172 - distributed.worker - INFO -          dashboard at:          10.6.101.66:38167
2025-09-05 09:11:54,172 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:44931
2025-09-05 09:11:54,172 - distributed.worker - INFO -          dashboard at:          10.6.101.66:44169
2025-09-05 09:11:54,172 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:46253
2025-09-05 09:11:54,172 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:37839
2025-09-05 09:11:54,172 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:39711
2025-09-05 09:11:54,172 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:35035
2025-09-05 09:11:54,172 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:44887
2025-09-05 09:11:54,172 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:37547
2025-09-05 09:11:54,172 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:46071
2025-09-05 09:11:54,172 - distributed.worker - INFO -          dashboard at:          10.6.101.66:43489
2025-09-05 09:11:54,172 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:44251
2025-09-05 09:11:54,172 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:43577
2025-09-05 09:11:54,172 - distributed.worker - INFO -          dashboard at:          10.6.101.66:39079
2025-09-05 09:11:54,172 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,172 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,172 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,172 - distributed.worker - INFO -          dashboard at:          10.6.101.66:44447
2025-09-05 09:11:54,172 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,172 - distributed.worker - INFO -          dashboard at:          10.6.101.66:36835
2025-09-05 09:11:54,172 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:37839
2025-09-05 09:11:54,172 - distributed.worker - INFO -          dashboard at:          10.6.101.66:42565
2025-09-05 09:11:54,172 - distributed.worker - INFO -          dashboard at:          10.6.101.66:40817
2025-09-05 09:11:54,172 - distributed.worker - INFO -          dashboard at:          10.6.101.66:36363
2025-09-05 09:11:54,172 - distributed.worker - INFO -          dashboard at:          10.6.101.66:37645
2025-09-05 09:11:54,172 - distributed.worker - INFO -          dashboard at:          10.6.101.66:43037
2025-09-05 09:11:54,172 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,172 - distributed.worker - INFO -          dashboard at:          10.6.101.66:44085
2025-09-05 09:11:54,172 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,172 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,172 - distributed.worker - INFO -          dashboard at:          10.6.101.66:37699
2025-09-05 09:11:54,172 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,172 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,172 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,172 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,172 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,172 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,172 - distributed.worker - INFO -          dashboard at:          10.6.101.66:36741
2025-09-05 09:11:54,172 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,172 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,172 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,172 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,172 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,172 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,172 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,172 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,172 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,172 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,172 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,172 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,172 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,172 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,172 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,172 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,172 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,172 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,172 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,172 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,172 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,172 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,172 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,172 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,172 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,172 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,172 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,172 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,172 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,172 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,172 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,172 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,172 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,172 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,172 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,172 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,173 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,173 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,173 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,173 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-w4ks2mmd
2025-09-05 09:11:54,173 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,173 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,173 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ecoeju_d
2025-09-05 09:11:54,173 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,173 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-stjfcpco
2025-09-05 09:11:54,173 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,173 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-2muxd30c
2025-09-05 09:11:54,173 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,173 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,173 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,173 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,173 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-i65uxx3o
2025-09-05 09:11:54,173 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-4uoi54mt
2025-09-05 09:11:54,173 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,173 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,173 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,173 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-mkfvk11o
2025-09-05 09:11:54,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,173 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-j4yzil_j
2025-09-05 09:11:54,173 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ie1nhgyq
2025-09-05 09:11:54,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,173 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-_y0lhraj
2025-09-05 09:11:54,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,173 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,173 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-9prhxan4
2025-09-05 09:11:54,173 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-6vmbs_0d
2025-09-05 09:11:54,173 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-4evidim8
2025-09-05 09:11:54,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,173 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-e_9tu2lz
2025-09-05 09:11:54,173 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-n9ldnb9_
2025-09-05 09:11:54,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,173 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-v84am5iv
2025-09-05 09:11:54,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,179 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:44159
2025-09-05 09:11:54,179 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:44159
2025-09-05 09:11:54,179 - distributed.worker - INFO -          dashboard at:          10.6.101.66:35753
2025-09-05 09:11:54,179 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,179 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,179 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,179 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,179 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-mfwnfzlp
2025-09-05 09:11:54,180 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,180 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:46123
2025-09-05 09:11:54,180 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:36175
2025-09-05 09:11:54,180 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:46123
2025-09-05 09:11:54,180 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:36175
2025-09-05 09:11:54,180 - distributed.worker - INFO -          dashboard at:          10.6.101.66:38119
2025-09-05 09:11:54,180 - distributed.worker - INFO -          dashboard at:          10.6.101.66:34255
2025-09-05 09:11:54,180 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,180 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,180 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,180 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,180 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,180 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,180 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,180 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,180 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-eq2pj7pq
2025-09-05 09:11:54,180 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-2uufl4vl
2025-09-05 09:11:54,180 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,180 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,200 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,201 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,201 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,201 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,210 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,211 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,211 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,213 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,213 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:44111
2025-09-05 09:11:54,213 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:44111
2025-09-05 09:11:54,213 - distributed.worker - INFO -          dashboard at:          10.6.101.66:42757
2025-09-05 09:11:54,213 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,213 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,213 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,213 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,213 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ncia_dik
2025-09-05 09:11:54,213 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,215 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,216 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,216 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,217 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,221 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,222 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:32789
2025-09-05 09:11:54,222 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:32789
2025-09-05 09:11:54,222 - distributed.worker - INFO -          dashboard at:          10.6.101.66:35643
2025-09-05 09:11:54,222 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,222 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,222 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,222 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,222 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,222 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-_etigxoz
2025-09-05 09:11:54,222 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,222 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,224 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,225 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,225 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,225 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,226 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,229 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,230 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,231 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,232 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:39081
2025-09-05 09:11:54,232 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:39081
2025-09-05 09:11:54,232 - distributed.worker - INFO -          dashboard at:          10.6.101.66:42811
2025-09-05 09:11:54,232 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,232 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,232 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,232 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,232 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-nrpzc8bu
2025-09-05 09:11:54,232 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,232 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,233 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,234 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,234 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,236 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,236 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,237 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,238 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,240 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,241 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,243 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,243 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,244 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,244 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,245 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,246 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,247 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,247 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,248 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,248 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,249 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,249 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,251 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,251 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,252 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,252 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,254 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,253 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,255 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,255 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,256 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,256 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,257 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,257 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,259 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,259 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,260 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,260 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,261 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,262 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,262 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,263 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,264 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,265 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,266 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,266 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,268 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,268 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,269 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,269 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,271 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,283 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,284 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,284 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,285 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,290 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,291 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,291 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,292 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,293 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,294 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,294 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,296 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,352 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:34787
2025-09-05 09:11:54,352 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:34787
2025-09-05 09:11:54,352 - distributed.worker - INFO -          dashboard at:          10.6.101.66:35789
2025-09-05 09:11:54,352 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,352 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,352 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,352 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,352 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-oznd5069
2025-09-05 09:11:54,352 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,380 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,381 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,381 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,383 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,413 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:40799
2025-09-05 09:11:54,413 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:40799
2025-09-05 09:11:54,413 - distributed.worker - INFO -          dashboard at:          10.6.101.66:46201
2025-09-05 09:11:54,413 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,413 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,413 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,413 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,413 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-mvve8d6l
2025-09-05 09:11:54,413 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,434 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:41197
2025-09-05 09:11:54,434 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:41197
2025-09-05 09:11:54,434 - distributed.worker - INFO -          dashboard at:          10.6.101.66:44191
2025-09-05 09:11:54,434 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,434 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,434 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,434 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,434 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-kojcojg3
2025-09-05 09:11:54,434 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,434 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:45211
2025-09-05 09:11:54,435 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:45211
2025-09-05 09:11:54,435 - distributed.worker - INFO -          dashboard at:          10.6.101.66:42449
2025-09-05 09:11:54,435 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,435 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,435 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,435 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,435 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-8vq0j014
2025-09-05 09:11:54,435 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,441 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,442 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,442 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,443 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,462 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:42339
2025-09-05 09:11:54,462 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:42339
2025-09-05 09:11:54,462 - distributed.worker - INFO -          dashboard at:          10.6.101.66:46821
2025-09-05 09:11:54,462 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,462 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,462 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,462 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,462 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-mtewxn9a
2025-09-05 09:11:54,462 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,469 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,470 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,470 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,471 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:33963
2025-09-05 09:11:54,471 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:33963
2025-09-05 09:11:54,471 - distributed.worker - INFO -          dashboard at:          10.6.101.66:40463
2025-09-05 09:11:54,471 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,471 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,471 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,471 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,471 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-zpzihrni
2025-09-05 09:11:54,471 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,472 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,476 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,477 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,477 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,479 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,480 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:42457
2025-09-05 09:11:54,480 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:42457
2025-09-05 09:11:54,480 - distributed.worker - INFO -          dashboard at:          10.6.101.66:46055
2025-09-05 09:11:54,481 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,481 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,481 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,481 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,481 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-rhok9_0u
2025-09-05 09:11:54,481 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,481 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:35627
2025-09-05 09:11:54,481 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:35627
2025-09-05 09:11:54,481 - distributed.worker - INFO -          dashboard at:          10.6.101.66:45199
2025-09-05 09:11:54,481 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,481 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,481 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,481 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,481 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-bynbdkqs
2025-09-05 09:11:54,481 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,482 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:45563
2025-09-05 09:11:54,482 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:45563
2025-09-05 09:11:54,482 - distributed.worker - INFO -          dashboard at:          10.6.101.66:46273
2025-09-05 09:11:54,482 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,482 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,482 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,482 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,482 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-cc8m6gdv
2025-09-05 09:11:54,482 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,487 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:42545
2025-09-05 09:11:54,487 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:42545
2025-09-05 09:11:54,487 - distributed.worker - INFO -          dashboard at:          10.6.101.66:39309
2025-09-05 09:11:54,487 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,487 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,487 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,487 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,487 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ybyq1upo
2025-09-05 09:11:54,487 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,493 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:35917
2025-09-05 09:11:54,493 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:35917
2025-09-05 09:11:54,493 - distributed.worker - INFO -          dashboard at:          10.6.101.66:45141
2025-09-05 09:11:54,493 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,493 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,493 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,493 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,493 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-b8g5qyyh
2025-09-05 09:11:54,493 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,493 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,494 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,494 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,494 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,495 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:36369
2025-09-05 09:11:54,495 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:36369
2025-09-05 09:11:54,495 - distributed.worker - INFO -          dashboard at:          10.6.101.66:39657
2025-09-05 09:11:54,495 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,495 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,495 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,495 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,495 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-r_yevv53
2025-09-05 09:11:54,495 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,498 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:36761
2025-09-05 09:11:54,498 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:36761
2025-09-05 09:11:54,498 - distributed.worker - INFO -          dashboard at:          10.6.101.66:36351
2025-09-05 09:11:54,498 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,498 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,498 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:35149
2025-09-05 09:11:54,498 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,498 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,498 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:35149
2025-09-05 09:11:54,498 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-1xd2w4rh
2025-09-05 09:11:54,498 - distributed.worker - INFO -          dashboard at:          10.6.101.66:36097
2025-09-05 09:11:54,498 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,498 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,498 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,498 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,498 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,498 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-89rry23j
2025-09-05 09:11:54,498 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:40249
2025-09-05 09:11:54,498 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,498 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:40249
2025-09-05 09:11:54,498 - distributed.worker - INFO -          dashboard at:          10.6.101.66:40267
2025-09-05 09:11:54,498 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:33053
2025-09-05 09:11:54,499 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,499 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,499 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:33053
2025-09-05 09:11:54,499 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,499 - distributed.worker - INFO -          dashboard at:          10.6.101.66:45923
2025-09-05 09:11:54,499 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,499 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,499 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-muq2rlkn
2025-09-05 09:11:54,499 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,499 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,499 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,499 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,499 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-6dtta8xi
2025-09-05 09:11:54,499 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,499 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,500 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,500 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,501 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,503 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,504 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,504 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,505 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,508 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,509 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,509 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,511 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,512 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,513 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,513 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,515 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:44873
2025-09-05 09:11:54,515 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:44873
2025-09-05 09:11:54,515 - distributed.worker - INFO -          dashboard at:          10.6.101.66:39925
2025-09-05 09:11:54,515 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,515 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,515 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,515 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,515 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-s385ojen
2025-09-05 09:11:54,515 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,515 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,516 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,517 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,517 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,518 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,519 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,519 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,519 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,519 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,521 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,522 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,522 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,523 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,526 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,527 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,528 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,529 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,530 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,530 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,531 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,531 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,532 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,533 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,534 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,534 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,536 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,536 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,537 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,542 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,542 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,543 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,544 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,554 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:46531
2025-09-05 09:11:54,554 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:46531
2025-09-05 09:11:54,554 - distributed.worker - INFO -          dashboard at:          10.6.101.66:35615
2025-09-05 09:11:54,554 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,554 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,554 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,554 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,554 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-g_hxs3q3
2025-09-05 09:11:54,554 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,576 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:40711
2025-09-05 09:11:54,576 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:40711
2025-09-05 09:11:54,576 - distributed.worker - INFO -          dashboard at:          10.6.101.66:35123
2025-09-05 09:11:54,576 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,576 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,576 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,577 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,577 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-9wdqsyx4
2025-09-05 09:11:54,577 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,581 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:43619
2025-09-05 09:11:54,581 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:43619
2025-09-05 09:11:54,581 - distributed.worker - INFO -          dashboard at:          10.6.101.66:37159
2025-09-05 09:11:54,581 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,581 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,581 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,581 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,581 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-7bh4rcyv
2025-09-05 09:11:54,581 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,581 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,583 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,583 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,585 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,585 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:38121
2025-09-05 09:11:54,585 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:38121
2025-09-05 09:11:54,585 - distributed.worker - INFO -          dashboard at:          10.6.101.66:42003
2025-09-05 09:11:54,585 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,585 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,585 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,585 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,585 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-_rx3crkt
2025-09-05 09:11:54,585 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,595 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:36273
2025-09-05 09:11:54,595 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:36273
2025-09-05 09:11:54,595 - distributed.worker - INFO -          dashboard at:          10.6.101.66:36255
2025-09-05 09:11:54,595 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,595 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,595 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,595 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,595 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-xeeyo9p2
2025-09-05 09:11:54,595 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,599 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,599 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,599 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,600 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,602 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:35261
2025-09-05 09:11:54,602 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:35261
2025-09-05 09:11:54,602 - distributed.worker - INFO -          dashboard at:          10.6.101.66:42753
2025-09-05 09:11:54,602 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,602 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,602 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,602 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,602 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-e11noujh
2025-09-05 09:11:54,602 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,606 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:46653
2025-09-05 09:11:54,606 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:46653
2025-09-05 09:11:54,606 - distributed.worker - INFO -          dashboard at:          10.6.101.66:38887
2025-09-05 09:11:54,606 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,607 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,607 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,607 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,607 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-_084mm5t
2025-09-05 09:11:54,607 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,609 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,609 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,609 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,611 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:36215
2025-09-05 09:11:54,611 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:36215
2025-09-05 09:11:54,611 - distributed.worker - INFO -          dashboard at:          10.6.101.66:46359
2025-09-05 09:11:54,611 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,611 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,611 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,611 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,611 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,611 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-qt2j764k
2025-09-05 09:11:54,611 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,611 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:43957
2025-09-05 09:11:54,611 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:43957
2025-09-05 09:11:54,611 - distributed.worker - INFO -          dashboard at:          10.6.101.66:35161
2025-09-05 09:11:54,611 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,611 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,611 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,611 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,611 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-hptcdj6x
2025-09-05 09:11:54,611 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,612 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:37413
2025-09-05 09:11:54,612 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:37413
2025-09-05 09:11:54,612 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:35377
2025-09-05 09:11:54,612 - distributed.worker - INFO -          dashboard at:          10.6.101.66:43869
2025-09-05 09:11:54,612 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,612 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:35377
2025-09-05 09:11:54,612 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,612 - distributed.worker - INFO -          dashboard at:          10.6.101.66:34957
2025-09-05 09:11:54,612 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,612 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,612 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,612 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,612 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,612 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-gb42ectv
2025-09-05 09:11:54,612 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,612 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,612 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-8p838jn6
2025-09-05 09:11:54,612 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,612 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:38645
2025-09-05 09:11:54,612 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:38645
2025-09-05 09:11:54,612 - distributed.worker - INFO -          dashboard at:          10.6.101.66:46569
2025-09-05 09:11:54,613 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,613 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,613 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,613 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,613 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-hbmm2n4h
2025-09-05 09:11:54,613 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,615 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,616 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,616 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,618 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,619 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.66:40669
2025-09-05 09:11:54,619 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.66:40669
2025-09-05 09:11:54,619 - distributed.worker - INFO -          dashboard at:          10.6.101.66:44515
2025-09-05 09:11:54,619 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,619 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,619 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,619 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,619 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-mfhw799l
2025-09-05 09:11:54,619 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,623 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,624 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,625 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,626 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,628 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,628 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,628 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,629 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,635 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,635 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,636 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,637 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,638 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,638 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,638 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,639 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,642 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,643 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,643 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,644 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,645 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,646 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,646 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,648 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,648 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,649 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,649 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,650 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,651 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,652 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,652 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,654 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,654 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,655 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,655 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,657 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:12:06,721 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,722 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,722 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,722 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,722 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,722 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,722 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,722 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,722 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,723 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,723 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,723 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,723 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,723 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,723 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,723 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,723 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,724 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,724 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,724 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,724 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,724 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,724 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,724 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,724 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,724 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,724 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,724 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,725 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,725 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,725 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,725 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,725 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,725 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,725 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,725 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,725 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,725 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,725 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,725 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,725 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,725 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,725 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,725 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,726 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,726 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,724 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,726 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,726 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,726 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,726 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,726 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,726 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,726 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,726 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,726 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,726 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,726 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,726 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,726 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,727 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,727 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,727 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,727 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,727 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,727 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,727 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,727 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,727 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,727 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,727 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,727 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,725 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,727 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,727 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,728 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,726 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,726 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,729 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,729 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,727 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,729 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,729 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,729 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,727 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,730 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,730 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,730 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,729 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,729 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,730 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,730 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,731 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,734 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,735 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,736 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,736 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,736 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,737 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,738 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,739 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,739 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,740 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,741 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:09,524 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,524 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,524 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,524 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,524 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,525 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,524 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,525 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,525 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,525 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,525 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,526 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,526 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,526 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,526 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,526 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,526 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,525 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,526 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,526 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,526 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,526 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,526 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,526 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,526 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,527 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,527 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,527 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,527 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,527 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,527 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,527 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,527 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,527 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,525 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,528 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,527 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,528 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,528 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,528 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,528 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,528 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,529 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,528 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,529 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,526 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,529 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,528 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,529 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,529 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,529 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,528 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,527 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,529 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,529 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,529 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,529 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,529 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,529 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,527 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,529 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,529 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,529 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,529 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,530 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,529 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,530 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,529 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,529 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,529 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,529 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,530 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,530 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,530 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,530 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,528 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,531 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,528 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,528 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,528 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,530 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,531 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,531 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,531 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,532 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,532 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,532 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,532 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,532 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,532 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,532 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,532 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,532 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,532 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,530 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,532 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,532 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,533 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,533 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,533 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,533 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,533 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,533 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,534 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,911 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,911 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,911 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,911 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,911 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,911 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,911 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,911 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,911 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,912 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,912 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,912 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,912 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,912 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,912 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,912 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,913 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,913 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,913 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,913 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,913 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,913 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,913 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,913 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,913 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,913 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,913 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,914 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,914 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,914 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,914 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,914 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,914 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,914 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,914 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,914 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,915 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,915 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,915 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,915 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,915 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,915 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,915 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,915 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,915 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,915 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,915 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,916 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,916 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,916 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,916 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,916 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,916 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,916 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,916 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,916 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,916 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,916 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,916 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,916 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,916 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,916 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,916 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,917 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,917 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,917 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,917 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,917 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,917 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,917 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,917 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,917 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,917 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,917 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,918 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,918 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,918 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,918 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,918 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,918 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,918 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,918 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,918 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,918 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,918 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,919 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,919 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,919 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,919 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,919 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,919 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,919 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,919 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,919 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,919 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,919 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,919 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,920 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,920 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,920 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,920 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,920 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,921 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,925 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:10,367 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,367 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,368 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,368 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,368 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,368 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,368 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,368 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,368 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,368 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,368 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,368 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,369 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,369 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,369 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,369 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,369 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,369 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,369 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,369 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,370 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,370 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,370 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,370 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,370 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,370 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,370 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,370 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,370 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,370 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,370 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,370 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,370 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,370 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,370 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,370 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,371 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,370 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,370 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,371 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,371 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,371 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,371 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,371 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,371 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,371 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,371 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,371 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,371 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,371 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,371 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,371 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,371 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,372 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,372 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,372 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,372 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,372 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,372 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,372 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,372 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,372 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,372 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,372 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,372 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,372 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,372 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,372 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,372 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,372 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,372 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,373 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,373 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,373 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,373 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,373 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,373 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,373 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,373 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,373 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,373 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,373 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,373 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,373 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,374 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,374 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,375 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,375 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,375 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,375 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,375 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,375 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,375 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,375 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,375 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,376 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,376 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,376 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,376 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,376 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,376 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,376 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,376 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,374 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:16:17,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,770 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:48,784 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:48,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:51,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:53,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:53,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,484 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:55,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:57,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,754 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,718 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:02,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:02,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:02,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:02,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:02,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:03,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:03,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:03,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:04,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:04,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:04,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:04,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:06,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:06,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,895 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,883 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,984 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,853 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,858 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:19,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:20,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:20,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:20,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:20,883 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:21,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:23,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:23,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:24,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:24,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:28,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:28,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:28,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:28,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:28,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:29,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:29,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:33,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:33,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,604 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,268 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,469 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,591 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,275 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,527 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:39,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:40,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:41,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:42,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:42,250 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:43,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,190 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,543 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,395 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,396 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,719 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,723 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:55,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:55,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:55,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:56,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:56,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:56,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:56,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,309 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,650 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,851 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:58,400 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:58,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:58,446 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:58,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:08,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:11,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:11,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:12,444 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,917 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,367 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,591 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,190 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,202 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,055 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:40,128 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,035 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:49,469 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:50,649 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:50,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:51,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:51,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:51,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:51,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:51,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:51,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:51,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:51,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:51,270 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:54,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:54,384 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:55,980 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:56,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:56,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:58,495 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:59,821 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:59,822 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:00,466 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:00,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:07,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,495 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,980 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,985 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,483 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,483 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,484 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,971 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:15,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:15,943 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:17,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:17,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:20,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:20,203 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:20,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:20,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:20,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:20,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:21,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:22,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:23,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:27,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:30,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:34,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:34,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:35,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:35,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:37,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:37,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:37,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,381 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:45,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:45,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:46,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,617 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,922 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:52,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:53,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:56,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:56,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:57,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:57,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:57,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:57,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:57,925 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:58,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:59,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:59,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:59,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:59,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:59,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:59,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:00,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:05,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:10,883 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.101.37:43961
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2876, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.101.66:51042 remote=tcp://10.6.101.37:43961>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:20:11,266 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,267 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:42339. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,267 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:39081. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,267 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:32789. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,266 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,266 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,266 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,266 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,266 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,266 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,266 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,268 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:40669. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,266 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,266 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,268 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:35149. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,267 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,266 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,266 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,266 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,269 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:42457. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,267 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,269 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:35377. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,266 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,267 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,267 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,269 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:37413. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,267 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,269 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:38645. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,269 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:35917. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,266 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,267 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,269 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:36215. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,267 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,269 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:46123. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,267 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,269 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:39711. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,267 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,269 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:35035. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,270 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:44159. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,268 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,270 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:36369. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,267 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,268 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,270 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:36175. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,270 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:45563. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,270 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:44931. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,270 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:42119. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,268 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,267 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,270 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:43555. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,268 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,271 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:36761. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,268 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,272 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:43619. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,271 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:46253. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,271 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:46071. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,271 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:39163. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,266 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,271 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:44251. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,268 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,267 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,271 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:41197. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,272 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:46653. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,270 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,272 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:37547. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,269 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,272 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:33963. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,272 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:37839. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,272 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:36273. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,273 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:45211. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,273 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:35261. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,273 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:34787. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,273 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:35627. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,273 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:46531. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,302 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:40279'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,303 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:38619'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,304 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:36031'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,304 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:45921'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,304 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:41409'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,304 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:36099'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,304 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:41347'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.39:32873, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:32871'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1251, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:38371'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1104, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 8231, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3227, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1343, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:40987'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2881, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3851, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 672, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:32985'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 669, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2869, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2890, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 7723, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:37569'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3426, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 8553, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:45637'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,306 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:34825'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.66:43957, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,306 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:44729'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2643, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1151, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,306 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:35079'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.67:46247, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3781, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 8238, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,307 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:45889'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 956, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1209, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,307 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:41609'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1160, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1152, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.66:44111, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:42755'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3423, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1874, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,308 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:39343'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,308 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 8229, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,308 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1000, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,308 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2843, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,308 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 8160, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,308 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:43557'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,308 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2976, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,308 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 844, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,308 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 356, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,308 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:40363'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3027, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.65:40403, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2083, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:44167'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 7720, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,309 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:44815'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2878, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3080, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,310 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:33655'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,310 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 979, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,310 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:36361'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,310 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:35659'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,310 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 684, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,310 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:46581'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,310 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 675, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,310 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1474, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,310 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:37297'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,310 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1116, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1462, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:43923'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3329, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,311 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:45687'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 689, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 8244, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,311 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:45571'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,312 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 523, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3811, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 676, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,312 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:46807'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,312 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3782, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,312 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3071, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,312 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:33461'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,312 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3010, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,312 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:41673'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,313 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:37191'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,312 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1823, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,313 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3843, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,313 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:45193'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,313 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1796, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,313 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1811, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,313 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2871, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,314 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2883, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,314 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 8713, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,315 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2531, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,315 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2543, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,315 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3223, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,317 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,316 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.53:38255, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,317 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,319 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2667, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,319 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,319 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.50:42645, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,319 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,319 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,319 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.47:37757, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,320 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:37577'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,322 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 8252, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,322 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.53:37759, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,323 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.50:44189, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,323 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 8130, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,323 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2979, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,323 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.66:44111, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,325 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1047, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,325 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 546, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,331 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,332 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:11,332 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 8230, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,332 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3425, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,863 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,072 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:12,072 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:12,074 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:43577. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,075 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:40249. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,089 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.66:56108 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:12,088 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.66:55922 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:12,093 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:41587'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,094 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:12,094 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:12,094 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:12,095 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:12,095 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:12,095 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:42077'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,096 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:12,096 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:12,097 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:12,097 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:12,097 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:12,098 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1538e7119090>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:12,105 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,103 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x149010767590>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:12,114 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,294 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,357 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,419 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,438 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,439 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,493 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,519 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,335 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,336 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,541 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,825 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,868 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,893 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:38371'. Reason: nanny-close-gracefully
2025-09-05 09:20:13,896 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:38371' closed.
2025-09-05 09:20:13,899 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:34825'. Reason: nanny-close-gracefully
2025-09-05 09:20:13,900 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:34825' closed.
2025-09-05 09:20:13,902 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:13,905 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:41271. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:13,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:13,920 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.66:55786 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:13,925 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:39997'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:13,926 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2325, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:13,926 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:13,926 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:13,926 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:13,926 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:13,926 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:13,930 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14cf75cb6690>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:14,108 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,118 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,251 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:45889'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,252 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:45889' closed.
2025-09-05 09:20:14,299 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,361 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,423 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,425 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,425 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,442 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,443 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,486 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:41587'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,488 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:41587' closed.
2025-09-05 09:20:14,498 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,508 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:42077'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,509 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:42077' closed.
2025-09-05 09:20:14,524 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,726 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:45193'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,727 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:45193' closed.
2025-09-05 09:20:14,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:14,761 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:14,763 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:33053. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,777 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.66:56122 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:14,782 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:39453'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,783 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:14,783 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:14,783 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:14,783 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:14,783 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:14,787 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:14,793 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,811 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,826 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:35659'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,827 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:35659' closed.
2025-09-05 09:20:14,848 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:44815'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,849 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:44815' closed.
2025-09-05 09:20:14,851 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:37577'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,852 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:37577' closed.
2025-09-05 09:20:14,872 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:40363'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,874 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:40363' closed.
2025-09-05 09:20:14,927 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:33655'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,928 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:33655' closed.
2025-09-05 09:20:14,997 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:36031'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,998 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:36031' closed.
2025-09-05 09:20:15,072 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:15,075 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:44111. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,075 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:15,092 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.66:55976 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:15,096 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:46177'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,097 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:15,097 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:15,098 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:15,098 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:15,098 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:15,100 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14c223fe8cd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:15,107 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,246 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,546 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:15,829 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:15,907 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:15,909 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:41269. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,913 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:15,915 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:38121. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:15,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:15,926 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.66:55796 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:15,929 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:45191'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,930 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:15,931 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:15,931 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:15,931 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:15,931 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:15,931 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.66:56150 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:15,935 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:44385'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,936 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:15,937 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:15,937 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:15,937 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:15,937 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:15,935 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:15,941 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,939 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:15,945 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,996 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:44729'. Reason: nanny-close-gracefully
2025-09-05 09:20:15,997 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:44729' closed.
2025-09-05 09:20:16,013 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:16,017 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:45055. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:16,034 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.66:55770 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:16,038 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:42637'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,039 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:16,039 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:16,039 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:16,039 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:16,040 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:16,043 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x153da141b350>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:16,048 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,192 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:42755'. Reason: nanny-close-gracefully
2025-09-05 09:20:16,193 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:42755' closed.
2025-09-05 09:20:16,428 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,429 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,622 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:16,715 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,715 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:16,717 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:43957. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,726 - distributed.worker - ERROR - failed during get data with tcp://10.6.101.66:43957 -> tcp://10.6.101.66:35917
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.101.66:43957 remote=tcp://10.6.101.66:37674>: Stream is closed
2025-09-05 09:20:16,735 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.66:56208 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:16,738 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:38951'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,739 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:16,739 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:16,739 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:16,739 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:16,739 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:16,742 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1518350ea590>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:16,747 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,795 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,815 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,833 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:37297'. Reason: nanny-close-gracefully
2025-09-05 09:20:16,834 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:37297' closed.
2025-09-05 09:20:16,848 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:41609'. Reason: nanny-close-gracefully
2025-09-05 09:20:16,849 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:41609' closed.
2025-09-05 09:20:17,079 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,110 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,238 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:39453'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,239 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:39453' closed.
2025-09-05 09:20:17,251 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,260 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:35079'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,260 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:35079' closed.
2025-09-05 09:20:17,320 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,322 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,324 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,485 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:44167'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,487 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:44167' closed.
2025-09-05 09:20:17,533 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:46177'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,534 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:46177' closed.
2025-09-05 09:20:17,719 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:40279'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,720 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:40279' closed.
2025-09-05 09:20:17,763 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,802 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:17,806 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:42545. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:17,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:17,823 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.66:56070 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:17,827 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:45343'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:17,828 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:17,829 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:17,829 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:17,829 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:17,829 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:17,832 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14ccd1428e10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:17,838 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,944 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,948 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,035 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,052 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,430 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:45191'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,430 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:44385'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,431 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:45191' closed.
2025-09-05 09:20:18,432 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:44385' closed.
2025-09-05 09:20:18,467 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:18,467 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:18,469 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:40711. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,469 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:40799. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,470 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,469 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:18,471 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:44887. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,472 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:42637'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,473 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:42637' closed.
2025-09-05 09:20:18,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:18,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:18,481 - distributed.worker - ERROR - failed during get data with tcp://10.6.101.66:40711 -> tcp://10.6.101.46:39113
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.101.66:40711 remote=tcp://10.6.101.46:47574>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-05 09:20:18,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:18,487 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.66:56004 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:18,487 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.66:55908 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:18,491 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:45239'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,492 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:18,492 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:36759'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,492 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:18,493 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:18,492 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.66:56144 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:18,493 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:18,493 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:18,493 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:18,493 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:18,494 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:18,494 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:18,494 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:18,495 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:37771'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,496 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:18,496 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:18,496 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:18,496 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:18,496 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:18,495 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x149fdc608510>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:18,495 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x148065137710>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:18,498 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14f177e72110>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:18,501 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,501 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,503 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,586 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,594 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:18,595 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.66:44873. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:18,601 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.66:56134 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:18,603 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.66:35651'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,603 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2655, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:18,604 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:18,604 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:18,604 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:18,604 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:18,604 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:18,605 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1456dc4a2e90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:18,609 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,626 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,674 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,719 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,750 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,839 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,839 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,840 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,125 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:39997'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,126 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:39997' closed.
2025-09-05 09:20:19,144 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:38951'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,145 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:38951' closed.
2025-09-05 09:20:19,176 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:37191'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,181 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:37191' closed.
2025-09-05 09:20:19,232 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,232 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,233 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,250 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,323 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,326 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,328 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,641 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,704 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:45571'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,705 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:45571' closed.
2025-09-05 09:20:19,723 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:36099'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,724 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:36099' closed.
2025-09-05 09:20:19,726 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:45637'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,727 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:45637' closed.
2025-09-05 09:20:19,767 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,841 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,901 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,964 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,996 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,039 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,183 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:46581'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,184 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:46581' closed.
2025-09-05 09:20:20,234 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,298 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:45343'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,299 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:45343' closed.
2025-09-05 09:20:20,434 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:39343'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,435 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:39343' closed.
2025-09-05 09:20:20,474 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,504 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,504 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,506 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,590 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,611 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,678 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,843 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,843 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,844 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,856 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:45921'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,857 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:45921' closed.
2025-09-05 09:20:20,903 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:37771'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,903 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:37771' closed.
2025-09-05 09:20:21,001 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:45239'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,002 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:36759'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,003 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:45239' closed.
2025-09-05 09:20:21,003 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:36759' closed.
2025-09-05 09:20:21,091 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:35651'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,092 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:46807'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,092 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:35651' closed.
2025-09-05 09:20:21,093 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:46807' closed.
2025-09-05 09:20:21,152 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:41409'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,153 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:41409' closed.
2025-09-05 09:20:21,236 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:41347'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,236 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,236 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,237 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,238 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:41347' closed.
2025-09-05 09:20:21,263 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:43923'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,264 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:43923' closed.
2025-09-05 09:20:21,272 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:41673'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,273 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:41673' closed.
2025-09-05 09:20:21,635 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:32871'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,636 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:32871' closed.
2025-09-05 09:20:21,645 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,650 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:38619'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,650 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:38619' closed.
2025-09-05 09:20:21,653 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:32985'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,654 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:32985' closed.
2025-09-05 09:20:21,794 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:33461'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,795 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:33461' closed.
2025-09-05 09:20:21,906 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,968 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,001 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,036 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:37569'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,037 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:37569' closed.
2025-09-05 09:20:22,238 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,305 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:45687'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,306 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:45687' closed.
2025-09-05 09:20:22,377 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:43557'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,378 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:43557' closed.
2025-09-05 09:20:22,408 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:40987'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,409 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:40987' closed.
2025-09-05 09:20:22,638 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.66:36361'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,638 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.66:36361' closed.
2025-09-05 09:20:22,641 - distributed.dask_worker - INFO - End worker
