Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 10:31:44,230 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:43613'
2025-09-03 10:31:44,238 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:44921'
2025-09-03 10:31:44,244 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:39723'
2025-09-03 10:31:44,248 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:38603'
2025-09-03 10:31:44,252 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:45131'
2025-09-03 10:31:44,308 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:34145'
2025-09-03 10:31:44,311 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:44987'
2025-09-03 10:31:44,316 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:44079'
2025-09-03 10:31:44,320 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:32775'
2025-09-03 10:31:44,324 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:33325'
2025-09-03 10:31:44,330 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:39345'
2025-09-03 10:31:44,334 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:44215'
2025-09-03 10:31:44,339 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:40733'
2025-09-03 10:31:44,343 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:37903'
2025-09-03 10:31:44,348 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:36947'
2025-09-03 10:31:44,352 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:33145'
2025-09-03 10:31:44,357 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:32895'
2025-09-03 10:31:44,361 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:37403'
2025-09-03 10:31:44,365 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:36353'
2025-09-03 10:31:44,370 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:34209'
2025-09-03 10:31:44,374 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:33171'
2025-09-03 10:31:44,379 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:41585'
2025-09-03 10:31:44,383 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:42681'
2025-09-03 10:31:44,388 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:38113'
2025-09-03 10:31:44,393 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:34601'
2025-09-03 10:31:44,397 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:36283'
2025-09-03 10:31:44,401 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:45425'
2025-09-03 10:31:44,406 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:35821'
2025-09-03 10:31:44,410 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:35431'
2025-09-03 10:31:44,415 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:36433'
2025-09-03 10:31:44,419 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:39089'
2025-09-03 10:31:44,424 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:34423'
2025-09-03 10:31:44,428 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:44073'
2025-09-03 10:31:44,432 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:41955'
2025-09-03 10:31:44,437 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:37351'
2025-09-03 10:31:44,440 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:38817'
2025-09-03 10:31:44,446 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:42099'
2025-09-03 10:31:44,451 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:33361'
2025-09-03 10:31:44,456 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:43153'
2025-09-03 10:31:44,460 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:43285'
2025-09-03 10:31:44,464 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:33085'
2025-09-03 10:31:44,563 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:38173'
2025-09-03 10:31:44,568 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:35225'
2025-09-03 10:31:44,573 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:38429'
2025-09-03 10:31:44,578 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:38175'
2025-09-03 10:31:44,581 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:33299'
2025-09-03 10:31:44,586 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:42865'
2025-09-03 10:31:44,590 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:36639'
2025-09-03 10:31:44,595 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:41565'
2025-09-03 10:31:44,600 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:38303'
2025-09-03 10:31:44,604 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:46171'
2025-09-03 10:31:44,609 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:38839'
2025-09-03 10:31:44,613 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:43787'
2025-09-03 10:31:44,618 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:35147'
2025-09-03 10:31:44,622 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:40803'
2025-09-03 10:31:44,627 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:43725'
2025-09-03 10:31:44,632 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:36185'
2025-09-03 10:31:44,637 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:41057'
2025-09-03 10:31:44,642 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:37439'
2025-09-03 10:31:44,647 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:41023'
2025-09-03 10:31:44,651 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:38583'
2025-09-03 10:31:44,656 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:36845'
2025-09-03 10:31:44,661 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:44765'
2025-09-03 10:31:44,667 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:41533'
2025-09-03 10:31:44,673 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:40799'
2025-09-03 10:31:44,677 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:35283'
2025-09-03 10:31:44,682 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:40783'
2025-09-03 10:31:44,687 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:45385'
2025-09-03 10:31:44,691 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:42773'
2025-09-03 10:31:44,696 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:40427'
2025-09-03 10:31:44,701 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:42841'
2025-09-03 10:31:44,705 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:42799'
2025-09-03 10:31:44,710 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:38301'
2025-09-03 10:31:44,715 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:46809'
2025-09-03 10:31:44,719 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:39083'
2025-09-03 10:31:44,724 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:37271'
2025-09-03 10:31:44,730 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:45511'
2025-09-03 10:31:44,734 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:36847'
2025-09-03 10:31:44,753 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:36881'
2025-09-03 10:31:44,757 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:36039'
2025-09-03 10:31:44,763 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:39133'
2025-09-03 10:31:44,766 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:42563'
2025-09-03 10:31:44,772 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:35299'
2025-09-03 10:31:44,778 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:33355'
2025-09-03 10:31:44,784 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:40357'
2025-09-03 10:31:44,788 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:36475'
2025-09-03 10:31:44,793 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:45105'
2025-09-03 10:31:44,797 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:33609'
2025-09-03 10:31:44,800 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:36533'
2025-09-03 10:31:44,807 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:39183'
2025-09-03 10:31:44,811 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:43423'
2025-09-03 10:31:44,818 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:44117'
2025-09-03 10:31:44,823 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:33435'
2025-09-03 10:31:44,826 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:37919'
2025-09-03 10:31:44,830 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:44939'
2025-09-03 10:31:44,832 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:36247'
2025-09-03 10:31:44,835 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:42955'
2025-09-03 10:31:44,838 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:39939'
2025-09-03 10:31:44,840 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:44951'
2025-09-03 10:31:44,841 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:36295'
2025-09-03 10:31:44,845 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:37783'
2025-09-03 10:31:44,856 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:44045'
2025-09-03 10:31:44,860 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:45833'
2025-09-03 10:31:44,864 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.23:46279'
2025-09-03 10:31:46,406 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:41449
2025-09-03 10:31:46,406 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:44955
2025-09-03 10:31:46,406 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:36573
2025-09-03 10:31:46,406 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:34441
2025-09-03 10:31:46,407 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:40735
2025-09-03 10:31:46,407 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:38541
2025-09-03 10:31:46,407 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:36041
2025-09-03 10:31:46,407 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:41031
2025-09-03 10:31:46,407 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:44687
2025-09-03 10:31:46,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:41449
2025-09-03 10:31:46,407 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:40481
2025-09-03 10:31:46,407 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:34943
2025-09-03 10:31:46,407 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:42615
2025-09-03 10:31:46,407 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:41145
2025-09-03 10:31:46,407 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:45973
2025-09-03 10:31:46,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:44955
2025-09-03 10:31:46,407 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:36131
2025-09-03 10:31:46,407 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:38669
2025-09-03 10:31:46,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:36573
2025-09-03 10:31:46,407 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:39995
2025-09-03 10:31:46,407 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:37977
2025-09-03 10:31:46,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:34441
2025-09-03 10:31:46,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:40735
2025-09-03 10:31:46,407 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:33399
2025-09-03 10:31:46,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:38541
2025-09-03 10:31:46,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:36041
2025-09-03 10:31:46,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:41031
2025-09-03 10:31:46,407 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:37087
2025-09-03 10:31:46,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:44687
2025-09-03 10:31:46,407 - distributed.worker - INFO -          dashboard at:          10.6.102.23:37007
2025-09-03 10:31:46,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:40481
2025-09-03 10:31:46,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:34943
2025-09-03 10:31:46,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:42615
2025-09-03 10:31:46,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:41145
2025-09-03 10:31:46,407 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:43163
2025-09-03 10:31:46,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:45973
2025-09-03 10:31:46,407 - distributed.worker - INFO -          dashboard at:          10.6.102.23:38723
2025-09-03 10:31:46,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:36131
2025-09-03 10:31:46,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:38669
2025-09-03 10:31:46,407 - distributed.worker - INFO -          dashboard at:          10.6.102.23:44643
2025-09-03 10:31:46,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:39995
2025-09-03 10:31:46,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:37977
2025-09-03 10:31:46,407 - distributed.worker - INFO -          dashboard at:          10.6.102.23:42525
2025-09-03 10:31:46,407 - distributed.worker - INFO -          dashboard at:          10.6.102.23:37105
2025-09-03 10:31:46,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:33399
2025-09-03 10:31:46,407 - distributed.worker - INFO -          dashboard at:          10.6.102.23:38671
2025-09-03 10:31:46,407 - distributed.worker - INFO -          dashboard at:          10.6.102.23:45997
2025-09-03 10:31:46,407 - distributed.worker - INFO -          dashboard at:          10.6.102.23:42735
2025-09-03 10:31:46,407 - distributed.worker - INFO -          dashboard at:          10.6.102.23:39261
2025-09-03 10:31:46,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:37087
2025-09-03 10:31:46,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,407 - distributed.worker - INFO -          dashboard at:          10.6.102.23:33135
2025-09-03 10:31:46,407 - distributed.worker - INFO -          dashboard at:          10.6.102.23:41375
2025-09-03 10:31:46,407 - distributed.worker - INFO -          dashboard at:          10.6.102.23:36053
2025-09-03 10:31:46,407 - distributed.worker - INFO -          dashboard at:          10.6.102.23:45667
2025-09-03 10:31:46,407 - distributed.worker - INFO -          dashboard at:          10.6.102.23:41289
2025-09-03 10:31:46,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:43163
2025-09-03 10:31:46,407 - distributed.worker - INFO -          dashboard at:          10.6.102.23:38409
2025-09-03 10:31:46,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,407 - distributed.worker - INFO -          dashboard at:          10.6.102.23:38383
2025-09-03 10:31:46,407 - distributed.worker - INFO -          dashboard at:          10.6.102.23:39319
2025-09-03 10:31:46,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,407 - distributed.worker - INFO -          dashboard at:          10.6.102.23:43397
2025-09-03 10:31:46,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,407 - distributed.worker - INFO -          dashboard at:          10.6.102.23:46173
2025-09-03 10:31:46,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,407 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,407 - distributed.worker - INFO -          dashboard at:          10.6.102.23:43009
2025-09-03 10:31:46,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,407 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,407 - distributed.worker - INFO -          dashboard at:          10.6.102.23:44125
2025-09-03 10:31:46,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,407 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_sekclbo
2025-09-03 10:31:46,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-p34qtmth
2025-09-03 10:31:46,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2pcpoycd
2025-09-03 10:31:46,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-y5ccow3t
2025-09-03 10:31:46,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-nwwqq5f6
2025-09-03 10:31:46,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-m3kwk51_
2025-09-03 10:31:46,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-duk3d1j4
2025-09-03 10:31:46,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-h5rzezfn
2025-09-03 10:31:46,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-fqqj_f2u
2025-09-03 10:31:46,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-gv75iifu
2025-09-03 10:31:46,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-8khy_edq
2025-09-03 10:31:46,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-d4_f5hz1
2025-09-03 10:31:46,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-l63p4jdk
2025-09-03 10:31:46,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4vc6m1pr
2025-09-03 10:31:46,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-9kw193j4
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-i5pufmyj
2025-09-03 10:31:46,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hbsu0pes
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-uw7n26s6
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-p1d7m4z6
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ash4bihq
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4fw2jmtk
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,575 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:45447
2025-09-03 10:31:46,575 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:45447
2025-09-03 10:31:46,575 - distributed.worker - INFO -          dashboard at:          10.6.102.23:38259
2025-09-03 10:31:46,575 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,575 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,575 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,575 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,575 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-za47nn7u
2025-09-03 10:31:46,575 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,658 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:38373
2025-09-03 10:31:46,658 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:38373
2025-09-03 10:31:46,658 - distributed.worker - INFO -          dashboard at:          10.6.102.23:46083
2025-09-03 10:31:46,658 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,658 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,658 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,658 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,658 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-abq7q6in
2025-09-03 10:31:46,658 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,695 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:41487
2025-09-03 10:31:46,695 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:41487
2025-09-03 10:31:46,695 - distributed.worker - INFO -          dashboard at:          10.6.102.23:34415
2025-09-03 10:31:46,695 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,695 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,695 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,695 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,695 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_vwlwt5u
2025-09-03 10:31:46,695 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,725 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:33275
2025-09-03 10:31:46,725 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:33275
2025-09-03 10:31:46,725 - distributed.worker - INFO -          dashboard at:          10.6.102.23:40819
2025-09-03 10:31:46,725 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,725 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,725 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,725 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,725 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-zqb0khp5
2025-09-03 10:31:46,725 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,789 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:43673
2025-09-03 10:31:46,789 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:43673
2025-09-03 10:31:46,789 - distributed.worker - INFO -          dashboard at:          10.6.102.23:43995
2025-09-03 10:31:46,789 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,789 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,789 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,789 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,789 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-nut82vtb
2025-09-03 10:31:46,789 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,853 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:38141
2025-09-03 10:31:46,853 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:38141
2025-09-03 10:31:46,853 - distributed.worker - INFO -          dashboard at:          10.6.102.23:33971
2025-09-03 10:31:46,853 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,853 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,853 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,853 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,853 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-iw0v04xj
2025-09-03 10:31:46,853 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,874 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:42657
2025-09-03 10:31:46,874 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:42657
2025-09-03 10:31:46,874 - distributed.worker - INFO -          dashboard at:          10.6.102.23:35025
2025-09-03 10:31:46,874 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,875 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,875 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,875 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,875 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-gw98ovjb
2025-09-03 10:31:46,875 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,894 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:36149
2025-09-03 10:31:46,894 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:36149
2025-09-03 10:31:46,894 - distributed.worker - INFO -          dashboard at:          10.6.102.23:45291
2025-09-03 10:31:46,894 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,894 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,894 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,894 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,894 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7fq8r7il
2025-09-03 10:31:46,894 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,900 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:34045
2025-09-03 10:31:46,900 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:34045
2025-09-03 10:31:46,900 - distributed.worker - INFO -          dashboard at:          10.6.102.23:36771
2025-09-03 10:31:46,900 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,900 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,900 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,900 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,900 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-keasjdaq
2025-09-03 10:31:46,900 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,911 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:46493
2025-09-03 10:31:46,912 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:46493
2025-09-03 10:31:46,912 - distributed.worker - INFO -          dashboard at:          10.6.102.23:46175
2025-09-03 10:31:46,912 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,912 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,912 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,912 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,912 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-fgvxkzas
2025-09-03 10:31:46,912 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,954 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:43287
2025-09-03 10:31:46,954 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:43287
2025-09-03 10:31:46,954 - distributed.worker - INFO -          dashboard at:          10.6.102.23:46533
2025-09-03 10:31:46,954 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,954 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,954 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,954 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,954 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-vn1no4op
2025-09-03 10:31:46,954 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,966 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:39223
2025-09-03 10:31:46,966 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:39223
2025-09-03 10:31:46,966 - distributed.worker - INFO -          dashboard at:          10.6.102.23:35973
2025-09-03 10:31:46,966 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,966 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,966 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,967 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,967 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-fafr0tst
2025-09-03 10:31:46,967 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,128 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:40015
2025-09-03 10:31:47,128 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:40015
2025-09-03 10:31:47,128 - distributed.worker - INFO -          dashboard at:          10.6.102.23:44263
2025-09-03 10:31:47,128 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,128 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,128 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,128 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,128 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-v8652as6
2025-09-03 10:31:47,129 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,135 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:42419
2025-09-03 10:31:47,135 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:42419
2025-09-03 10:31:47,135 - distributed.worker - INFO -          dashboard at:          10.6.102.23:38785
2025-09-03 10:31:47,135 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,135 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,135 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,135 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,135 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-y5c7aknh
2025-09-03 10:31:47,135 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,272 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:40347
2025-09-03 10:31:47,272 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:40347
2025-09-03 10:31:47,272 - distributed.worker - INFO -          dashboard at:          10.6.102.23:40871
2025-09-03 10:31:47,272 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,272 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,272 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,272 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,272 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-u5zr1hvw
2025-09-03 10:31:47,272 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,289 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:45063
2025-09-03 10:31:47,289 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:45063
2025-09-03 10:31:47,289 - distributed.worker - INFO -          dashboard at:          10.6.102.23:39517
2025-09-03 10:31:47,289 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,289 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,289 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,289 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,289 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-0oui_9cn
2025-09-03 10:31:47,289 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,293 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:45285
2025-09-03 10:31:47,293 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:45285
2025-09-03 10:31:47,293 - distributed.worker - INFO -          dashboard at:          10.6.102.23:36733
2025-09-03 10:31:47,293 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,293 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,293 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,293 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,293 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-8ak7az27
2025-09-03 10:31:47,293 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,345 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:40009
2025-09-03 10:31:47,345 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:40009
2025-09-03 10:31:47,345 - distributed.worker - INFO -          dashboard at:          10.6.102.23:45749
2025-09-03 10:31:47,345 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,345 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,345 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,345 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,345 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hd_8gbt1
2025-09-03 10:31:47,345 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,389 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:33071
2025-09-03 10:31:47,389 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:33071
2025-09-03 10:31:47,389 - distributed.worker - INFO -          dashboard at:          10.6.102.23:38305
2025-09-03 10:31:47,389 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,389 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,389 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,389 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,389 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-bb5vfugs
2025-09-03 10:31:47,390 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,451 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:43869
2025-09-03 10:31:47,451 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:43869
2025-09-03 10:31:47,451 - distributed.worker - INFO -          dashboard at:          10.6.102.23:46865
2025-09-03 10:31:47,451 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,451 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,451 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,451 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,452 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-l3qfudy7
2025-09-03 10:31:47,452 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,494 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:33163
2025-09-03 10:31:47,495 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:33163
2025-09-03 10:31:47,495 - distributed.worker - INFO -          dashboard at:          10.6.102.23:42337
2025-09-03 10:31:47,495 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,495 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,495 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,495 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,495 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-p2ui45v2
2025-09-03 10:31:47,495 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,509 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:44897
2025-09-03 10:31:47,509 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:44897
2025-09-03 10:31:47,509 - distributed.worker - INFO -          dashboard at:          10.6.102.23:42151
2025-09-03 10:31:47,509 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,509 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,509 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,509 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,509 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-yeg4r9r9
2025-09-03 10:31:47,509 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,564 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:46671
2025-09-03 10:31:47,564 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:46671
2025-09-03 10:31:47,564 - distributed.worker - INFO -          dashboard at:          10.6.102.23:44061
2025-09-03 10:31:47,564 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,564 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,564 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,564 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,564 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-h1v7v7ew
2025-09-03 10:31:47,564 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,570 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:37451
2025-09-03 10:31:47,570 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:37451
2025-09-03 10:31:47,570 - distributed.worker - INFO -          dashboard at:          10.6.102.23:42167
2025-09-03 10:31:47,570 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,570 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,570 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,570 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,570 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-8zj8ww91
2025-09-03 10:31:47,570 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,580 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:43097
2025-09-03 10:31:47,580 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:43097
2025-09-03 10:31:47,580 - distributed.worker - INFO -          dashboard at:          10.6.102.23:34121
2025-09-03 10:31:47,580 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,580 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,580 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,580 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,580 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-iraqq0c8
2025-09-03 10:31:47,580 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,587 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:35217
2025-09-03 10:31:47,587 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:35217
2025-09-03 10:31:47,587 - distributed.worker - INFO -          dashboard at:          10.6.102.23:46515
2025-09-03 10:31:47,587 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,587 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,587 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,587 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,587 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-f9scqbux
2025-09-03 10:31:47,587 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,591 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:35197
2025-09-03 10:31:47,591 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:35197
2025-09-03 10:31:47,592 - distributed.worker - INFO -          dashboard at:          10.6.102.23:38613
2025-09-03 10:31:47,592 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,592 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,592 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,592 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,592 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-j73op0qq
2025-09-03 10:31:47,592 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,660 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:38491
2025-09-03 10:31:47,660 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:38491
2025-09-03 10:31:47,660 - distributed.worker - INFO -          dashboard at:          10.6.102.23:35277
2025-09-03 10:31:47,660 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,661 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,661 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,661 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,661 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_8cyowaw
2025-09-03 10:31:47,661 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,698 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:34711
2025-09-03 10:31:47,698 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:34711
2025-09-03 10:31:47,698 - distributed.worker - INFO -          dashboard at:          10.6.102.23:38069
2025-09-03 10:31:47,698 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,698 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,698 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,698 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,698 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6laslzz5
2025-09-03 10:31:47,698 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,707 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:44229
2025-09-03 10:31:47,707 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:44229
2025-09-03 10:31:47,707 - distributed.worker - INFO -          dashboard at:          10.6.102.23:36559
2025-09-03 10:31:47,707 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,707 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,707 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,707 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,707 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-unrz4iiy
2025-09-03 10:31:47,707 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,709 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:44403
2025-09-03 10:31:47,709 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:44403
2025-09-03 10:31:47,709 - distributed.worker - INFO -          dashboard at:          10.6.102.23:45353
2025-09-03 10:31:47,709 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,709 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,709 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,709 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,709 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-gjtq4xxb
2025-09-03 10:31:47,709 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,712 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:40309
2025-09-03 10:31:47,712 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:40309
2025-09-03 10:31:47,712 - distributed.worker - INFO -          dashboard at:          10.6.102.23:32995
2025-09-03 10:31:47,712 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,712 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,712 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,712 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,712 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-pmphchmi
2025-09-03 10:31:47,712 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,718 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:43249
2025-09-03 10:31:47,718 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:43249
2025-09-03 10:31:47,718 - distributed.worker - INFO -          dashboard at:          10.6.102.23:41001
2025-09-03 10:31:47,718 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,718 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,718 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,718 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,718 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-t43p35sa
2025-09-03 10:31:47,718 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,744 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:34513
2025-09-03 10:31:47,744 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:34513
2025-09-03 10:31:47,744 - distributed.worker - INFO -          dashboard at:          10.6.102.23:34041
2025-09-03 10:31:47,744 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,744 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,744 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,744 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,744 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-x2yy9tu3
2025-09-03 10:31:47,745 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,752 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:41617
2025-09-03 10:31:47,753 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:41617
2025-09-03 10:31:47,753 - distributed.worker - INFO -          dashboard at:          10.6.102.23:45497
2025-09-03 10:31:47,753 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,753 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,753 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,753 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,753 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-r1dl7cne
2025-09-03 10:31:47,753 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,757 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:36429
2025-09-03 10:31:47,757 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:36429
2025-09-03 10:31:47,757 - distributed.worker - INFO -          dashboard at:          10.6.102.23:43957
2025-09-03 10:31:47,757 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,757 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,757 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,758 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,758 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-lju21m1l
2025-09-03 10:31:47,758 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,766 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:39299
2025-09-03 10:31:47,766 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:39299
2025-09-03 10:31:47,766 - distributed.worker - INFO -          dashboard at:          10.6.102.23:42777
2025-09-03 10:31:47,766 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,766 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,766 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,766 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,766 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-caudipke
2025-09-03 10:31:47,766 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,786 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:43193
2025-09-03 10:31:47,786 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:43193
2025-09-03 10:31:47,786 - distributed.worker - INFO -          dashboard at:          10.6.102.23:40177
2025-09-03 10:31:47,786 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,786 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,786 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,786 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,786 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ahsbtjey
2025-09-03 10:31:47,786 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,790 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:41149
2025-09-03 10:31:47,791 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:41149
2025-09-03 10:31:47,791 - distributed.worker - INFO -          dashboard at:          10.6.102.23:34771
2025-09-03 10:31:47,791 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,791 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,791 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,791 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,791 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2idez13m
2025-09-03 10:31:47,791 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,803 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:45267
2025-09-03 10:31:47,803 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:45267
2025-09-03 10:31:47,803 - distributed.worker - INFO -          dashboard at:          10.6.102.23:41589
2025-09-03 10:31:47,803 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,803 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,803 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,803 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,803 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ixa9jcfd
2025-09-03 10:31:47,804 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,817 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:37449
2025-09-03 10:31:47,817 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:37449
2025-09-03 10:31:47,817 - distributed.worker - INFO -          dashboard at:          10.6.102.23:46369
2025-09-03 10:31:47,817 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,817 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,817 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,817 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,817 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-9um5vkgb
2025-09-03 10:31:47,817 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,875 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:47,877 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,879 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:47,901 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:43895
2025-09-03 10:31:47,901 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:43895
2025-09-03 10:31:47,901 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:47,901 - distributed.worker - INFO -          dashboard at:          10.6.102.23:41931
2025-09-03 10:31:47,901 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,901 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,901 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,901 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,901 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-jyoq8h7u
2025-09-03 10:31:47,901 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,902 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,902 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,904 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:47,904 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:44855
2025-09-03 10:31:47,904 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:44855
2025-09-03 10:31:47,904 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:40767
2025-09-03 10:31:47,904 - distributed.worker - INFO -          dashboard at:          10.6.102.23:42993
2025-09-03 10:31:47,905 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:40767
2025-09-03 10:31:47,905 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,905 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,905 - distributed.worker - INFO -          dashboard at:          10.6.102.23:36887
2025-09-03 10:31:47,905 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,905 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,905 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,905 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,905 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-g476udkq
2025-09-03 10:31:47,905 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,905 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,905 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,905 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-h4mi7y6v
2025-09-03 10:31:47,905 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,933 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:34229
2025-09-03 10:31:47,933 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:34229
2025-09-03 10:31:47,933 - distributed.worker - INFO -          dashboard at:          10.6.102.23:38237
2025-09-03 10:31:47,933 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,933 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,933 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,933 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,933 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-nhwaew1g
2025-09-03 10:31:47,933 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,964 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:33769
2025-09-03 10:31:47,964 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:33769
2025-09-03 10:31:47,964 - distributed.worker - INFO -          dashboard at:          10.6.102.23:46219
2025-09-03 10:31:47,964 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,964 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,964 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:47,964 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:47,964 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-z34y38pq
2025-09-03 10:31:47,964 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,006 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:33751
2025-09-03 10:31:48,006 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:33751
2025-09-03 10:31:48,006 - distributed.worker - INFO -          dashboard at:          10.6.102.23:36949
2025-09-03 10:31:48,006 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,006 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,006 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,006 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,006 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-1qqs9_16
2025-09-03 10:31:48,006 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,009 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:39059
2025-09-03 10:31:48,009 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:39059
2025-09-03 10:31:48,009 - distributed.worker - INFO -          dashboard at:          10.6.102.23:33251
2025-09-03 10:31:48,009 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,009 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,009 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,009 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,009 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-xa_6f7wj
2025-09-03 10:31:48,009 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,028 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,030 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,030 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,032 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,040 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:46539
2025-09-03 10:31:48,040 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:46539
2025-09-03 10:31:48,040 - distributed.worker - INFO -          dashboard at:          10.6.102.23:38075
2025-09-03 10:31:48,040 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,040 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,040 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,040 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,040 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-llys95vr
2025-09-03 10:31:48,040 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,050 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:46265
2025-09-03 10:31:48,050 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:46265
2025-09-03 10:31:48,050 - distributed.worker - INFO -          dashboard at:          10.6.102.23:43421
2025-09-03 10:31:48,050 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,050 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,050 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,050 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,050 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-a69_n54x
2025-09-03 10:31:48,050 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,087 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:40983
2025-09-03 10:31:48,088 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:40983
2025-09-03 10:31:48,088 - distributed.worker - INFO -          dashboard at:          10.6.102.23:44485
2025-09-03 10:31:48,088 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,088 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,088 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,088 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,088 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-qvn5obyn
2025-09-03 10:31:48,088 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,140 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:34877
2025-09-03 10:31:48,140 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:34877
2025-09-03 10:31:48,140 - distributed.worker - INFO -          dashboard at:          10.6.102.23:37237
2025-09-03 10:31:48,140 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,140 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,140 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,140 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,140 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-gduc5x1y
2025-09-03 10:31:48,140 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,144 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:44177
2025-09-03 10:31:48,144 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:44177
2025-09-03 10:31:48,144 - distributed.worker - INFO -          dashboard at:          10.6.102.23:35189
2025-09-03 10:31:48,144 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,144 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,144 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,144 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,144 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-nexyaytn
2025-09-03 10:31:48,144 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,145 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:40899
2025-09-03 10:31:48,145 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:40899
2025-09-03 10:31:48,145 - distributed.worker - INFO -          dashboard at:          10.6.102.23:39273
2025-09-03 10:31:48,145 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,145 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,145 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,145 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,145 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-tov3ay_u
2025-09-03 10:31:48,145 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,220 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:46043
2025-09-03 10:31:48,221 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:46043
2025-09-03 10:31:48,221 - distributed.worker - INFO -          dashboard at:          10.6.102.23:39949
2025-09-03 10:31:48,221 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,221 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,221 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,221 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,221 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-j9h2ssr7
2025-09-03 10:31:48,221 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,235 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:41599
2025-09-03 10:31:48,235 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:41599
2025-09-03 10:31:48,235 - distributed.worker - INFO -          dashboard at:          10.6.102.23:43605
2025-09-03 10:31:48,235 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,235 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,235 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,236 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,236 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-8mf4b0_z
2025-09-03 10:31:48,236 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,343 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:46553
2025-09-03 10:31:48,343 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:46553
2025-09-03 10:31:48,343 - distributed.worker - INFO -          dashboard at:          10.6.102.23:45715
2025-09-03 10:31:48,343 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,343 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,343 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,343 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,343 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-9sd0gqdp
2025-09-03 10:31:48,343 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,348 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:34985
2025-09-03 10:31:48,348 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:34985
2025-09-03 10:31:48,348 - distributed.worker - INFO -          dashboard at:          10.6.102.23:42767
2025-09-03 10:31:48,348 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,348 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,348 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,348 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,348 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-eoqrianc
2025-09-03 10:31:48,348 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,350 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:33517
2025-09-03 10:31:48,350 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:33517
2025-09-03 10:31:48,350 - distributed.worker - INFO -          dashboard at:          10.6.102.23:41479
2025-09-03 10:31:48,350 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,350 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,350 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,350 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,350 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hq9_2cxc
2025-09-03 10:31:48,350 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,365 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:42281
2025-09-03 10:31:48,365 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:42281
2025-09-03 10:31:48,365 - distributed.worker - INFO -          dashboard at:          10.6.102.23:39521
2025-09-03 10:31:48,365 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,365 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,365 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,365 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,365 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6je9fvu6
2025-09-03 10:31:48,365 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,422 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:37755
2025-09-03 10:31:48,423 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:37755
2025-09-03 10:31:48,423 - distributed.worker - INFO -          dashboard at:          10.6.102.23:45343
2025-09-03 10:31:48,423 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,423 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,423 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,423 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,423 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4gxmhk6o
2025-09-03 10:31:48,423 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,453 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:44363
2025-09-03 10:31:48,453 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:44363
2025-09-03 10:31:48,453 - distributed.worker - INFO -          dashboard at:          10.6.102.23:44661
2025-09-03 10:31:48,453 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,453 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,453 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,453 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,453 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-0o398v7l
2025-09-03 10:31:48,453 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,461 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:33871
2025-09-03 10:31:48,461 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:33871
2025-09-03 10:31:48,461 - distributed.worker - INFO -          dashboard at:          10.6.102.23:43549
2025-09-03 10:31:48,461 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,461 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,461 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,461 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,461 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-mgl7uyxp
2025-09-03 10:31:48,462 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,463 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,464 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,466 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,475 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,476 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,476 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,478 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,513 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:46085
2025-09-03 10:31:48,513 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:46085
2025-09-03 10:31:48,514 - distributed.worker - INFO -          dashboard at:          10.6.102.23:36561
2025-09-03 10:31:48,514 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,514 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,514 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,514 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,514 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-wt2a86si
2025-09-03 10:31:48,514 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,518 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:39533
2025-09-03 10:31:48,518 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:39533
2025-09-03 10:31:48,518 - distributed.worker - INFO -          dashboard at:          10.6.102.23:39827
2025-09-03 10:31:48,518 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,518 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,518 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,518 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,518 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-g4syg2zf
2025-09-03 10:31:48,519 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,529 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:43897
2025-09-03 10:31:48,529 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:43897
2025-09-03 10:31:48,529 - distributed.worker - INFO -          dashboard at:          10.6.102.23:46471
2025-09-03 10:31:48,529 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,529 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,529 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,529 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,529 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2vah5qmo
2025-09-03 10:31:48,529 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,530 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:42759
2025-09-03 10:31:48,530 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:42759
2025-09-03 10:31:48,530 - distributed.worker - INFO -          dashboard at:          10.6.102.23:38033
2025-09-03 10:31:48,530 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,530 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,530 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,530 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,530 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hdyv81_3
2025-09-03 10:31:48,531 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,535 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:41301
2025-09-03 10:31:48,535 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:41301
2025-09-03 10:31:48,535 - distributed.worker - INFO -          dashboard at:          10.6.102.23:35255
2025-09-03 10:31:48,535 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,535 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,535 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,535 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,535 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-e_fbj_n2
2025-09-03 10:31:48,536 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,538 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:44159
2025-09-03 10:31:48,538 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:44159
2025-09-03 10:31:48,538 - distributed.worker - INFO -          dashboard at:          10.6.102.23:35697
2025-09-03 10:31:48,538 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,538 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,538 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,538 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,538 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-frtywpdc
2025-09-03 10:31:48,539 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,582 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:36191
2025-09-03 10:31:48,582 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:36191
2025-09-03 10:31:48,582 - distributed.worker - INFO -          dashboard at:          10.6.102.23:44727
2025-09-03 10:31:48,582 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,583 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,583 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,583 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,583 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6gqdh24e
2025-09-03 10:31:48,583 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,597 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:38341
2025-09-03 10:31:48,597 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:38341
2025-09-03 10:31:48,597 - distributed.worker - INFO -          dashboard at:          10.6.102.23:46491
2025-09-03 10:31:48,597 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,597 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,597 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,597 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,597 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-9wh3hjjl
2025-09-03 10:31:48,597 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,622 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:38737
2025-09-03 10:31:48,622 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:38737
2025-09-03 10:31:48,622 - distributed.worker - INFO -          dashboard at:          10.6.102.23:45413
2025-09-03 10:31:48,622 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,622 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,622 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,622 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,622 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-y8_s_l_i
2025-09-03 10:31:48,622 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,630 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:45055
2025-09-03 10:31:48,630 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:46611
2025-09-03 10:31:48,630 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:45055
2025-09-03 10:31:48,630 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:46611
2025-09-03 10:31:48,630 - distributed.worker - INFO -          dashboard at:          10.6.102.23:38941
2025-09-03 10:31:48,630 - distributed.worker - INFO -          dashboard at:          10.6.102.23:44091
2025-09-03 10:31:48,630 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,630 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,630 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,630 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,630 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,630 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,630 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,630 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,630 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-8h2jxhgn
2025-09-03 10:31:48,630 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6y047t03
2025-09-03 10:31:48,630 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,630 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,635 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:38435
2025-09-03 10:31:48,635 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:38435
2025-09-03 10:31:48,635 - distributed.worker - INFO -          dashboard at:          10.6.102.23:39307
2025-09-03 10:31:48,635 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,635 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,635 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,635 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,635 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_5qvcrf7
2025-09-03 10:31:48,636 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,641 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:39541
2025-09-03 10:31:48,641 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:39541
2025-09-03 10:31:48,641 - distributed.worker - INFO -          dashboard at:          10.6.102.23:38047
2025-09-03 10:31:48,641 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,641 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,641 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,641 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,642 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7g6ho2a7
2025-09-03 10:31:48,642 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,643 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:46747
2025-09-03 10:31:48,643 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:46747
2025-09-03 10:31:48,643 - distributed.worker - INFO -          dashboard at:          10.6.102.23:40065
2025-09-03 10:31:48,643 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,643 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,643 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,643 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,643 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-0c_2v_5b
2025-09-03 10:31:48,644 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,645 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:35239
2025-09-03 10:31:48,645 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:35239
2025-09-03 10:31:48,645 - distributed.worker - INFO -          dashboard at:          10.6.102.23:44753
2025-09-03 10:31:48,645 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,645 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,645 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,645 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,645 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4vwj8rcr
2025-09-03 10:31:48,645 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,650 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:33069
2025-09-03 10:31:48,650 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:33069
2025-09-03 10:31:48,650 - distributed.worker - INFO -          dashboard at:          10.6.102.23:45969
2025-09-03 10:31:48,650 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,650 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,650 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,650 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,650 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-xpfxbzlm
2025-09-03 10:31:48,650 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,650 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:33565
2025-09-03 10:31:48,650 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:33565
2025-09-03 10:31:48,650 - distributed.worker - INFO -          dashboard at:          10.6.102.23:45259
2025-09-03 10:31:48,650 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,650 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,650 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,650 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,650 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-386i69an
2025-09-03 10:31:48,650 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,652 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:42283
2025-09-03 10:31:48,652 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:42283
2025-09-03 10:31:48,652 - distributed.worker - INFO -          dashboard at:          10.6.102.23:44695
2025-09-03 10:31:48,652 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,652 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,652 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,652 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,652 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-32e5v5t7
2025-09-03 10:31:48,653 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,653 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:44047
2025-09-03 10:31:48,653 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:44047
2025-09-03 10:31:48,653 - distributed.worker - INFO -          dashboard at:          10.6.102.23:43099
2025-09-03 10:31:48,653 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,653 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,653 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,653 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,653 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-5a9qxo_i
2025-09-03 10:31:48,653 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,659 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.23:33909
2025-09-03 10:31:48,660 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.23:33909
2025-09-03 10:31:48,660 - distributed.worker - INFO -          dashboard at:          10.6.102.23:41619
2025-09-03 10:31:48,660 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,660 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,660 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:48,660 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:48,660 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-a0gyzj1y
2025-09-03 10:31:48,660 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,903 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,904 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,904 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,906 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,929 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,930 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,930 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,932 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:49,108 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:49,109 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:49,109 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:49,111 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:49,148 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:49,149 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:49,149 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:49,151 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:49,161 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:49,162 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:49,162 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:49,164 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:49,200 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:49,201 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:49,201 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:49,203 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:49,667 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:49,668 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:49,668 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:49,670 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:49,681 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:49,682 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:49,682 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:49,684 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:49,694 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:49,695 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:49,695 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:49,697 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:49,733 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:49,734 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:49,734 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:49,736 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:49,931 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:49,932 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:49,933 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:49,934 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:49,944 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:49,946 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:49,946 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:49,948 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:49,958 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:49,959 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:49,959 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:49,961 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:49,970 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:49,971 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:49,971 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:49,973 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:50,419 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:50,420 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,420 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,422 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:50,432 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:50,433 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,433 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,435 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:50,445 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:50,447 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,447 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,449 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:50,632 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:50,633 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,633 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,635 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:50,644 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:50,646 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,646 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,648 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:50,658 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:50,659 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,659 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,661 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:50,699 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:50,700 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,700 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,703 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:50,712 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:50,713 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,714 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,715 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:50,807 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:50,809 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,809 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,810 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:50,822 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:50,823 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,823 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,825 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:50,835 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:50,836 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,838 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:50,848 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:50,849 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,849 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,851 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:50,861 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:50,863 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,863 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,864 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:50,943 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:50,944 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,945 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,946 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:51,228 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:51,230 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,230 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,232 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:51,692 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:51,693 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,693 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,695 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:51,745 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:51,746 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,746 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,748 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:51,799 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:51,800 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,800 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,802 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:51,812 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:51,813 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,813 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,815 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:51,825 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:51,826 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,827 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,829 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:51,839 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:51,840 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,840 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,842 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:51,866 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:51,867 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,867 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,870 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:51,893 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:51,894 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,894 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,896 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:51,906 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:51,907 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,907 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,909 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:52,941 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:52,942 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,943 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,944 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:53,382 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:53,383 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,383 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,385 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:53,396 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:53,397 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,398 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,400 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:53,728 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:53,729 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,729 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,731 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:53,743 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:53,744 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,744 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,746 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:53,756 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:53,757 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,757 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,759 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:53,770 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:53,771 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,771 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,773 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,352 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,354 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,354 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,356 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,745 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,746 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,747 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,748 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,956 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,958 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,958 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,959 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,984 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,985 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,985 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,987 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,998 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,999 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,999 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,001 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,012 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,013 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,013 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,015 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,097 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,098 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,099 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,101 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,139 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,141 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,141 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,143 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,181 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,183 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,183 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,185 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,195 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,197 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,197 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,198 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,209 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,210 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,210 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,212 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,223 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,224 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,224 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,226 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,237 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,238 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,239 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,240 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,331 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,332 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,333 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,334 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,359 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,360 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,360 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,362 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,373 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,374 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,374 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,376 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,387 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,389 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,389 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,391 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,401 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,403 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,403 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,405 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,415 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,416 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,417 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,418 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,429 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,430 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,430 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,432 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,622 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,624 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,624 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,626 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,636 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,638 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,638 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,640 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,651 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,653 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,653 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,655 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,666 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,668 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,668 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,669 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,871 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,873 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,873 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,875 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,885 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,887 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,887 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,888 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,901 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,902 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,903 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,904 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,916 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,917 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,917 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,919 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:03,051 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:03,052 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:03,053 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:03,054 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,549 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,550 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,551 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,552 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,567 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,571 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,571 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,576 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,580 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,582 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,582 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,584 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,596 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,597 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,597 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,599 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,612 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,614 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,614 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,616 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,628 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,630 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,630 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,632 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,645 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,647 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,648 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,652 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,659 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,661 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,661 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,663 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,675 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,676 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,677 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,678 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,693 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,698 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,698 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,703 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,709 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,714 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,714 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,718 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,723 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,725 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,725 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,726 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,740 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,743 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,743 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,746 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,755 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,757 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,757 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,759 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,775 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,780 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,780 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,785 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,788 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,789 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,790 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,791 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,804 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,805 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,806 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,807 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,820 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,822 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,822 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,824 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,838 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,843 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,843 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,848 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,854 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,858 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,859 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,863 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,868 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,870 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,870 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,872 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:12,005 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:12,010 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:12,010 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:12,015 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:18,880 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:18,906 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:20,937 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:20,953 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:21,815 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:21,829 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:21,839 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:22,804 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:22,817 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:22,831 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:22,845 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:22,872 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:25,359 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:26,016 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:38,770 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:38,770 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:38,772 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:42,555 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:42,652 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:42,666 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:42,681 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:42,702 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:42,718 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:42,730 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:42,748 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:42,762 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:42,785 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:42,794 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:42,809 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:47,685 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:47,687 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:47,687 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:47,688 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:47,705 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:47,707 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:47,707 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:47,711 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:47,720 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:47,721 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:47,721 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:47,723 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:49,882 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:49,906 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:51,938 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:51,951 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:53,804 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:53,817 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:53,831 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:53,845 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:53,873 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:35:51,493 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,498 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,609 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,615 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,667 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,672 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,884 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,888 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,980 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,985 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,057 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,064 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,073 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,074 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,090 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,092 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,118 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,127 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,128 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,133 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,198 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,203 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,323 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,328 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,395 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,396 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,397 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,399 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,425 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,427 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,450 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,452 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,481 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,485 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,539 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,544 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,609 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,612 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,796 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,802 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,905 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,913 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,944 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,951 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,954 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,957 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,028 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,038 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,058 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,063 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,243 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,244 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,244 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,248 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,255 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,256 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,270 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,271 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,271 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,276 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,309 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,312 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,351 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,356 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,383 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,389 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,392 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,393 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,814 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,821 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,843 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,849 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,961 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,965 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,178 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,179 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,247 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,249 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,293 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,298 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,347 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,349 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,408 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,408 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,410 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,417 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,426 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,427 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,608 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,611 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,689 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,691 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,713 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,717 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,717 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,718 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,725 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,727 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,748 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,749 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,756 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,757 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,818 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,824 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,010 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,016 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,044 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,048 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,063 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,065 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,145 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,148 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,203 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,204 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,209 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,209 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,246 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,252 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,256 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,259 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,262 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,266 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,502 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,503 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,525 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,527 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,538 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,542 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,713 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,714 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,894 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,896 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,907 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,910 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,969 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,970 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,108 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,113 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,411 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,412 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,506 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,509 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,678 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,683 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,810 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,812 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,900 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,906 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,020 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,022 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,093 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,098 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,117 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,119 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,223 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,227 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,277 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,283 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,543 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,546 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,716 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,717 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,719 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,722 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,752 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,753 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,766 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,769 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,989 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,991 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,140 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,142 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,178 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,184 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,397 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,399 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,444 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,447 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,901 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,902 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,678 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,684 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,733 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,735 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,961 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,962 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,158 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,161 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,438 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,443 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,653 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,654 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,784 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,789 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,980 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,985 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,944 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,946 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,971 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,975 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:03,117 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:03,119 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:04,481 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:04,483 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:06,145 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:06,147 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:09,710 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:09,713 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:22,883 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,885 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,890 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,892 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,895 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,897 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,898 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,899 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,900 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,902 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,900 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,906 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,907 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,909 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,916 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,918 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,918 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,920 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,920 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,922 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,923 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,924 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,925 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,926 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,935 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,938 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,943 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,945 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,948 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,948 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,950 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,950 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,951 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,953 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,955 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,957 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,958 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,959 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,394 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,394 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,396 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,397 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,405 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,408 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,495 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,497 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,498 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,500 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,501 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,504 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,510 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,512 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,513 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,515 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,516 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,518 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,532 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,534 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,539 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,541 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,548 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,550 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,785 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,785 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,787 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,787 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,801 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,803 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,827 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,831 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,832 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,833 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,837 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,839 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,844 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,846 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,854 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,856 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,856 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,858 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,888 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,888 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,890 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,890 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,893 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,895 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,899 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,901 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,019 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,017 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,021 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,022 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,033 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,033 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,035 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,035 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,076 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,078 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,079 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,084 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,086 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,089 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,234 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,236 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,239 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,241 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,333 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,335 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,404 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,406 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,436 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,438 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,442 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,443 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,443 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,444 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,445 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,445 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,453 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,454 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,456 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,458 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,458 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,459 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,502 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,504 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,510 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,512 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,513 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,515 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,583 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,583 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,585 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,588 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,587 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,595 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,670 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,672 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,683 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,685 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,690 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,688 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,692 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,693 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,757 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,757 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,759 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,759 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,759 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,761 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,760 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,765 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,849 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,851 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,852 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,855 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,896 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,898 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,930 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,932 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,938 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,940 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,946 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,948 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,965 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,967 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,985 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,989 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,017 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,023 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,042 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,047 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,051 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,052 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,054 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,054 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,065 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,066 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,067 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,068 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,322 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,324 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,495 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,498 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,551 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,553 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,633 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,635 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,658 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,660 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,756 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,758 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,852 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,854 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,906 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,911 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,013 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,015 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,043 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,049 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,126 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,128 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,252 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,254 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,994 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,999 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:29,327 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:29,333 - distributed.utils - INFO - Reload module qme_vars from .py file
