Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-10-01 11:26:33,882 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:45105'
2025-10-01 11:26:33,890 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:34881'
2025-10-01 11:26:33,895 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:42995'
2025-10-01 11:26:33,900 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:33903'
2025-10-01 11:26:33,904 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:45493'
2025-10-01 11:26:33,908 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:44507'
2025-10-01 11:26:33,912 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:38241'
2025-10-01 11:26:33,916 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:34287'
2025-10-01 11:26:33,924 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:45955'
2025-10-01 11:26:33,928 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:40165'
2025-10-01 11:26:33,933 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:37781'
2025-10-01 11:26:33,938 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:45957'
2025-10-01 11:26:33,942 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:43545'
2025-10-01 11:26:33,946 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:35051'
2025-10-01 11:26:33,951 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:38237'
2025-10-01 11:26:33,954 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:35599'
2025-10-01 11:26:33,962 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:36063'
2025-10-01 11:26:33,968 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:41197'
2025-10-01 11:26:33,972 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:44237'
2025-10-01 11:26:33,975 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:34149'
2025-10-01 11:26:34,049 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:39755'
2025-10-01 11:26:34,053 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:45683'
2025-10-01 11:26:34,058 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:43789'
2025-10-01 11:26:34,062 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:41983'
2025-10-01 11:26:34,066 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:36721'
2025-10-01 11:26:34,070 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:34533'
2025-10-01 11:26:34,075 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:42871'
2025-10-01 11:26:34,080 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:34835'
2025-10-01 11:26:34,083 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:41259'
2025-10-01 11:26:34,088 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:35483'
2025-10-01 11:26:34,092 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:41991'
2025-10-01 11:26:34,097 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:45565'
2025-10-01 11:26:34,101 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:39737'
2025-10-01 11:26:34,106 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:45641'
2025-10-01 11:26:34,111 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:44017'
2025-10-01 11:26:34,115 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:42959'
2025-10-01 11:26:34,119 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:41149'
2025-10-01 11:26:34,123 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:45531'
2025-10-01 11:26:34,126 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:35561'
2025-10-01 11:26:34,131 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:43589'
2025-10-01 11:26:34,134 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:34593'
2025-10-01 11:26:34,139 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:33925'
2025-10-01 11:26:34,145 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:44523'
2025-10-01 11:26:34,149 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:38129'
2025-10-01 11:26:34,153 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:42051'
2025-10-01 11:26:34,157 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:40575'
2025-10-01 11:26:34,162 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:35861'
2025-10-01 11:26:34,165 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:45727'
2025-10-01 11:26:34,170 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:43383'
2025-10-01 11:26:34,173 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:45685'
2025-10-01 11:26:34,177 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:35657'
2025-10-01 11:26:34,181 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.84.19:33301'
2025-10-01 11:26:35,158 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:45113
2025-10-01 11:26:35,158 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:36025
2025-10-01 11:26:35,158 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:41353
2025-10-01 11:26:35,158 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:36241
2025-10-01 11:26:35,158 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:37069
2025-10-01 11:26:35,158 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:45113
2025-10-01 11:26:35,158 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:36025
2025-10-01 11:26:35,158 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:41353
2025-10-01 11:26:35,158 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:36241
2025-10-01 11:26:35,158 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:37069
2025-10-01 11:26:35,158 - distributed.worker - INFO -          dashboard at:           10.6.84.19:43611
2025-10-01 11:26:35,158 - distributed.worker - INFO -          dashboard at:           10.6.84.19:41021
2025-10-01 11:26:35,158 - distributed.worker - INFO -          dashboard at:           10.6.84.19:37469
2025-10-01 11:26:35,158 - distributed.worker - INFO -          dashboard at:           10.6.84.19:42315
2025-10-01 11:26:35,158 - distributed.worker - INFO -          dashboard at:           10.6.84.19:38591
2025-10-01 11:26:35,158 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,158 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,158 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,158 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,158 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,158 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,158 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,158 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,158 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,158 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,158 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,158 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,158 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,158 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,158 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,158 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,158 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,158 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,158 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,158 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,158 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-a3pmigbo
2025-10-01 11:26:35,158 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-y8rbt1dm
2025-10-01 11:26:35,158 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-d5i5ncof
2025-10-01 11:26:35,158 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-w2a70hkm
2025-10-01 11:26:35,158 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-p9_dvdro
2025-10-01 11:26:35,158 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,158 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,158 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,158 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,158 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,177 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:39259
2025-10-01 11:26:35,177 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:39259
2025-10-01 11:26:35,177 - distributed.worker - INFO -          dashboard at:           10.6.84.19:42277
2025-10-01 11:26:35,177 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,177 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,177 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,177 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,177 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-k43p13al
2025-10-01 11:26:35,177 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,178 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:38661
2025-10-01 11:26:35,178 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:38661
2025-10-01 11:26:35,178 - distributed.worker - INFO -          dashboard at:           10.6.84.19:38083
2025-10-01 11:26:35,178 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,178 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,178 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,178 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,178 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-qvto_f2l
2025-10-01 11:26:35,178 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,185 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,186 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,186 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,188 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,188 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:33313
2025-10-01 11:26:35,188 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:33313
2025-10-01 11:26:35,188 - distributed.worker - INFO -          dashboard at:           10.6.84.19:43521
2025-10-01 11:26:35,189 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,189 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,189 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,189 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,189 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-3d2t_wti
2025-10-01 11:26:35,189 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,189 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:33327
2025-10-01 11:26:35,189 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:33327
2025-10-01 11:26:35,189 - distributed.worker - INFO -          dashboard at:           10.6.84.19:45723
2025-10-01 11:26:35,189 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,189 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,189 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,189 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,189 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-ar360y4c
2025-10-01 11:26:35,189 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,193 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,194 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,194 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,194 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:40877
2025-10-01 11:26:35,194 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:40877
2025-10-01 11:26:35,194 - distributed.worker - INFO -          dashboard at:           10.6.84.19:37879
2025-10-01 11:26:35,195 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,195 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,195 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,195 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,195 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-ib6t6op2
2025-10-01 11:26:35,195 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,196 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:34151
2025-10-01 11:26:35,196 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:34151
2025-10-01 11:26:35,196 - distributed.worker - INFO -          dashboard at:           10.6.84.19:34681
2025-10-01 11:26:35,196 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,196 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,196 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,196 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,196 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,196 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-itz5t3zm
2025-10-01 11:26:35,196 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,198 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,199 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,199 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,200 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:37277
2025-10-01 11:26:35,200 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:37277
2025-10-01 11:26:35,200 - distributed.worker - INFO -          dashboard at:           10.6.84.19:37629
2025-10-01 11:26:35,200 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,200 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,200 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,200 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,200 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-0hexvs36
2025-10-01 11:26:35,200 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,200 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:46261
2025-10-01 11:26:35,200 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:46261
2025-10-01 11:26:35,200 - distributed.worker - INFO -          dashboard at:           10.6.84.19:35795
2025-10-01 11:26:35,200 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,200 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,200 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,200 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,201 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-3dtpcmci
2025-10-01 11:26:35,201 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,201 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,201 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,202 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,203 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,204 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,205 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,206 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:44343
2025-10-01 11:26:35,206 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:44343
2025-10-01 11:26:35,206 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,206 - distributed.worker - INFO -          dashboard at:           10.6.84.19:38699
2025-10-01 11:26:35,206 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,206 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,206 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,206 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,206 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,206 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-ma2fjdxz
2025-10-01 11:26:35,206 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,208 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,208 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,208 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,208 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,209 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,211 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,211 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:40407
2025-10-01 11:26:35,211 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:40407
2025-10-01 11:26:35,211 - distributed.worker - INFO -          dashboard at:           10.6.84.19:35953
2025-10-01 11:26:35,211 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,211 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,211 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,211 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,211 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-hp71uwaq
2025-10-01 11:26:35,211 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,211 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,211 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,211 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:34467
2025-10-01 11:26:35,212 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:34467
2025-10-01 11:26:35,212 - distributed.worker - INFO -          dashboard at:           10.6.84.19:35455
2025-10-01 11:26:35,212 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,212 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,212 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,212 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,212 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-pzqya9ko
2025-10-01 11:26:35,212 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,212 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:42005
2025-10-01 11:26:35,212 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:42005
2025-10-01 11:26:35,212 - distributed.worker - INFO -          dashboard at:           10.6.84.19:35771
2025-10-01 11:26:35,212 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,212 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,212 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,212 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,212 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-ck4ajj02
2025-10-01 11:26:35,212 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,212 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,220 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:41151
2025-10-01 11:26:35,220 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:41151
2025-10-01 11:26:35,220 - distributed.worker - INFO -          dashboard at:           10.6.84.19:43067
2025-10-01 11:26:35,220 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,220 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,220 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,220 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,220 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-48fc03q2
2025-10-01 11:26:35,220 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,222 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:36175
2025-10-01 11:26:35,223 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:36175
2025-10-01 11:26:35,223 - distributed.worker - INFO -          dashboard at:           10.6.84.19:34499
2025-10-01 11:26:35,223 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,223 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,223 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,223 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,223 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-rxavkha0
2025-10-01 11:26:35,223 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,228 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:32991
2025-10-01 11:26:35,228 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:32991
2025-10-01 11:26:35,228 - distributed.worker - INFO -          dashboard at:           10.6.84.19:43837
2025-10-01 11:26:35,228 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,228 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,228 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,228 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,228 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-dw47isq2
2025-10-01 11:26:35,228 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,267 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:43311
2025-10-01 11:26:35,267 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:43311
2025-10-01 11:26:35,267 - distributed.worker - INFO -          dashboard at:           10.6.84.19:32995
2025-10-01 11:26:35,267 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,267 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,267 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,267 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,267 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-_ipf9lpl
2025-10-01 11:26:35,267 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,288 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,288 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,288 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,289 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,293 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,293 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,293 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,295 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,296 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,297 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,299 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,299 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:42713
2025-10-01 11:26:35,299 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:42713
2025-10-01 11:26:35,299 - distributed.worker - INFO -          dashboard at:           10.6.84.19:41167
2025-10-01 11:26:35,299 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,300 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,300 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,300 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,300 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-cnrojcv2
2025-10-01 11:26:35,300 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,300 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,302 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,302 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,303 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,304 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,305 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,305 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,307 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,307 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,308 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,308 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,309 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,310 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,310 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,310 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,311 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,312 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,313 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,313 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,314 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,314 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,315 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,315 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,316 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,317 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,317 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,317 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,318 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,319 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,319 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,319 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,320 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,321 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,321 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,321 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,323 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,328 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,329 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,330 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,331 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,343 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,344 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,344 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,344 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,345 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,345 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,346 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,346 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,486 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:33277
2025-10-01 11:26:35,486 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:33277
2025-10-01 11:26:35,487 - distributed.worker - INFO -          dashboard at:           10.6.84.19:33907
2025-10-01 11:26:35,487 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,487 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,487 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,487 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,487 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-cs_w5srr
2025-10-01 11:26:35,487 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,512 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:36529
2025-10-01 11:26:35,513 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:36529
2025-10-01 11:26:35,513 - distributed.worker - INFO -          dashboard at:           10.6.84.19:45897
2025-10-01 11:26:35,513 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,513 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,513 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,513 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,513 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-assiyb08
2025-10-01 11:26:35,513 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,513 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,514 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,514 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,516 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,557 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:36017
2025-10-01 11:26:35,558 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:36017
2025-10-01 11:26:35,558 - distributed.worker - INFO -          dashboard at:           10.6.84.19:44109
2025-10-01 11:26:35,558 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,558 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,558 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,558 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,558 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-0y4gqnaj
2025-10-01 11:26:35,558 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,561 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:34017
2025-10-01 11:26:35,561 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:34017
2025-10-01 11:26:35,561 - distributed.worker - INFO -          dashboard at:           10.6.84.19:38153
2025-10-01 11:26:35,561 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,561 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,561 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,561 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,561 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-khaplu_m
2025-10-01 11:26:35,561 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,569 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,570 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,570 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,572 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,581 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:39095
2025-10-01 11:26:35,581 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:39095
2025-10-01 11:26:35,581 - distributed.worker - INFO -          dashboard at:           10.6.84.19:36711
2025-10-01 11:26:35,581 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,581 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,581 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,581 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,581 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-sud9uru5
2025-10-01 11:26:35,581 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,592 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:36693
2025-10-01 11:26:35,592 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:36693
2025-10-01 11:26:35,592 - distributed.worker - INFO -          dashboard at:           10.6.84.19:34895
2025-10-01 11:26:35,592 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,592 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,592 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,592 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,592 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-ewsasui0
2025-10-01 11:26:35,592 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,592 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:41513
2025-10-01 11:26:35,592 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:41513
2025-10-01 11:26:35,592 - distributed.worker - INFO -          dashboard at:           10.6.84.19:43957
2025-10-01 11:26:35,592 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,592 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,592 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,592 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,592 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-b0phr_q2
2025-10-01 11:26:35,593 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,594 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:39331
2025-10-01 11:26:35,594 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:39331
2025-10-01 11:26:35,594 - distributed.worker - INFO -          dashboard at:           10.6.84.19:44443
2025-10-01 11:26:35,594 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,594 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,594 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,594 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,594 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-qv0sq3u9
2025-10-01 11:26:35,594 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,595 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,596 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,596 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,597 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,597 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,598 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,598 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,600 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,601 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:42953
2025-10-01 11:26:35,601 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:42953
2025-10-01 11:26:35,601 - distributed.worker - INFO -          dashboard at:           10.6.84.19:39249
2025-10-01 11:26:35,601 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,601 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,601 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,601 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,601 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-pdzrc7mb
2025-10-01 11:26:35,601 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,601 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,602 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,602 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,603 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,606 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,607 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,607 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,607 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,608 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:45711
2025-10-01 11:26:35,608 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:45711
2025-10-01 11:26:35,608 - distributed.worker - INFO -          dashboard at:           10.6.84.19:36985
2025-10-01 11:26:35,608 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,608 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,608 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,608 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,608 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-6vosvp6j
2025-10-01 11:26:35,608 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,615 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:38005
2025-10-01 11:26:35,615 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:38005
2025-10-01 11:26:35,615 - distributed.worker - INFO -          dashboard at:           10.6.84.19:35939
2025-10-01 11:26:35,615 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,615 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,615 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,615 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,615 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-az9h1flm
2025-10-01 11:26:35,615 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,617 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,617 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,618 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,618 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:44001
2025-10-01 11:26:35,618 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:44001
2025-10-01 11:26:35,618 - distributed.worker - INFO -          dashboard at:           10.6.84.19:40731
2025-10-01 11:26:35,618 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,618 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,618 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,618 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,618 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-4fp8bxdl
2025-10-01 11:26:35,618 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,619 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,619 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,620 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:34293
2025-10-01 11:26:35,620 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:34293
2025-10-01 11:26:35,620 - distributed.worker - INFO -          dashboard at:           10.6.84.19:36657
2025-10-01 11:26:35,620 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,620 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,620 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,620 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,620 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-r2hxs3p6
2025-10-01 11:26:35,620 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,620 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,621 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,622 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,622 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:40229
2025-10-01 11:26:35,622 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:40229
2025-10-01 11:26:35,622 - distributed.worker - INFO -          dashboard at:           10.6.84.19:41919
2025-10-01 11:26:35,622 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,622 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,622 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,622 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,622 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-dvel9xai
2025-10-01 11:26:35,622 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,623 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:38597
2025-10-01 11:26:35,623 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:38597
2025-10-01 11:26:35,623 - distributed.worker - INFO -          dashboard at:           10.6.84.19:32829
2025-10-01 11:26:35,623 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,623 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,623 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,623 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,623 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-29iv6bmb
2025-10-01 11:26:35,623 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,623 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,624 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,625 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,625 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:40287
2025-10-01 11:26:35,625 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:40287
2025-10-01 11:26:35,625 - distributed.worker - INFO -          dashboard at:           10.6.84.19:44679
2025-10-01 11:26:35,626 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,626 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,626 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,626 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,626 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-cjbvuyhx
2025-10-01 11:26:35,626 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,626 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:46439
2025-10-01 11:26:35,626 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,626 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:46439
2025-10-01 11:26:35,626 - distributed.worker - INFO -          dashboard at:           10.6.84.19:35689
2025-10-01 11:26:35,626 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,626 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,626 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,626 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,626 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-xy7yr_u0
2025-10-01 11:26:35,626 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,628 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:43281
2025-10-01 11:26:35,628 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:43281
2025-10-01 11:26:35,628 - distributed.worker - INFO -          dashboard at:           10.6.84.19:45409
2025-10-01 11:26:35,628 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,628 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,628 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,628 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,628 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,628 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-om62y8iy
2025-10-01 11:26:35,628 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,628 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:45501
2025-10-01 11:26:35,628 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:45501
2025-10-01 11:26:35,628 - distributed.worker - INFO -          dashboard at:           10.6.84.19:35705
2025-10-01 11:26:35,629 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,629 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,629 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,629 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,629 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-vaf1iki8
2025-10-01 11:26:35,629 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,629 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,629 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,631 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,632 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:34701
2025-10-01 11:26:35,632 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:34701
2025-10-01 11:26:35,632 - distributed.worker - INFO -          dashboard at:           10.6.84.19:36911
2025-10-01 11:26:35,632 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,632 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,632 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,632 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,632 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-leepc14j
2025-10-01 11:26:35,632 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,633 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:45503
2025-10-01 11:26:35,634 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:45503
2025-10-01 11:26:35,634 - distributed.worker - INFO -          dashboard at:           10.6.84.19:37991
2025-10-01 11:26:35,634 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,634 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:44657
2025-10-01 11:26:35,634 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,634 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:44657
2025-10-01 11:26:35,634 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,634 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,634 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,634 - distributed.worker - INFO -          dashboard at:           10.6.84.19:37951
2025-10-01 11:26:35,634 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,634 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-3y0y2g8b
2025-10-01 11:26:35,634 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,634 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,634 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,634 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,634 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-is83t3yr
2025-10-01 11:26:35,634 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,634 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,634 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,635 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,636 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,636 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,636 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,637 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,637 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:39997
2025-10-01 11:26:35,637 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:39997
2025-10-01 11:26:35,637 - distributed.worker - INFO -          dashboard at:           10.6.84.19:46291
2025-10-01 11:26:35,637 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:32835
2025-10-01 11:26:35,637 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,638 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,638 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:32835
2025-10-01 11:26:35,638 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,638 - distributed.worker - INFO -          dashboard at:           10.6.84.19:46575
2025-10-01 11:26:35,638 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,638 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,638 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-3scdrrip
2025-10-01 11:26:35,638 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,638 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,638 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,638 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,638 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-8safn24s
2025-10-01 11:26:35,638 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,638 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:33745
2025-10-01 11:26:35,638 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:33745
2025-10-01 11:26:35,638 - distributed.worker - INFO -          dashboard at:           10.6.84.19:33167
2025-10-01 11:26:35,638 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,638 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,638 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,638 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,638 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-4u1b3ag1
2025-10-01 11:26:35,638 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,639 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,639 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,640 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,641 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,641 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,641 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:34679
2025-10-01 11:26:35,642 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:34679
2025-10-01 11:26:35,642 - distributed.worker - INFO -          dashboard at:           10.6.84.19:37737
2025-10-01 11:26:35,642 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,642 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,642 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,642 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,642 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-fzkfghtz
2025-10-01 11:26:35,642 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,642 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,642 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,642 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,642 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,642 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,643 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,643 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,644 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,644 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,644 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,644 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,647 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,648 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,648 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,648 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,648 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,648 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,649 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,649 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:36177
2025-10-01 11:26:35,649 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:36177
2025-10-01 11:26:35,649 - distributed.worker - INFO -          dashboard at:           10.6.84.19:34747
2025-10-01 11:26:35,649 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,649 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,649 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,649 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,649 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-83ata_di
2025-10-01 11:26:35,650 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,649 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,650 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,651 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,651 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,652 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,652 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,653 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,653 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,654 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:46409
2025-10-01 11:26:35,654 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:46409
2025-10-01 11:26:35,654 - distributed.worker - INFO -          dashboard at:           10.6.84.19:33563
2025-10-01 11:26:35,654 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,654 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,654 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,654 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,654 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-3hcukttp
2025-10-01 11:26:35,654 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,655 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,655 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,655 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,655 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,656 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,656 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,656 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,657 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:37889
2025-10-01 11:26:35,657 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:37889
2025-10-01 11:26:35,657 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,657 - distributed.worker - INFO -          dashboard at:           10.6.84.19:33401
2025-10-01 11:26:35,657 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,657 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,657 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,657 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,657 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-_95xwg7n
2025-10-01 11:26:35,657 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,657 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,658 - distributed.worker - INFO -       Start worker at:     tcp://10.6.84.19:41763
2025-10-01 11:26:35,658 - distributed.worker - INFO -          Listening to:     tcp://10.6.84.19:41763
2025-10-01 11:26:35,658 - distributed.worker - INFO -          dashboard at:           10.6.84.19:44807
2025-10-01 11:26:35,658 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,658 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,658 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:26:35,658 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:26:35,658 - distributed.worker - INFO -       Local Directory: /jobfs/151330090.gadi-pbs/dask-scratch-space/worker-9jedni6u
2025-10-01 11:26:35,658 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,658 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,659 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,659 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,660 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,660 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,661 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,661 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,661 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,663 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,664 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,664 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,665 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,665 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,666 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,666 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,667 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,668 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,668 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,668 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,669 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,670 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,671 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,671 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,671 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,671 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,671 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,672 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,672 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:35,676 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:26:35,677 - distributed.worker - INFO -         Registered to:       tcp://10.6.84.1:8707
2025-10-01 11:26:35,677 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:26:35,679 - distributed.core - INFO - Starting established connection to tcp://10.6.84.1:8707
2025-10-01 11:26:51,923 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,923 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,923 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,923 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,923 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,923 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,923 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,924 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,924 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,924 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,924 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,924 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,924 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,924 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,924 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,925 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,925 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,925 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,925 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,925 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,925 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,925 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,925 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,926 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,926 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,926 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,926 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,926 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,926 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,926 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,926 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,926 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,926 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,926 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,926 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,927 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,927 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,927 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,927 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,927 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,927 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,927 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,927 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,927 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,927 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,927 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,927 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,927 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,926 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,927 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,928 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,928 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,928 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,928 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,928 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,928 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,928 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,928 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,928 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,928 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,928 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,928 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,928 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,927 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,928 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,928 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,927 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,929 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,929 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,929 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,929 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,927 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,929 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,930 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,930 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,928 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,930 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,930 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,930 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,930 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,930 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,930 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,931 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,929 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,931 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,931 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,931 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,932 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,932 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,931 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,930 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,931 - distributed.worker - INFO - Starting Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 11:26:51,929 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,934 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,936 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,937 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,937 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,937 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,937 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,938 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,938 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,940 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,940 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:51,940 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-01 11:26:54,256 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,256 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,256 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,256 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,256 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,257 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,257 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,257 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,257 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,257 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,257 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,259 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,259 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,259 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,259 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,259 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,259 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,259 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,259 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,259 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,259 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,259 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,259 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,259 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,259 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,260 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,259 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,259 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,260 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,260 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,260 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,260 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,260 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,260 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,260 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,260 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,260 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,260 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,260 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,260 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,260 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,260 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,260 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,261 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,261 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,261 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,261 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,262 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,262 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,261 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,261 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,262 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,262 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,262 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,262 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,262 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,261 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,262 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,263 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,263 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,260 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,260 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,263 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,263 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,263 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,263 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,263 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,263 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,261 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,263 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,263 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,264 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,264 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,261 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,264 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,264 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,264 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,264 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,264 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,265 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,262 - distributed.worker - INFO - Starting Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 11:26:54,265 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,266 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,266 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,267 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-01 11:26:54,662 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,662 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,663 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,663 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,663 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,663 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,663 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,663 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,663 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,663 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,664 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,664 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,664 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,664 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,664 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,664 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,664 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,664 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,665 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,665 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,665 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,665 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,665 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,665 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,665 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,665 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,665 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,666 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,666 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,666 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,666 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,666 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,666 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,666 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,666 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,666 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,666 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,666 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,667 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,667 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,667 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,667 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,667 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,667 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,667 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,667 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,668 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,668 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,668 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,668 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,668 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,668 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,668 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,668 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,668 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,668 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,668 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,668 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,668 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,669 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,669 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,669 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,669 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,669 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,668 - distributed.worker - INFO - Starting Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 11:26:54,669 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,669 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,669 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,669 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,669 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,669 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,669 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,670 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,670 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,670 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,670 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,670 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,670 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,670 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,670 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,670 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,671 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,671 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,671 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,671 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,671 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,672 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,672 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,672 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,672 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,672 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,672 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,672 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,672 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,672 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,672 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,672 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,673 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,673 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,673 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,673 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,673 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,674 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:54,674 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-01 11:26:55,061 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,061 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,061 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,061 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,062 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,062 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,062 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,062 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,062 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,062 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,062 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,062 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,062 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,062 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,063 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,063 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,063 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,063 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,063 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,063 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,063 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,063 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,064 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,064 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,064 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,063 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,064 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,064 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,064 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,064 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,064 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,064 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,064 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,064 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,064 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,064 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,065 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,064 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,064 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,065 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,064 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,064 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,065 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,065 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,064 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,065 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,065 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,065 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,065 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,065 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,065 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,065 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,066 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,065 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,065 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,065 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,065 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,065 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,065 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,065 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,066 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,066 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,066 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,066 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,066 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,066 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,066 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,066 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,066 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,066 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,066 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,066 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,066 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,066 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,067 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,067 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,067 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,067 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,067 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,067 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,067 - distributed.worker - INFO - Starting Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 11:26:55,068 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,068 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,068 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,068 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,068 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,069 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,069 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,069 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,069 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,069 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,069 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,069 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,070 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,070 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,070 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,070 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,070 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,070 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,070 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,071 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,071 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,073 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 11:26:55,076 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-01 17:26:19,576 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,577 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,577 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,577 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,577 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,577 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,577 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,577 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,577 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,577 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,577 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,577 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:46409. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,577 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,577 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,577 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,577 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,578 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:40407. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,578 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:32991. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,578 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:40877. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,578 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,578 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:37277. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,578 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:34151. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,578 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:44343. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,578 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:33313. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,578 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,579 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:42005. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,579 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:41353. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,578 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,579 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:45113. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,579 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:46261. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,576 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,579 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:38661. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,577 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,577 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,577 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,580 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:34679. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,580 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:39259. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,578 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,580 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:32835. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,580 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:36529. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,580 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:36175. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,580 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,580 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,580 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,580 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:36177. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,580 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,580 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,580 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:36241. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,578 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,579 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,581 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:36025. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,581 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:34017. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,580 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,578 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,577 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,581 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:40287. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,578 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,581 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:40229. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,579 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,581 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,581 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,581 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:34293. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,577 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,581 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,582 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:45711. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,582 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:43281. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,582 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:33745. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,582 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:36017. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,582 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:38597. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,582 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:41151. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,580 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,582 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:44657. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,582 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:37069. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,583 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:45503. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,580 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,580 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,580 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,580 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,583 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:36693. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,580 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,583 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:34701. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,583 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:39095. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,581 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,583 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:34467. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,583 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:33277. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,577 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,584 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:44001. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,584 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:45501. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,584 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:42953. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,584 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:46439. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,584 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:38005. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,585 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:39331. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,577 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.84.19:45466 remote=tcp://10.6.84.1:8707>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:26:19,576 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.84.19:45476 remote=tcp://10.6.84.1:8707>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:26:19,585 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,585 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,585 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:33327. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,585 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:43311. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,577 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.84.19:45472 remote=tcp://10.6.84.1:8707>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:26:19,586 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:37889. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,579 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,586 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:41513. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,586 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,587 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:42713. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,588 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:39997. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,582 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.84.19:45422 remote=tcp://10.6.84.1:8707>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:26:19,582 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.84.19:45382 remote=tcp://10.6.84.1:8707>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:26:19,577 - distributed.core - INFO - Connection to tcp://10.6.84.1:8707 has been closed.
2025-10-01 17:26:19,582 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.84.19:45436 remote=tcp://10.6.84.1:8707>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:26:19,582 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.84.19:45380 remote=tcp://10.6.84.1:8707>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:26:19,582 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.84.19:45392 remote=tcp://10.6.84.1:8707>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:26:19,582 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.84.19:45394 remote=tcp://10.6.84.1:8707>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:26:19,582 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.84.19:45408 remote=tcp://10.6.84.1:8707>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:26:19,582 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.84.19:45320 remote=tcp://10.6.84.1:8707>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:26:19,582 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.84.19:45406 remote=tcp://10.6.84.1:8707>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:26:19,583 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.84.19:45304 remote=tcp://10.6.84.1:8707>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:26:19,590 - distributed.worker - INFO - Stopping worker at tcp://10.6.84.19:41763. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,582 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.84.19:45462 remote=tcp://10.6.84.1:8707>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:26:19,583 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.84.19:45366 remote=tcp://10.6.84.1:8707>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:26:19,585 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.84.19:45336 remote=tcp://10.6.84.1:8707>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:26:19,584 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.84.19:45356 remote=tcp://10.6.84.1:8707>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:26:19,584 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.84.19:45344 remote=tcp://10.6.84.1:8707>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:26:19,586 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.84.19:45348 remote=tcp://10.6.84.1:8707>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:26:19,587 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.84.19:45424 remote=tcp://10.6.84.1:8707>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:26:19,587 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.84.19:45460 remote=tcp://10.6.84.1:8707>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:26:19,590 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.84.19:45446 remote=tcp://10.6.84.1:8707>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:26:19,598 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:35861'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,600 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,601 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,601 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,601 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,601 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,607 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,609 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:35051'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,610 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:38237'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,610 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:43545'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,610 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:45683'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,611 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,611 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,611 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:41197'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,611 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,611 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:35599'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,611 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,611 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,611 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,611 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:44237'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,611 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,611 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,611 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,612 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,612 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,612 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:45493'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,612 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,612 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,612 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,612 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,612 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,612 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,612 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,612 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:44507'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,612 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,612 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,612 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,612 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,612 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:40165'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,612 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,612 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,612 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,612 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,612 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,613 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,613 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:45105'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,613 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,613 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,613 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,613 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,613 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,613 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:45955'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,613 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,613 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,613 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,613 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,613 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:45685'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,613 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,613 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,613 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,613 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,613 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,613 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,613 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:45565'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,613 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,614 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,614 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,614 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,614 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,614 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:39755'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,614 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,614 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,614 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,614 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,614 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,614 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:34533'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,614 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,614 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,614 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,614 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,614 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:42995'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,614 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,614 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,614 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,615 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,614 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,615 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,615 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:45957'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,615 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,615 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,615 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,615 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,615 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,615 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:34881'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,615 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,615 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,615 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,615 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,615 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:40575'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,615 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,615 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,615 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,616 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,616 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,616 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,616 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,616 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,616 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,616 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,616 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,616 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,616 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,616 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,616 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,616 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,616 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,616 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,616 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,616 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,616 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,617 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,617 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,617 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,617 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,617 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,617 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,617 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,618 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,618 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,619 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,619 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,619 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,619 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,620 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,620 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,620 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,621 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,621 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,621 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,622 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,622 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,622 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,623 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,623 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:35483'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,623 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,623 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,623 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:34593'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,624 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:35561'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,624 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,624 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:41149'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,624 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,624 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:42051'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,624 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,624 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:36721'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,624 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,624 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,625 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:41259'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,625 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,625 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,625 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:45727'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,625 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,625 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,625 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,625 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,625 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:39737'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,625 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,625 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,625 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,625 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,625 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:38241'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,625 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,625 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,625 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,626 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:33903'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,626 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:34835'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,626 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:43589'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,626 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:42959'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,626 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,627 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:44017'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,627 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,627 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,627 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,627 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,627 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:35657'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,627 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,627 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,627 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,627 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,627 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:43789'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,627 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,627 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,627 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,627 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,627 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,627 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,627 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,627 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:33925'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,627 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,627 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,627 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,628 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:36063'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,627 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,628 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,628 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,628 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,628 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:37781'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,628 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,628 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,628 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,628 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,628 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,628 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:41991'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,628 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,628 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,628 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,628 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,628 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,628 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:45641'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,628 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,628 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,628 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,628 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,628 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,628 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,628 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,628 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,628 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:33301'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,629 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,629 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,629 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,629 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,629 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,629 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,629 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,629 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:34149'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,629 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,629 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,629 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,629 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,629 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,629 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,629 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:45531'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,629 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,629 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,629 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,629 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,629 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,629 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,629 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:41983'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,630 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,630 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,630 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,630 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,630 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,630 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,630 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,630 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:34287'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,630 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,630 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,630 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,630 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,630 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:43383'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,630 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,630 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,630 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,630 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:44523'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,630 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,630 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,630 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,630 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:42871'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,630 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,630 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,630 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,631 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,631 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,631 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,631 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,631 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,631 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,631 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,631 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,631 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,631 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,631 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,632 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,632 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,632 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,632 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,632 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,632 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,632 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,632 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,632 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,632 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,632 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.84.19:38129'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:26:19,632 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,633 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,633 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,633 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,633 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,633 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,633 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,633 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,633 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,633 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,633 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,633 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,633 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:26:19,634 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,634 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,634 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,634 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,634 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,634 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,634 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,634 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,635 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,635 - distributed.worker - INFO - Removing Worker plugin qme_utils.py09885cb1-e9be-4fc3-ac9e-51e449096f1a
2025-10-01 17:26:19,635 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,636 - distributed.worker - INFO - Removing Worker plugin qme_vars.pycc3a558a-d414-4877-958f-dcc94964550c
2025-10-01 17:26:19,636 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,636 - distributed.worker - INFO - Removing Worker plugin qme_train.pyd1b46f2c-cb4e-467a-8a21-212e03c56830
2025-10-01 17:26:19,636 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,636 - distributed.worker - INFO - Removing Worker plugin qme_apply.pya5db5a44-6f68-498c-9fd0-f4f67f585ec5
2025-10-01 17:26:19,636 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,636 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,637 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,637 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,638 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,638 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,638 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,638 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,638 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,631 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-01 17:26:19,639 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,640 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,640 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,641 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,645 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,645 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,646 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,647 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:19,648 - distributed.nanny - INFO - Worker closed
2025-10-01 17:26:21,612 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,623 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,623 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,623 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,624 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,624 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,624 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,625 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,625 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,626 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,626 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,626 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,627 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,627 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,627 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,627 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,628 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,628 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,628 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,630 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,636 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,636 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,637 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,637 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,638 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,638 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,638 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,639 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,640 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,640 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,640 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,640 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,640 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,640 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,640 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,641 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,642 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,643 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,643 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,643 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,643 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,645 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,645 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,645 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,645 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,651 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,653 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,654 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,655 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:21,656 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:26:22,039 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:45493'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,044 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:34881'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,048 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:42051'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,049 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:34149'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,052 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:38237'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,059 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:45685'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,061 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:35483'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,062 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:35861'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,063 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:35051'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,063 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:34533'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,064 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:45565'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,065 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:39737'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,065 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:45727'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,066 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:45105'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,067 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:45493' closed.
2025-10-01 17:26:22,070 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:36063'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,071 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:41197'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,071 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:43789'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,071 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:44507'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,073 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:34881' closed.
2025-10-01 17:26:22,074 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:42051' closed.
2025-10-01 17:26:22,074 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:34149' closed.
2025-10-01 17:26:22,074 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:43545'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,076 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:38237' closed.
2025-10-01 17:26:22,076 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:45683'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,076 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:42959'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,079 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:45685' closed.
2025-10-01 17:26:22,080 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:36721'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,083 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:45641'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,083 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:35483' closed.
2025-10-01 17:26:22,083 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:35861' closed.
2025-10-01 17:26:22,084 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:35051' closed.
2025-10-01 17:26:22,084 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:34533' closed.
2025-10-01 17:26:22,084 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:44017'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,085 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:45565' closed.
2025-10-01 17:26:22,085 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:39737' closed.
2025-10-01 17:26:22,085 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:45727' closed.
2025-10-01 17:26:22,085 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:45105' closed.
2025-10-01 17:26:22,085 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:34593'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,086 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:39755'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,087 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:40575'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,087 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:36063' closed.
2025-10-01 17:26:22,087 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:41197' closed.
2025-10-01 17:26:22,087 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:43789' closed.
2025-10-01 17:26:22,087 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:44507' closed.
2025-10-01 17:26:22,093 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:40165'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,094 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:43545' closed.
2025-10-01 17:26:22,094 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:45683' closed.
2025-10-01 17:26:22,094 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:42959' closed.
2025-10-01 17:26:22,096 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:36721' closed.
2025-10-01 17:26:22,096 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:44237'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,096 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:35657'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,096 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:42995'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,097 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:35561'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,097 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:35599'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,098 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:41149'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,098 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:45641' closed.
2025-10-01 17:26:22,098 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:34835'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,099 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:44017' closed.
2025-10-01 17:26:22,099 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:34593' closed.
2025-10-01 17:26:22,100 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:39755' closed.
2025-10-01 17:26:22,100 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:40575' closed.
2025-10-01 17:26:22,100 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:33903'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,100 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:40165' closed.
2025-10-01 17:26:22,102 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:44237' closed.
2025-10-01 17:26:22,102 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:35657' closed.
2025-10-01 17:26:22,102 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:42995' closed.
2025-10-01 17:26:22,102 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:35561' closed.
2025-10-01 17:26:22,103 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:35599' closed.
2025-10-01 17:26:22,103 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:41149' closed.
2025-10-01 17:26:22,103 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:34835' closed.
2025-10-01 17:26:22,103 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:44523'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,103 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:33903' closed.
2025-10-01 17:26:22,104 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:44523' closed.
2025-10-01 17:26:22,107 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:45957'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,107 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:45957' closed.
2025-10-01 17:26:22,119 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:45955'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,120 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:34287'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,121 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:45955' closed.
2025-10-01 17:26:22,123 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:34287' closed.
2025-10-01 17:26:22,131 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:43383'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,132 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:43383' closed.
2025-10-01 17:26:22,135 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:45531'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,136 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:45531' closed.
2025-10-01 17:26:22,137 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:43589'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,138 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:43589' closed.
2025-10-01 17:26:22,145 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:38129'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,145 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:38129' closed.
2025-10-01 17:26:22,151 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:42871'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,152 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:42871' closed.
2025-10-01 17:26:22,191 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:33925'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,192 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:33925' closed.
2025-10-01 17:26:22,206 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:38241'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,207 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:41259'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,208 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:41991'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,208 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:38241' closed.
2025-10-01 17:26:22,209 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:41259' closed.
2025-10-01 17:26:22,209 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:41991' closed.
2025-10-01 17:26:22,216 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:33301'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,217 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:33301' closed.
2025-10-01 17:26:22,223 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:37781'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,224 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:37781' closed.
2025-10-01 17:26:22,262 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.84.19:41983'. Reason: nanny-close-gracefully
2025-10-01 17:26:22,263 - distributed.nanny - INFO - Nanny at 'tcp://10.6.84.19:41983' closed.
2025-10-01 17:26:22,265 - distributed.dask_worker - INFO - End worker
