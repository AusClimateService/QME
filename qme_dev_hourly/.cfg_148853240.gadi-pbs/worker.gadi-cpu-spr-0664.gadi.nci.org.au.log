Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-05 09:25:25,251 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:39473'
2025-09-05 09:25:25,261 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:44991'
2025-09-05 09:25:25,265 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:39891'
2025-09-05 09:25:25,269 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:40223'
2025-09-05 09:25:25,273 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:35619'
2025-09-05 09:25:25,277 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:42095'
2025-09-05 09:25:25,280 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:41187'
2025-09-05 09:25:25,284 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:34793'
2025-09-05 09:25:25,289 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:39575'
2025-09-05 09:25:25,294 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:43111'
2025-09-05 09:25:25,298 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:35199'
2025-09-05 09:25:25,302 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:39619'
2025-09-05 09:25:25,305 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:34331'
2025-09-05 09:25:25,309 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:43261'
2025-09-05 09:25:25,311 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:39917'
2025-09-05 09:25:25,314 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:40071'
2025-09-05 09:25:25,319 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:34159'
2025-09-05 09:25:25,323 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:41471'
2025-09-05 09:25:25,327 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:35643'
2025-09-05 09:25:25,331 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:37253'
2025-09-05 09:25:25,432 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:39633'
2025-09-05 09:25:25,437 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:32813'
2025-09-05 09:25:25,441 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:35183'
2025-09-05 09:25:25,445 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:39139'
2025-09-05 09:25:25,448 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:34867'
2025-09-05 09:25:25,452 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:34695'
2025-09-05 09:25:25,457 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:37345'
2025-09-05 09:25:25,462 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:32983'
2025-09-05 09:25:25,466 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:37803'
2025-09-05 09:25:25,468 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:44177'
2025-09-05 09:25:25,472 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:42863'
2025-09-05 09:25:25,476 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:39271'
2025-09-05 09:25:25,480 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:37169'
2025-09-05 09:25:25,484 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:34399'
2025-09-05 09:25:25,487 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:40153'
2025-09-05 09:25:25,492 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:46825'
2025-09-05 09:25:25,495 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:41399'
2025-09-05 09:25:25,498 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:39627'
2025-09-05 09:25:25,502 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:34671'
2025-09-05 09:25:25,507 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:36259'
2025-09-05 09:25:25,509 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:44381'
2025-09-05 09:25:25,514 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:43875'
2025-09-05 09:25:25,518 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:34817'
2025-09-05 09:25:25,523 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:36455'
2025-09-05 09:25:25,528 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:44183'
2025-09-05 09:25:25,533 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:46357'
2025-09-05 09:25:25,539 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:35521'
2025-09-05 09:25:25,543 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:36591'
2025-09-05 09:25:25,546 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:45701'
2025-09-05 09:25:25,550 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:37937'
2025-09-05 09:25:25,554 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:35743'
2025-09-05 09:25:25,557 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:46751'
2025-09-05 09:25:26,463 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:35737
2025-09-05 09:25:26,463 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:36007
2025-09-05 09:25:26,463 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:37109
2025-09-05 09:25:26,463 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:42129
2025-09-05 09:25:26,463 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:46517
2025-09-05 09:25:26,463 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:44383
2025-09-05 09:25:26,463 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:39013
2025-09-05 09:25:26,463 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:35329
2025-09-05 09:25:26,463 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:34011
2025-09-05 09:25:26,463 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:38983
2025-09-05 09:25:26,463 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:40371
2025-09-05 09:25:26,463 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:39749
2025-09-05 09:25:26,463 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:46673
2025-09-05 09:25:26,463 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:33565
2025-09-05 09:25:26,463 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:35737
2025-09-05 09:25:26,463 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:37111
2025-09-05 09:25:26,463 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:44035
2025-09-05 09:25:26,463 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:36007
2025-09-05 09:25:26,463 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:37109
2025-09-05 09:25:26,463 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:42129
2025-09-05 09:25:26,463 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:46517
2025-09-05 09:25:26,463 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:34527
2025-09-05 09:25:26,463 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:44383
2025-09-05 09:25:26,464 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:39013
2025-09-05 09:25:26,464 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:35329
2025-09-05 09:25:26,464 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:34011
2025-09-05 09:25:26,464 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:38983
2025-09-05 09:25:26,464 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:40371
2025-09-05 09:25:26,464 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:39749
2025-09-05 09:25:26,464 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:46673
2025-09-05 09:25:26,464 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:33565
2025-09-05 09:25:26,464 - distributed.worker - INFO -          dashboard at:          10.6.105.16:38795
2025-09-05 09:25:26,464 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:37111
2025-09-05 09:25:26,464 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:44035
2025-09-05 09:25:26,464 - distributed.worker - INFO -          dashboard at:          10.6.105.16:45363
2025-09-05 09:25:26,464 - distributed.worker - INFO -          dashboard at:          10.6.105.16:43061
2025-09-05 09:25:26,464 - distributed.worker - INFO -          dashboard at:          10.6.105.16:42293
2025-09-05 09:25:26,464 - distributed.worker - INFO -          dashboard at:          10.6.105.16:40959
2025-09-05 09:25:26,464 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:34527
2025-09-05 09:25:26,464 - distributed.worker - INFO -          dashboard at:          10.6.105.16:35595
2025-09-05 09:25:26,464 - distributed.worker - INFO -          dashboard at:          10.6.105.16:34879
2025-09-05 09:25:26,464 - distributed.worker - INFO -          dashboard at:          10.6.105.16:33725
2025-09-05 09:25:26,464 - distributed.worker - INFO -          dashboard at:          10.6.105.16:39215
2025-09-05 09:25:26,464 - distributed.worker - INFO -          dashboard at:          10.6.105.16:44935
2025-09-05 09:25:26,464 - distributed.worker - INFO -          dashboard at:          10.6.105.16:43003
2025-09-05 09:25:26,464 - distributed.worker - INFO -          dashboard at:          10.6.105.16:45929
2025-09-05 09:25:26,464 - distributed.worker - INFO -          dashboard at:          10.6.105.16:41909
2025-09-05 09:25:26,464 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,464 - distributed.worker - INFO -          dashboard at:          10.6.105.16:42123
2025-09-05 09:25:26,464 - distributed.worker - INFO -          dashboard at:          10.6.105.16:44997
2025-09-05 09:25:26,464 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,464 - distributed.worker - INFO -          dashboard at:          10.6.105.16:46417
2025-09-05 09:25:26,464 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,464 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,464 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,464 - distributed.worker - INFO -          dashboard at:          10.6.105.16:46447
2025-09-05 09:25:26,464 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,464 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,464 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,464 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,464 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,464 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,464 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,464 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,464 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,464 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,464 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,464 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,464 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,464 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,464 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,464 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,464 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,464 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,464 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,464 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,464 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,464 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,464 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,464 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,464 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,464 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,464 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,464 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,464 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,464 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,464 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,464 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,464 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,464 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,464 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,464 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,464 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,464 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,464 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,464 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,464 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,464 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,464 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,464 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-k6g7l0h8
2025-09-05 09:25:26,464 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-6w7ym9di
2025-09-05 09:25:26,464 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,464 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-fef23nzj
2025-09-05 09:25:26,464 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-_leqepv6
2025-09-05 09:25:26,464 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-732yd700
2025-09-05 09:25:26,464 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,464 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-0433_ois
2025-09-05 09:25:26,464 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-vw71aeqe
2025-09-05 09:25:26,465 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-l_wqme30
2025-09-05 09:25:26,464 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,464 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-n2w6inni
2025-09-05 09:25:26,464 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-1tceezbm
2025-09-05 09:25:26,464 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-9tk5cufj
2025-09-05 09:25:26,464 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-ksnp77kl
2025-09-05 09:25:26,464 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-cpwbf_1b
2025-09-05 09:25:26,465 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-bxfjvn7n
2025-09-05 09:25:26,465 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-1_p8j26b
2025-09-05 09:25:26,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,465 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-4mkw33ew
2025-09-05 09:25:26,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,465 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-sgqltibs
2025-09-05 09:25:26,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,473 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:36625
2025-09-05 09:25:26,474 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:36625
2025-09-05 09:25:26,474 - distributed.worker - INFO -          dashboard at:          10.6.105.16:36113
2025-09-05 09:25:26,474 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,474 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,474 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,474 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,474 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-r95yg6w7
2025-09-05 09:25:26,475 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,489 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,489 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,489 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,490 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,493 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,493 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,493 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,494 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,496 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,496 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,497 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,497 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,499 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,500 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,500 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,501 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,502 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,503 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,503 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,504 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,504 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,505 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,505 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,505 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,508 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,509 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,510 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,510 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,511 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,511 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,511 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,512 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,513 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,513 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,513 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,514 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,514 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,515 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,515 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,515 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,516 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,517 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,517 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,517 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,518 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,518 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,518 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,519 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,520 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,520 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,520 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,520 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,521 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,521 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,521 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,521 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,521 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,522 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,522 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,523 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,523 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,524 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,524 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,525 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,525 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,525 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,526 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,526 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,527 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,528 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,579 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:42915
2025-09-05 09:25:26,579 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:42915
2025-09-05 09:25:26,579 - distributed.worker - INFO -          dashboard at:          10.6.105.16:45739
2025-09-05 09:25:26,579 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,579 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,579 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,579 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,579 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-wuof_ya0
2025-09-05 09:25:26,579 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,603 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,605 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,605 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,607 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,619 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:39957
2025-09-05 09:25:26,619 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:39957
2025-09-05 09:25:26,619 - distributed.worker - INFO -          dashboard at:          10.6.105.16:45181
2025-09-05 09:25:26,619 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,619 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,619 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,619 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,619 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-vto4fpdk
2025-09-05 09:25:26,619 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,629 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:37077
2025-09-05 09:25:26,629 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:37077
2025-09-05 09:25:26,629 - distributed.worker - INFO -          dashboard at:          10.6.105.16:37455
2025-09-05 09:25:26,629 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,629 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,629 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,629 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,629 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-p13b61_9
2025-09-05 09:25:26,629 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,630 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:33221
2025-09-05 09:25:26,630 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:33221
2025-09-05 09:25:26,630 - distributed.worker - INFO -          dashboard at:          10.6.105.16:43791
2025-09-05 09:25:26,630 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,630 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,630 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,630 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,630 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-z2o_922e
2025-09-05 09:25:26,630 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:44771
2025-09-05 09:25:26,631 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,631 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:44771
2025-09-05 09:25:26,631 - distributed.worker - INFO -          dashboard at:          10.6.105.16:39547
2025-09-05 09:25:26,631 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,631 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,631 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,631 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,631 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-vk7xme17
2025-09-05 09:25:26,631 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,632 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,633 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,633 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,633 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,645 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,646 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,646 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,646 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:34547
2025-09-05 09:25:26,646 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:34547
2025-09-05 09:25:26,646 - distributed.worker - INFO -          dashboard at:          10.6.105.16:46647
2025-09-05 09:25:26,646 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,646 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,646 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,646 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,646 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-2yg_f2x9
2025-09-05 09:25:26,646 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,646 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,654 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,655 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,656 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,656 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:40257
2025-09-05 09:25:26,656 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:40257
2025-09-05 09:25:26,656 - distributed.worker - INFO -          dashboard at:          10.6.105.16:38731
2025-09-05 09:25:26,656 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,656 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,656 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,656 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,656 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-y5qyn24s
2025-09-05 09:25:26,656 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,657 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,658 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,659 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,659 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,661 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,661 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:39209
2025-09-05 09:25:26,661 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:39209
2025-09-05 09:25:26,661 - distributed.worker - INFO -          dashboard at:          10.6.105.16:45943
2025-09-05 09:25:26,661 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,661 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,661 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,661 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,661 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-o3ecfg_l
2025-09-05 09:25:26,661 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,662 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,663 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,665 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,671 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,672 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,672 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,672 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,674 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,675 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,675 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,676 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,677 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:43887
2025-09-05 09:25:26,677 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:43887
2025-09-05 09:25:26,677 - distributed.worker - INFO -          dashboard at:          10.6.105.16:33339
2025-09-05 09:25:26,677 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,677 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,677 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,677 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,677 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-9_8gpeep
2025-09-05 09:25:26,677 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,683 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:39795
2025-09-05 09:25:26,683 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:39795
2025-09-05 09:25:26,683 - distributed.worker - INFO -          dashboard at:          10.6.105.16:45677
2025-09-05 09:25:26,683 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,683 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,683 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,683 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,683 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-_uebi84w
2025-09-05 09:25:26,683 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,693 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:36041
2025-09-05 09:25:26,693 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:36041
2025-09-05 09:25:26,693 - distributed.worker - INFO -          dashboard at:          10.6.105.16:38483
2025-09-05 09:25:26,693 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,693 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,693 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,693 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,693 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-dl3x2i33
2025-09-05 09:25:26,693 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,699 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,700 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,700 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,701 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,704 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,705 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,705 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,706 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,710 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,710 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,710 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,711 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,743 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:33185
2025-09-05 09:25:26,743 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:33185
2025-09-05 09:25:26,743 - distributed.worker - INFO -          dashboard at:          10.6.105.16:40393
2025-09-05 09:25:26,743 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,743 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,743 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,743 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,743 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-ks5bsnlp
2025-09-05 09:25:26,743 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,745 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:33951
2025-09-05 09:25:26,745 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:33951
2025-09-05 09:25:26,745 - distributed.worker - INFO -          dashboard at:          10.6.105.16:36363
2025-09-05 09:25:26,745 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,745 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,745 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,745 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,745 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-n78yrqxl
2025-09-05 09:25:26,745 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,749 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:33945
2025-09-05 09:25:26,749 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:33945
2025-09-05 09:25:26,749 - distributed.worker - INFO -          dashboard at:          10.6.105.16:34365
2025-09-05 09:25:26,749 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,749 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,749 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,749 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,749 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-u_ute7yl
2025-09-05 09:25:26,749 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,754 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:41941
2025-09-05 09:25:26,754 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:41941
2025-09-05 09:25:26,754 - distributed.worker - INFO -          dashboard at:          10.6.105.16:40735
2025-09-05 09:25:26,754 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,754 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,754 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,754 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,754 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-glufed9l
2025-09-05 09:25:26,754 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,761 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,762 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,762 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,762 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,768 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:35947
2025-09-05 09:25:26,769 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:35947
2025-09-05 09:25:26,769 - distributed.worker - INFO -          dashboard at:          10.6.105.16:42837
2025-09-05 09:25:26,769 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,769 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,769 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,769 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,769 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-lo_zrq0e
2025-09-05 09:25:26,769 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,769 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,770 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,770 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,771 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,773 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,774 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:43617
2025-09-05 09:25:26,774 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:43617
2025-09-05 09:25:26,774 - distributed.worker - INFO -          dashboard at:          10.6.105.16:38619
2025-09-05 09:25:26,774 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,774 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,774 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,775 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,775 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-wv77bnua
2025-09-05 09:25:26,774 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,775 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,775 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,776 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,779 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,780 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,780 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,782 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,795 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,796 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,796 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,797 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,800 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,801 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,801 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,802 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,807 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:46327
2025-09-05 09:25:26,807 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:46327
2025-09-05 09:25:26,807 - distributed.worker - INFO -          dashboard at:          10.6.105.16:44567
2025-09-05 09:25:26,808 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,808 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,808 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,808 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,808 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-ferxy6pi
2025-09-05 09:25:26,808 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,810 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:43133
2025-09-05 09:25:26,810 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:43133
2025-09-05 09:25:26,810 - distributed.worker - INFO -          dashboard at:          10.6.105.16:45991
2025-09-05 09:25:26,810 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,810 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,810 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,810 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,810 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-tkjnpng8
2025-09-05 09:25:26,811 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,831 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,832 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,832 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,834 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,837 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,838 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,838 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,839 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:38589
2025-09-05 09:25:26,839 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:38589
2025-09-05 09:25:26,839 - distributed.worker - INFO -          dashboard at:          10.6.105.16:41917
2025-09-05 09:25:26,839 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,839 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,839 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,839 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,839 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-hvye4dkw
2025-09-05 09:25:26,839 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,839 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,863 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,864 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,864 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,865 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:43575
2025-09-05 09:25:26,865 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:43575
2025-09-05 09:25:26,865 - distributed.worker - INFO -          dashboard at:          10.6.105.16:40845
2025-09-05 09:25:26,865 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,865 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,865 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,865 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,865 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-0v87wqme
2025-09-05 09:25:26,865 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,866 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,879 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,879 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,879 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,880 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,901 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:38641
2025-09-05 09:25:26,901 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:38641
2025-09-05 09:25:26,901 - distributed.worker - INFO -          dashboard at:          10.6.105.16:32899
2025-09-05 09:25:26,902 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,902 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,902 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,902 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,902 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-joqhdjrs
2025-09-05 09:25:26,902 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,915 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:39103
2025-09-05 09:25:26,915 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:39103
2025-09-05 09:25:26,915 - distributed.worker - INFO -          dashboard at:          10.6.105.16:33129
2025-09-05 09:25:26,915 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,915 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,916 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,916 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,916 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-wvfhyucg
2025-09-05 09:25:26,916 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,923 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:36055
2025-09-05 09:25:26,923 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:36055
2025-09-05 09:25:26,923 - distributed.worker - INFO -          dashboard at:          10.6.105.16:38107
2025-09-05 09:25:26,923 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,923 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,923 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,923 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,924 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-el40ryx1
2025-09-05 09:25:26,924 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,926 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,927 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,927 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,928 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,930 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,930 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,930 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,931 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,934 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:43113
2025-09-05 09:25:26,934 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:43113
2025-09-05 09:25:26,934 - distributed.worker - INFO -          dashboard at:          10.6.105.16:40579
2025-09-05 09:25:26,934 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,935 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,935 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,935 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,935 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-6n396kl_
2025-09-05 09:25:26,935 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,942 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:36453
2025-09-05 09:25:26,942 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:36453
2025-09-05 09:25:26,942 - distributed.worker - INFO -          dashboard at:          10.6.105.16:39451
2025-09-05 09:25:26,942 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,942 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,942 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,942 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,942 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-g_sw1hq6
2025-09-05 09:25:26,942 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,946 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,947 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:41825
2025-09-05 09:25:26,947 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:41825
2025-09-05 09:25:26,947 - distributed.worker - INFO -          dashboard at:          10.6.105.16:34139
2025-09-05 09:25:26,947 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,947 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,947 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,947 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,947 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-tk1wj35p
2025-09-05 09:25:26,947 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,947 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,947 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,948 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:34125
2025-09-05 09:25:26,948 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:34125
2025-09-05 09:25:26,948 - distributed.worker - INFO -          dashboard at:          10.6.105.16:41625
2025-09-05 09:25:26,948 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,948 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,948 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,948 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,948 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-zgzs8q48
2025-09-05 09:25:26,948 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,948 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,951 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:35157
2025-09-05 09:25:26,952 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:35157
2025-09-05 09:25:26,952 - distributed.worker - INFO -          dashboard at:          10.6.105.16:46183
2025-09-05 09:25:26,952 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,952 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,952 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,952 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,952 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-67weeov7
2025-09-05 09:25:26,952 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,954 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:34427
2025-09-05 09:25:26,954 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:34427
2025-09-05 09:25:26,954 - distributed.worker - INFO -          dashboard at:          10.6.105.16:33885
2025-09-05 09:25:26,954 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,954 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,954 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,954 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,954 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-re5xm_zw
2025-09-05 09:25:26,954 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,957 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,958 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,958 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,960 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,961 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:46575
2025-09-05 09:25:26,961 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:46575
2025-09-05 09:25:26,961 - distributed.worker - INFO -          dashboard at:          10.6.105.16:34775
2025-09-05 09:25:26,961 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,961 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,961 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,961 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,962 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-5pvao6_p
2025-09-05 09:25:26,962 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,962 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,962 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,962 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,963 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,966 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:34617
2025-09-05 09:25:26,966 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:34617
2025-09-05 09:25:26,966 - distributed.worker - INFO -          dashboard at:          10.6.105.16:36741
2025-09-05 09:25:26,966 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,966 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,966 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,966 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,966 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-b4z3rubm
2025-09-05 09:25:26,966 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,968 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,968 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,969 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,970 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,971 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,971 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,971 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,972 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,977 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,978 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,979 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,980 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,980 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,981 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,981 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,982 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,982 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:35401
2025-09-05 09:25:26,982 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:35401
2025-09-05 09:25:26,982 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,982 - distributed.worker - INFO -          dashboard at:          10.6.105.16:44951
2025-09-05 09:25:26,982 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,982 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,982 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,982 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,982 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,982 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-_0j6_4fx
2025-09-05 09:25:26,982 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,983 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,983 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,986 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,986 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:45403
2025-09-05 09:25:26,986 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:45403
2025-09-05 09:25:26,986 - distributed.worker - INFO -          dashboard at:          10.6.105.16:33831
2025-09-05 09:25:26,986 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,986 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,986 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:26,986 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:26,986 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-prms5yyy
2025-09-05 09:25:26,986 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,986 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,987 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,988 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:26,996 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:26,997 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:26,997 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:26,997 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:27,005 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:27,005 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:27,006 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:27,007 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:50,844 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,845 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,845 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,845 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,845 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,845 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,845 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,845 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,845 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,846 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,846 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,846 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,846 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,846 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,846 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,847 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,847 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,847 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,847 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,847 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,847 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,847 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,847 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,847 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,847 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,847 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,847 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,848 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,848 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,848 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,848 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,848 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,848 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,848 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,848 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,848 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,848 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,848 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,848 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,848 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,848 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,849 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,849 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,849 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,849 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,849 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,849 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,849 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,849 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,849 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,849 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,849 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,849 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,849 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,849 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,849 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,850 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,850 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,850 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,850 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,850 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,850 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,850 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,850 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,851 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,851 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,851 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,851 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,851 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,851 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,851 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,850 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,852 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,852 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,852 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,852 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,852 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,850 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,853 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,853 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,853 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,853 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,853 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,853 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,851 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,851 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,851 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,853 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,853 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,854 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,858 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,860 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,858 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,858 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,861 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,862 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,863 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,863 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,863 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,861 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,864 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,868 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,869 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,871 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:53,592 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,592 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,592 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,593 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,592 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,592 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,592 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,593 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,593 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,593 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,593 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,593 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,593 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,593 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,594 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,594 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,594 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,594 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,594 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,594 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,595 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,595 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,595 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,595 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,595 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,595 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,595 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,595 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,594 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,595 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,595 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,594 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,596 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,595 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,595 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,595 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,596 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,596 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,595 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,595 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,596 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,595 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,594 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,596 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,596 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,595 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,596 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,596 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,596 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,595 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,596 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,597 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,596 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,597 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,597 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,597 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,596 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,596 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,597 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,597 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,598 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,598 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,598 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,598 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,597 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,598 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,596 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,598 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,597 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,599 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,597 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,599 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,596 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,599 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,599 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,597 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,599 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,598 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,599 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,597 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,600 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,597 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,600 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,598 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,597 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,600 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,598 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,600 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,598 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,600 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,597 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,600 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,600 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,601 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,601 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,601 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,601 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,601 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,601 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,601 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,602 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,602 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,602 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,602 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,975 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,975 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,975 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,975 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,976 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,976 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,976 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,976 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,976 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,976 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,977 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,976 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,977 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,977 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,977 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,977 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,977 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,977 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,977 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,977 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,977 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,978 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,978 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,978 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,978 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,978 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,978 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,978 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,978 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,979 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,979 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,979 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,979 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,979 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,979 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,979 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,979 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,979 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,979 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,979 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,979 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,980 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,980 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,980 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,980 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,980 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,980 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,980 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,980 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,980 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,980 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,981 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,981 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,981 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,981 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,981 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,981 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,981 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,981 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,981 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,981 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,982 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,982 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,982 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,982 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,982 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,982 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,982 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,982 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,982 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,982 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,982 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,983 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,983 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,983 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,983 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,983 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,983 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,983 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,983 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,983 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,983 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,983 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,984 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,984 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,984 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,984 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,984 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,984 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,984 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,984 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,984 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,984 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,984 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,984 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,984 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,984 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,984 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,984 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,984 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,984 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,984 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,985 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,985 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,353 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,353 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,354 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,354 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,354 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,354 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,354 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,354 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,354 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,354 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,355 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,355 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,355 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,354 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,355 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,355 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,355 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,355 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,355 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,356 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,356 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,355 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,356 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,356 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,356 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,356 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,356 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,356 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,356 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,356 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,356 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,356 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,356 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,356 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,357 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,357 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,357 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,357 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,357 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,357 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,357 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,357 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,357 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,357 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,357 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,357 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,357 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,357 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,357 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,357 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,358 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,358 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,358 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,358 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,358 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,358 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,358 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,358 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,358 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,358 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,358 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,358 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,358 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,358 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,358 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,358 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,358 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,358 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,358 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,359 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,359 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,359 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,359 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,359 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,359 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,359 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,359 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,359 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,359 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,359 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,359 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,359 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,359 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,360 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,360 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,360 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,360 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,360 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,360 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,361 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,361 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,361 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,361 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,361 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,362 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,362 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,362 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,362 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,362 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,362 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,362 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,362 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,368 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,371 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:33:00,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,068 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,492 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,543 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,568 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,601 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,617 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:47,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:47,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:48,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:48,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:48,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:48,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:48,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:49,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:49,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:49,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:50,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:51,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:52,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:52,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:52,848 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:54,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:56,718 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:56,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,254 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,665 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:58,081 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:58,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:58,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:58,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:58,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:58,718 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:58,718 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:58,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:59,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:00,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:00,275 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:06,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:06,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:06,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:07,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:07,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:08,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:08,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:08,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:10,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:10,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:19,426 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:51,665 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:51,667 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:51,667 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:54,771 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:57,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:00,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:00,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:01,038 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:01,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:01,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:01,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:01,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:01,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:03,551 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:03,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:03,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:04,771 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:05,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:05,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:05,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:05,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:05,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:05,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:05,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:05,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:05,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:07,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:07,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:07,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:07,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:07,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:07,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:07,287 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:07,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:07,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:07,527 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:07,530 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:08,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:08,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:08,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:09,093 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:09,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:09,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:09,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:13,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:14,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:14,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:15,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:15,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:15,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:17,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:17,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:17,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:17,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:17,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:17,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:19,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:20,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:20,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:20,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:21,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:21,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:21,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:22,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:22,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:23,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:24,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:24,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:24,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:24,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:24,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:24,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:24,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:26,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:28,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:28,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:28,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:31,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:32,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:33,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:33,786 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:34,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:34,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:36,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:36,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:36,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:36,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:36,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:36,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:39,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:39,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:40,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:40,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:40,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:40,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:40,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:43,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:43,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:44,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:44,483 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:45,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:45,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:45,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:45,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:46,430 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:46,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:46,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:48,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:49,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:49,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:51,090 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:54,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:54,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:55,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:58,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:03,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:04,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:05,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:06,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:06,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:07,014 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:08,068 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:08,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:08,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:08,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:11,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:11,165 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:11,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:11,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:11,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:11,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:12,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:13,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:14,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:15,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:16,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:19,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:19,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:21,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:21,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:25,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:25,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:25,752 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:26,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:26,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:26,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:26,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:26,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:27,307 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:27,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:28,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:28,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:28,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:28,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:28,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:28,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:28,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:28,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:28,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:29,044 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:29,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:29,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:31,985 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:32,524 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:33,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:33,771 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:33,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:33,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:34,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:36,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:36,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:37,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:40,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:40,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:41,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:41,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:41,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:41,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:44,097 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:44,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:44,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:45,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:45,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:45,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:45,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:45,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:45,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:46,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:48,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:48,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:48,430 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:48,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:48,665 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:48,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:48,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,342 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,395 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:50,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:50,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:50,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:52,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:52,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:12,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:13,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:13,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:13,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,634 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,667 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:16,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:16,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:16,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:16,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:16,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:17,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:17,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:17,521 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:18,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:18,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:18,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:18,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:19,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:22,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:22,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:22,250 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:23,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:23,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:26,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:26,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:26,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:26,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:29,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:29,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:29,963 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:30,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:31,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:33,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:35,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:36,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:36,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:47,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:47,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:48,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:48,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:49,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:49,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:49,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:49,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:49,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:50,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:50,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:50,140 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:52,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:52,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:52,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:52,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:53,238 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:53,239 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:55,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:57,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:57,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:57,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:57,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:57,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:58,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:58,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:00,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:02,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:02,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:02,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:02,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:02,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:03,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:05,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:05,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:05,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:05,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:05,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:05,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:07,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:07,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:07,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:07,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:10,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:11,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:12,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:12,275 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:12,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:12,980 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:13,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:14,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:15,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:16,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:16,007 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:16,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:16,479 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:16,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:16,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:16,608 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:16,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:17,548 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:20,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:20,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:20,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:20,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:20,345 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:20,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:20,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:20,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:20,723 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:20,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:20,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:20,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:21,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:21,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:21,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:23,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:25,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:29,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:29,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:29,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:30,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:30,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:31,310 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:31,859 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:32,287 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:33,100 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:34,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:36,556 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:40,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:40,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:49,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:49,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:49,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:49,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:50,527 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:50,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:50,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:50,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:50,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:51,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:52,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:53,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:53,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:53,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:54,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:54,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:55,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:55,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:57,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:57,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:57,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:57,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:57,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:57,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:58,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:58,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:58,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:00,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:07,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:07,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:08,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:08,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:08,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:09,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:09,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:09,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:10,238 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:10,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:10,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:10,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:11,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:11,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:11,943 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:13,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:13,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:13,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:13,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:13,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:13,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:13,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:13,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:13,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:13,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:13,844 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:15,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:15,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:16,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:17,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:17,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:17,400 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:18,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:18,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:18,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:19,655 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:19,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:20,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:20,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:20,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:28,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:29,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:29,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:29,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:29,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:30,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:30,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:30,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:31,337 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:31,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:33,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:33,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:33,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:34,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:36,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:38,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:38,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:38,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:38,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:38,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:38,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:38,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:38,957 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:39,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:39,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:39,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:39,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:39,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:40,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:41,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:41,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:42,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:42,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:42,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:43,860 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:43,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:44,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:44,282 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:44,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:44,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:44,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:44,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:44,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:45,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:45,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:45,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:45,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:46,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:49,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:49,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:50,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:50,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:50,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:50,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:50,765 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:51,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:51,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:51,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:51,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:51,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:51,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:51,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:51,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:51,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:51,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:51,568 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:51,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:51,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:52,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:52,822 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:53,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:53,239 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:53,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:53,244 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:53,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:53,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:53,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:57,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:57,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:57,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:57,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:59,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:01,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:05,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:13,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:14,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:14,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:14,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:18,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:18,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:19,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:19,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:19,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:19,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:19,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:19,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:19,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:19,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:20,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:21,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:21,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:21,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:22,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:22,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:23,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:23,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:23,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:23,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:24,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:24,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:24,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:24,187 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:24,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:24,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:24,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:24,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:24,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:24,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:25,128 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:25,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:25,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:25,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:26,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:26,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:26,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:29,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:29,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:29,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:30,178 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:30,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:30,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:30,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:30,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:30,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:30,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:31,187 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:31,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:31,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:31,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:32,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:32,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:32,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:33,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:33,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:33,067 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:33,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:34,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:34,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:34,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:35,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:35,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:35,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:36,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:36,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:36,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:37,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:37,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:37,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:38,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:39,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:39,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:39,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:39,238 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:39,254 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:39,305 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:39,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:39,559 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:39,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:39,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:39,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:39,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:39,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:40,444 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:40,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:40,847 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:40,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:40,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:40,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:41,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:43,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:43,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:44,359 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:44,652 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:45,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:45,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:45,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:45,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:45,868 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:45,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:46,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:46,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:46,466 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:47,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:49,957 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:49,963 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:49,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:52,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:52,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:52,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:53,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:54,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:55,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:55,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:59,723 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:03,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:03,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:03,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:03,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:03,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:09,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:10,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:11,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:11,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:11,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:11,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:11,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:11,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:12,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:12,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:12,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:12,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:12,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:14,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:14,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:14,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:15,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:16,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:18,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:19,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:20,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:20,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:20,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:20,692 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:21,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:22,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:22,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:22,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:22,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:22,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:24,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:24,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:24,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:25,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:25,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:25,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:27,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:27,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:27,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:27,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:28,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:28,848 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,883 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:30,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:30,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:30,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:30,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:31,441 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:31,445 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:31,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:31,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:33,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:33,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:33,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:33,458 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:33,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:34,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:37,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:47,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:48,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:48,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:48,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:48,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:48,844 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:49,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:50,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:50,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:50,838 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:53,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:54,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:56,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:56,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:59,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:59,678 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:59,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:59,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:00,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:00,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:00,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:00,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:01,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:01,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:01,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:02,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:02,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:02,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:02,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:02,530 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:02,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:03,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:03,369 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:03,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:06,838 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:06,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:07,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:08,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:09,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:09,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:09,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:09,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:10,305 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:11,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:11,007 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:12,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:12,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:12,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:12,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:12,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:12,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:12,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:12,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:13,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:13,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:17,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:17,367 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:18,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:18,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:18,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:18,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:19,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:21,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:22,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:22,275 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:22,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:22,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:22,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:22,767 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:23,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:23,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:23,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:27,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:27,603 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:27,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:28,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:28,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:29,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:29,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:29,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:29,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:29,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:30,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:30,445 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:31,574 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:31,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:31,617 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:33,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:33,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:34,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:35,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:35,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:38,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:38,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:40,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:40,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:40,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:40,794 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:41,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:41,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:41,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:41,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:41,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:45,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:45,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:46,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:46,381 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:47,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:47,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:47,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:47,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:48,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:48,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:48,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:48,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:49,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:49,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:49,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:49,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:49,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:49,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:49,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:49,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:50,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:50,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:50,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:51,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:51,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:51,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:51,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:51,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:51,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:51,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:51,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:52,723 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:52,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:52,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:52,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:52,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:53,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:53,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:53,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:53,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:59,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:59,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:59,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:59,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:00,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:00,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:00,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:00,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:00,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,822 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:03,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:03,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:05,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:05,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:06,097 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:06,100 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:06,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:06,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:06,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:08,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:12,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:12,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:12,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:13,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:13,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:15,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:17,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:17,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:19,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:19,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:19,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:20,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:21,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:21,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:21,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:22,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:22,275 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:22,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:22,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:22,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:22,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:22,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:22,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:57,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:57,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:57,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:57,851 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:57,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:58,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:59,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:59,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:59,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:59,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:02,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:02,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:03,093 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:03,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:03,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:03,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:03,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:03,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:03,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:04,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:05,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:05,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:05,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:06,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:06,622 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:06,623 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:06,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:06,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:06,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:09,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:09,014 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:09,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:09,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:09,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:09,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:10,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:11,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:11,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:11,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:12,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:16,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:16,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:16,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:16,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:17,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:19,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:20,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:20,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:21,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:21,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:21,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:21,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:23,007 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:23,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:23,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:23,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:23,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:23,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:24,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:24,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:24,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:24,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:26,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:26,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:26,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:27,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:27,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:27,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:27,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:27,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:27,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:27,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:27,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:27,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:28,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:31,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:35,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:35,649 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:42,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:43,239 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:44,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:45,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:45,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:45,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:45,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:45,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:45,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:46,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:46,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:46,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:46,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:48,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:48,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:50,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:50,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:50,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:50,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:50,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:50,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:51,929 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:52,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:52,524 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:52,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:54,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:54,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:55,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:57,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:02,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:02,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:02,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:02,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:02,980 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:03,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:03,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:03,524 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:03,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:03,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:03,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:03,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:05,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:06,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:06,821 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:07,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:08,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:09,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:09,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:10,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:10,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:11,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:12,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:13,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:13,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:14,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:16,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:16,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:16,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:16,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:17,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:20,430 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:28,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:28,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:30,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:31,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:31,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:32,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:32,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:32,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:32,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:32,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:33,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:33,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:33,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:33,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:33,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:33,823 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:34,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:34,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:34,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:34,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:34,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:34,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:34,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:34,649 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:34,650 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:34,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:34,908 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:35,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:35,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:35,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:35,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:35,981 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:36,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:36,770 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:36,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:36,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:36,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:37,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:37,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:37,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:37,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:37,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:37,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:37,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:37,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:38,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:46,381 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:47,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:47,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:20,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:21,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:23,548 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:23,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:23,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:23,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:25,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:25,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:27,276 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:28,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:29,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:29,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:29,524 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:29,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:29,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:31,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:33,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:34,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:34,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:34,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:34,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:34,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:35,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:35,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:35,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:35,859 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:37,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:37,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:37,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:37,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:38,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:38,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:38,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:38,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:39,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:39,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:40,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:40,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:40,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:40,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:40,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:40,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:40,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:43,543 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:43,543 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:47,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:50,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:50,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:50,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:50,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:52,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:52,081 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:52,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:53,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:53,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:53,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:54,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:54,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:55,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:55,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:55,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:55,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:55,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:55,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:55,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:55,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:55,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:55,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:55,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:55,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:56,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:56,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:56,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:56,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:56,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:57,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:57,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:57,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:57,784 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:57,847 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:58,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:58,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:58,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:58,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:00,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:00,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:00,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:00,719 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:01,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:02,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:07,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:07,369 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:07,873 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:08,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:08,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:09,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:09,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:09,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:09,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:09,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:09,691 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:09,698 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:09,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:09,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:09,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:10,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:11,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:12,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:12,479 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:12,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:12,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:12,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:13,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:13,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:13,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:13,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:13,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:14,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:14,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:14,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:14,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:15,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:15,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:15,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:15,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:16,101 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:16,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:16,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:17,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:17,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:17,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:17,453 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:17,678 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:17,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:17,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:17,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:18,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:21,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:26,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:27,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:28,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:29,530 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:29,532 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:29,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:30,381 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:31,287 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:31,373 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:32,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:33,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:33,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:33,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:33,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:33,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:34,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:34,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:34,337 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:35,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:35,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:36,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:38,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:38,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:39,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:40,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:40,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:40,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:40,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:42,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:42,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:42,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:42,355 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:42,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:42,367 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:42,530 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:43,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:44,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:44,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:52,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:52,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:55,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:18,771 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:18,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:18,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:19,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:19,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:20,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:20,890 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:22,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:23,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:23,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:23,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:23,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:24,305 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:24,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:24,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:25,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:25,495 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:26,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:26,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:27,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:27,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:28,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:28,752 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:28,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:29,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:29,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:29,202 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:29,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:30,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:30,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:30,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:31,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:32,234 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:32,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:32,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:32,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:32,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:32,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:33,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:43,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:43,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:43,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:44,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:45,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:47,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:47,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:47,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:47,718 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:47,718 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:47,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:47,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:47,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:47,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:48,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:48,655 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:49,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:49,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:49,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:49,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:50,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:50,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:50,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:50,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:50,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:50,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:51,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:51,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:52,459 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:52,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:52,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:53,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:53,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:53,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:53,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:53,527 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:53,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:54,655 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:55,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:55,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:56,096 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:56,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:57,556 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:57,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:57,979 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:58,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:59,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:59,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:07,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:07,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:07,649 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:14,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:14,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:14,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:14,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:14,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:15,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:15,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:15,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:15,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:15,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:16,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:16,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:17,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:20,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:20,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:20,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:21,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:21,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:21,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:21,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:22,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:22,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:22,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:22,751 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:22,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:22,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:22,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:22,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:22,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:22,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:23,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:23,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:23,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:24,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:26,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:27,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:27,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:27,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:27,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:28,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:28,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:28,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:29,458 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:30,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:30,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:31,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:31,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:31,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:31,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:32,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:38,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:38,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:38,985 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:38,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:38,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:38,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:39,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:39,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:39,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:39,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:39,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:40,890 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:43,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:43,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:44,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:44,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:44,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:44,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:45,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:46,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:46,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:48,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:48,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:49,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:50,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:50,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:50,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:50,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:50,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:51,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:52,067 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:52,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:52,766 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,767 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:39749. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,766 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,766 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,767 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,766 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,767 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,766 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,766 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,766 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,766 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,768 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,768 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,768 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:34011. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,768 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:34547. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,768 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:46517. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,768 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:33185. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,768 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:42129. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,769 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:39013. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,769 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:33565. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,769 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:36007. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,769 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:42915. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,769 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:37111. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,769 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:39209. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,767 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,767 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,766 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,767 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,769 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:33945. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,769 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:33951. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,770 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:44035. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,770 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:41941. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,767 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,766 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.16:51060 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,770 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:39957. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,770 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,771 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:34427. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,767 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.16:50932 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,766 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.16:51036 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,766 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.16:51076 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,767 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.16:50952 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,766 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.16:51026 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,767 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.16:51004 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,767 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.16:50990 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,767 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.16:50976 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,766 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.16:51052 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,766 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.16:51098 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,767 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.16:50966 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,767 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.16:50968 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,773 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,767 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.16:51018 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,766 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.16:51032 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,766 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.16:51114 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,775 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:35947. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,774 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,775 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:34125. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,774 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,774 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,774 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,774 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,776 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:43133. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,776 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:34617. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,776 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:41825. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,774 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,775 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,775 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,776 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:39103. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,775 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,775 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,776 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:45403. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,775 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,775 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,776 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:35157. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,777 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:36055. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,777 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:38641. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,777 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:43575. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,777 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:38589. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,777 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:43113. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,775 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,775 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,777 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:36453. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,777 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:35401. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,777 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:39917'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,780 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764032, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,780 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.14:37317, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,780 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763186, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,781 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,781 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,781 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,781 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,781 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,789 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:35643'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,790 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:45701'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,790 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:41471'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,790 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:43261'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,791 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:34159'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,791 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763108, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,791 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 768131, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,791 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:35183'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,791 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.26:40289, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,791 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765280, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,791 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.13:40727, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,791 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765713, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,792 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:35619'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,792 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765077, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,792 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763107, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,792 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,792 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765682, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,792 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.15:38651, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,792 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765000, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,792 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,792 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:40071'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,792 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764999, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,792 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,792 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,792 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:35199'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,792 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,792 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,792 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,793 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:34793'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,793 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,793 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,793 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,793 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.32:33871, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,793 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,793 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,793 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,793 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,793 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:32813'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,793 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.24:38893, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,793 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,793 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,793 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,793 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 767743, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,793 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,793 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.8:33267, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,793 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,793 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,793 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,793 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.26:34379, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,793 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:40153'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,793 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765008, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,793 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763376, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,793 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,793 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,794 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.1:33981, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,794 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.27:37005, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,794 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,794 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765678, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,794 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:41399'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,794 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,794 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,794 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.7:41205, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,794 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764019, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,794 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764840, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,794 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766396, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,794 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:46825'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,794 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,794 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764842, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,794 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763347, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,794 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,794 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,794 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:44991'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,794 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.14:38841, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,794 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,794 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,795 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.16:33221, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,795 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763346, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,795 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:39627'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,795 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765408, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,795 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:39633'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,795 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766408, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,795 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765672, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,795 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764739, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,795 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766076, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,796 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764843, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,796 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.17:37207, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,796 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.11:38431, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,796 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763304, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,796 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765627, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,797 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,797 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,797 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,797 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,797 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,797 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,797 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,797 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,797 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,797 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.25:37155, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,797 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,797 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,797 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765819, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,798 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.21:45597, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,798 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.10:44925, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,801 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:37803'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,801 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:37937'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,801 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:42863'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,801 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:37169'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,802 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:36259'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,802 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:43875'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,802 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:36455'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,802 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763110, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,802 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:35743'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,802 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765274, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.14:39163, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765288, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,803 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:36591'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766665, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.17:41903, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,803 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:46751'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.14:41431, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,803 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,803 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:39271'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.16:43617, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:35521'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763287, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,804 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:34671'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764948, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,804 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:44381'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.8:33267, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763158, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.21:43437, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,804 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:46357'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763501, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 767291, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763448, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765601, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763478, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763446, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763449, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766658, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.17:45529, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.2:39139, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.8:36515, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.16:39795, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763137, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763109, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,806 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.11:45295, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,806 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763136, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,806 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765064, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,806 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766260, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,806 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765679, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,806 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.14:41379, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,806 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765063, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,806 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763793, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,806 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.26:45761, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,806 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.15:45113, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,806 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765685, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,807 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.10:44925, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,807 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.1:36227, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,807 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.16:40257, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,807 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765279, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,809 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,809 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,809 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,809 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,809 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,810 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,811 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,811 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,811 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,812 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,812 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,812 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,812 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,812 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,812 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,836 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:52,843 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:52,849 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:53,127 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:53,128 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:40257. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:53,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:53,148 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.16:50356 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:53,153 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:34695'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:53,154 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:53,154 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:53,154 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:53,154 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:53,154 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:53,172 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x147abeb89790>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:53,183 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:53,311 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:53,313 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:35737. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:53,317 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:53,319 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:34527. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:53,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:53,331 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.16:50166 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:53,335 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:39473'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:53,336 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 767188, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:53,336 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:53,337 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:53,337 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:53,337 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:53,337 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:53,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:53,344 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.16:50300 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:53,344 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:53,352 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:39575'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:53,353 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.22:36481, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:53,353 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.9:38915, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:53,354 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:53,354 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:53,354 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:53,354 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:53,354 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:53,362 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x146737c5fbd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:53,371 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:53,599 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:53,602 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:38983. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:53,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:53,624 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.16:50264 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:53,629 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:42095'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:53,630 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:53,630 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:53,630 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:53,630 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:53,630 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:53,639 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x149e202e2e10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:53,647 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:54,840 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:54,848 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:54,853 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:55,093 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:55,093 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:55,094 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:55,095 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:37109. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:55,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:55,115 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.16:50194 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:55,119 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:39619'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:55,120 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:55,120 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:55,120 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:55,120 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:55,120 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:55,127 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1466d72dbb50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:55,133 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:55,186 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:55,209 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:55,268 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:55,374 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:55,444 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:55,442 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:55,445 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:39795. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:55,461 - distributed.worker - ERROR - failed during get data with tcp://10.6.105.16:39795 -> tcp://10.6.105.16:45403
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.105.16:39795 remote=tcp://10.6.105.16:41092>: Stream is closed
2025-09-05 09:49:55,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:55,481 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.16:50390 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:55,485 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:40153'. Reason: nanny-close-gracefully
2025-09-05 09:49:55,488 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:35521'. Reason: nanny-close-gracefully
2025-09-05 09:49:55,489 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:43875'. Reason: nanny-close-gracefully
2025-09-05 09:49:55,490 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:40153' closed.
2025-09-05 09:49:55,492 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:35521' closed.
2025-09-05 09:49:55,492 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:43875' closed.
2025-09-05 09:49:55,493 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:37345'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:55,494 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:55,494 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:55,494 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:55,495 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:55,495 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:55,508 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x153acbab6cd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:55,519 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:55,650 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:55,662 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:34695'. Reason: nanny-close-gracefully
2025-09-05 09:49:55,663 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:34695' closed.
2025-09-05 09:49:55,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:55,744 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:55,747 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:37077. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:55,769 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.16:50338 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:55,773 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:44183'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:55,773 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:55,774 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:55,774 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:55,774 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:55,774 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:55,780 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14f235a06bd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:55,787 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:55,862 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:39575'. Reason: nanny-close-gracefully
2025-09-05 09:49:55,864 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:39575' closed.
2025-09-05 09:49:55,894 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:55,895 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:43887. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:55,896 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:55,898 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:40371. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:55,903 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:55,906 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:36041. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:55,909 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:55,912 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:46673. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:55,912 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.105.6:42187
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.105.16:56262 remote=tcp://10.6.105.6:42187>: Stream is closed
2025-09-05 09:49:55,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:55,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:55,919 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.16:50378 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:55,922 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:37253'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:55,923 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:55,923 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:55,923 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:55,923 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:55,923 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:55,923 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.16:50282 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:55,925 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:55,927 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:40223'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:55,927 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.14:41379, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:55,928 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:55,928 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:55,928 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:55,928 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:55,928 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:55,928 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1496881de7d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:55,928 - distributed.worker - ERROR - failed during get data with tcp://10.6.105.16:46673 -> tcp://10.6.105.25:44195
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.105.16:46673 remote=tcp://10.6.105.25:43530>: Stream is closed
2025-09-05 09:49:55,929 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.16:50404 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:55,933 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:32983'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:55,934 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:55,934 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.17:35845, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:55,934 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.16:36625, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:55,935 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:55,935 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:55,935 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:55,935 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:55,935 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:55,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:55,935 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14742fca0310>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:55,940 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.16:50204 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:55,941 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:55,940 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x151d55fb2650>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:55,942 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:41187'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:55,943 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:55,943 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:55,943 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:55,943 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:55,944 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:55,946 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:55,950 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1490b19c8a10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:55,955 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:56,054 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:56,054 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:46327. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:56,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:56,064 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.16:50470 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:56,065 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:44177'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:56,066 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:56,066 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:56,066 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:56,066 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:56,066 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:56,071 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x150116730310>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:56,075 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:56,127 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:42095'. Reason: nanny-close-gracefully
2025-09-05 09:49:56,128 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:42095' closed.
2025-09-05 09:49:56,792 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:57,000 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:57,097 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:57,098 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:57,136 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:57,182 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:57,181 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:57,184 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:44383. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:57,185 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:57,187 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:36625. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:57,186 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:57,188 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:46575. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:57,202 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:57,207 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.16:50160 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:57,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:57,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:57,210 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:34331'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:57,211 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:57,211 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:57,211 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:57,211 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:57,211 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:57,214 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:57,214 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:39891'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:57,213 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.16:50600 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:57,215 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.16:50310 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:57,217 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:57,217 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:57,217 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:57,217 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:57,217 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:57,217 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:34399'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:57,218 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766756, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:57,218 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:57,218 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:57,219 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:57,219 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:57,219 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:57,217 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14ea3e7d26d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:57,222 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:57,223 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:57,222 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1478aacb5950>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:57,224 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14fc1bc622d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:57,228 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:57,239 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:57,272 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:57,448 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:57,498 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:57,500 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:44771. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:57,523 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:57,524 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:57,528 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.16:50352 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:57,540 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:34867'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:57,541 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:57,541 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:57,541 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:57,541 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:57,541 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:57,547 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x148fee1b4810>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:57,554 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:57,572 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:39473'. Reason: nanny-close-gracefully
2025-09-05 09:49:57,573 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:39473' closed.
2025-09-05 09:49:57,584 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:39619'. Reason: nanny-close-gracefully
2025-09-05 09:49:57,584 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:39619' closed.
2025-09-05 09:49:57,645 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:39633'. Reason: nanny-close-gracefully
2025-09-05 09:49:57,646 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:39633' closed.
2025-09-05 09:49:57,718 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:39917'. Reason: nanny-close-gracefully
2025-09-05 09:49:57,719 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:39917' closed.
2025-09-05 09:49:57,795 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:46825'. Reason: nanny-close-gracefully
2025-09-05 09:49:57,801 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:46825' closed.
2025-09-05 09:49:57,933 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:35199'. Reason: nanny-close-gracefully
2025-09-05 09:49:57,934 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:35199' closed.
2025-09-05 09:49:57,936 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:57,944 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:57,949 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:57,957 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:58,078 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:58,107 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:58,128 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:37345'. Reason: nanny-close-gracefully
2025-09-05 09:49:58,130 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:37345' closed.
2025-09-05 09:49:58,241 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:44183'. Reason: nanny-close-gracefully
2025-09-05 09:49:58,244 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:44183' closed.
2025-09-05 09:49:58,385 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:37253'. Reason: nanny-close-gracefully
2025-09-05 09:49:58,386 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:37253' closed.
2025-09-05 09:49:58,415 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:40223'. Reason: nanny-close-gracefully
2025-09-05 09:49:58,417 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:41187'. Reason: nanny-close-gracefully
2025-09-05 09:49:58,418 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:40223' closed.
2025-09-05 09:49:58,419 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:41187' closed.
2025-09-05 09:49:58,477 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:32983'. Reason: nanny-close-gracefully
2025-09-05 09:49:58,478 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:32983' closed.
2025-09-05 09:49:58,515 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:44177'. Reason: nanny-close-gracefully
2025-09-05 09:49:58,516 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:44177' closed.
2025-09-05 09:49:58,796 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:59,003 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:59,186 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:59,226 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:59,226 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:59,231 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:59,243 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:59,267 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:36455'. Reason: nanny-close-gracefully
2025-09-05 09:49:59,268 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:36455' closed.
2025-09-05 09:49:59,274 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,408 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,465 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:45701'. Reason: nanny-close-gracefully
2025-09-05 09:49:59,466 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:45701' closed.
2025-09-05 09:49:59,557 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:59,680 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:34331'. Reason: nanny-close-gracefully
2025-09-05 09:49:59,681 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:34331' closed.
2025-09-05 09:49:59,723 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:36591'. Reason: nanny-close-gracefully
2025-09-05 09:49:59,724 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:36591' closed.
2025-09-05 09:49:59,727 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:44381'. Reason: nanny-close-gracefully
2025-09-05 09:49:59,727 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:36259'. Reason: nanny-close-gracefully
2025-09-05 09:49:59,728 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:44381' closed.
2025-09-05 09:49:59,728 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:36259' closed.
2025-09-05 09:49:59,782 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:39891'. Reason: nanny-close-gracefully
2025-09-05 09:49:59,783 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:39891' closed.
2025-09-05 09:50:00,009 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:34867'. Reason: nanny-close-gracefully
2025-09-05 09:50:00,010 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:34867' closed.
2025-09-05 09:50:00,075 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,111 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:00,225 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,544 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,555 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,584 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:41399'. Reason: nanny-close-gracefully
2025-09-05 09:50:00,585 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:41399' closed.
2025-09-05 09:50:00,618 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,675 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,779 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,834 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,835 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,871 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,886 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,207 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,212 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:50:01,215 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:43617. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:50:01,213 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:50:01,218 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:35329. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:50:01,238 - distributed.worker - ERROR - failed during get data with tcp://10.6.105.16:43617 -> tcp://10.6.105.16:39103
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.105.16:43617 remote=tcp://10.6.105.16:34940>: Stream is closed
2025-09-05 09:50:01,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:50:01,250 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.16:50456 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:50:01,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:50:01,253 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:34817'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:50:01,254 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:50:01,254 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:50:01,254 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:50:01,254 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:50:01,254 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:50:01,255 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.16:50254 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:50:01,260 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,260 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:43111'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:50:01,261 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,261 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:50:01,261 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:50:01,262 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:50:01,262 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:50:01,262 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:50:01,265 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x146085fee610>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:50:01,272 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,277 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:01,399 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,412 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:01,519 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:50:01,521 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.16:33221. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:50:01,542 - distributed.worker - ERROR - failed during get data with tcp://10.6.105.16:33221 -> tcp://10.6.105.16:33945
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.105.16:33221 remote=tcp://10.6.105.16:40556>: Stream is closed
2025-09-05 09:50:01,548 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:50:01,552 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.16:50348 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:50:01,554 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.16:39139'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:50:01,555 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:50:01,555 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:50:01,555 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:50:01,555 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:50:01,555 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:50:01,557 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,557 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x146c09c786d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:50:01,561 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,561 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,581 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,611 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,613 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,770 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:35183'. Reason: nanny-close-gracefully
2025-09-05 09:50:01,771 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:35183' closed.
2025-09-05 09:50:01,883 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:37937'. Reason: nanny-close-gracefully
2025-09-05 09:50:01,884 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:37937' closed.
2025-09-05 09:50:02,079 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,229 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,559 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,566 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:34793'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,567 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:34793' closed.
2025-09-05 09:50:02,622 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,679 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,695 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:46357'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,696 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:46357' closed.
2025-09-05 09:50:02,783 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,837 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,839 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,874 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,889 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,013 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:46751'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,014 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:46751' closed.
2025-09-05 09:50:03,040 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:35619'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,041 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:35619' closed.
2025-09-05 09:50:03,152 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:42863'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,153 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:42863' closed.
2025-09-05 09:50:03,179 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:41471'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,180 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:41471' closed.
2025-09-05 09:50:03,211 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,264 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,264 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,276 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,331 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:32813'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,332 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:32813' closed.
2025-09-05 09:50:03,361 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:35743'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,362 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:35743' closed.
2025-09-05 09:50:03,364 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:37803'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,365 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:37803' closed.
2025-09-05 09:50:03,403 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,422 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:35643'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,422 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:35643' closed.
2025-09-05 09:50:03,440 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:43261'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,441 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:43261' closed.
2025-09-05 09:50:03,561 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,564 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,585 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,617 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,678 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:34399'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,679 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:34399' closed.
2025-09-05 09:50:03,738 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:34817'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,739 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:34817' closed.
2025-09-05 09:50:03,741 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:39271'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,742 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:39271' closed.
2025-09-05 09:50:03,830 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:43111'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,831 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:43111' closed.
2025-09-05 09:50:03,857 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:39627'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,858 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:39627' closed.
2025-09-05 09:50:04,101 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:39139'. Reason: nanny-close-gracefully
2025-09-05 09:50:04,101 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:39139' closed.
2025-09-05 09:50:04,158 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:37169'. Reason: nanny-close-gracefully
2025-09-05 09:50:04,159 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:34159'. Reason: nanny-close-gracefully
2025-09-05 09:50:04,160 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:40071'. Reason: nanny-close-gracefully
2025-09-05 09:50:04,160 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:37169' closed.
2025-09-05 09:50:04,161 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:34159' closed.
2025-09-05 09:50:04,161 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:40071' closed.
2025-09-05 09:50:04,271 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:34671'. Reason: nanny-close-gracefully
2025-09-05 09:50:04,272 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:34671' closed.
2025-09-05 09:50:04,279 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.16:44991'. Reason: nanny-close-gracefully
2025-09-05 09:50:04,280 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.16:44991' closed.
2025-09-05 09:50:04,282 - distributed.dask_worker - INFO - End worker
