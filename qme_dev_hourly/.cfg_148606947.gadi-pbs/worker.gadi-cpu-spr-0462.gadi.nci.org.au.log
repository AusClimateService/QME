Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 10:31:51,444 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:37091'
2025-09-03 10:31:51,456 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:37983'
2025-09-03 10:31:51,462 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:43783'
2025-09-03 10:31:51,468 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:45701'
2025-09-03 10:31:51,472 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:46225'
2025-09-03 10:31:51,539 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:44225'
2025-09-03 10:31:51,544 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:34159'
2025-09-03 10:31:51,548 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:46707'
2025-09-03 10:31:51,553 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:37707'
2025-09-03 10:31:51,560 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:42195'
2025-09-03 10:31:51,566 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:46287'
2025-09-03 10:31:51,570 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:33363'
2025-09-03 10:31:51,574 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:41801'
2025-09-03 10:31:51,578 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:42589'
2025-09-03 10:31:51,583 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:40481'
2025-09-03 10:31:51,588 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:44957'
2025-09-03 10:31:51,593 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:42321'
2025-09-03 10:31:51,597 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:35563'
2025-09-03 10:31:51,602 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:33937'
2025-09-03 10:31:51,606 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:46067'
2025-09-03 10:31:51,611 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:36437'
2025-09-03 10:31:51,615 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:41683'
2025-09-03 10:31:51,619 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:43461'
2025-09-03 10:31:51,623 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:46549'
2025-09-03 10:31:51,628 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:39961'
2025-09-03 10:31:51,632 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:39735'
2025-09-03 10:31:51,636 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:37923'
2025-09-03 10:31:51,640 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:46281'
2025-09-03 10:31:51,644 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:44311'
2025-09-03 10:31:51,648 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:45629'
2025-09-03 10:31:51,651 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:44491'
2025-09-03 10:31:51,655 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:43949'
2025-09-03 10:31:51,660 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:37227'
2025-09-03 10:31:51,664 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:45193'
2025-09-03 10:31:51,667 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:34045'
2025-09-03 10:31:51,672 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:46791'
2025-09-03 10:31:51,676 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:42859'
2025-09-03 10:31:51,680 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:40599'
2025-09-03 10:31:51,688 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:36047'
2025-09-03 10:31:51,693 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:45293'
2025-09-03 10:31:51,697 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:35083'
2025-09-03 10:31:51,859 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:43181'
2025-09-03 10:31:51,864 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:46153'
2025-09-03 10:31:51,869 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:36635'
2025-09-03 10:31:51,874 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:41993'
2025-09-03 10:31:51,879 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:42383'
2025-09-03 10:31:51,882 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:37657'
2025-09-03 10:31:51,886 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:41823'
2025-09-03 10:31:51,890 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:33519'
2025-09-03 10:31:51,894 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:39585'
2025-09-03 10:31:51,899 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:40643'
2025-09-03 10:31:51,903 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:37123'
2025-09-03 10:31:51,907 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:41717'
2025-09-03 10:31:51,911 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:39641'
2025-09-03 10:31:51,917 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:41051'
2025-09-03 10:31:51,920 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:43173'
2025-09-03 10:31:51,925 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:43941'
2025-09-03 10:31:51,929 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:33115'
2025-09-03 10:31:51,934 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:39093'
2025-09-03 10:31:51,937 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:46725'
2025-09-03 10:31:51,943 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:45383'
2025-09-03 10:31:51,949 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:42883'
2025-09-03 10:31:51,953 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:34939'
2025-09-03 10:31:51,957 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:41299'
2025-09-03 10:31:51,960 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:46007'
2025-09-03 10:31:51,966 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:33301'
2025-09-03 10:31:51,970 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:33259'
2025-09-03 10:31:51,974 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:43405'
2025-09-03 10:31:51,979 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:36063'
2025-09-03 10:31:51,982 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:33665'
2025-09-03 10:31:51,987 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:46617'
2025-09-03 10:31:51,991 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:42631'
2025-09-03 10:31:51,995 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:44287'
2025-09-03 10:31:51,999 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:46399'
2025-09-03 10:31:52,004 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:33775'
2025-09-03 10:31:52,008 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:43897'
2025-09-03 10:31:52,012 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:38327'
2025-09-03 10:31:52,018 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:34101'
2025-09-03 10:31:52,040 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:32985'
2025-09-03 10:31:52,045 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:37129'
2025-09-03 10:31:52,049 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:33703'
2025-09-03 10:31:52,054 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:40129'
2025-09-03 10:31:52,059 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:36797'
2025-09-03 10:31:52,062 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:34465'
2025-09-03 10:31:52,066 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:39179'
2025-09-03 10:31:52,073 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:44739'
2025-09-03 10:31:52,075 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:45653'
2025-09-03 10:31:52,080 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:43777'
2025-09-03 10:31:52,083 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:40205'
2025-09-03 10:31:52,085 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:35579'
2025-09-03 10:31:52,088 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:46677'
2025-09-03 10:31:52,094 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:36511'
2025-09-03 10:31:52,097 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:46589'
2025-09-03 10:31:52,100 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:42807'
2025-09-03 10:31:52,105 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:37005'
2025-09-03 10:31:52,108 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:35047'
2025-09-03 10:31:52,114 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:34233'
2025-09-03 10:31:52,117 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:39377'
2025-09-03 10:31:52,122 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:33813'
2025-09-03 10:31:52,126 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:43859'
2025-09-03 10:31:52,131 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:42629'
2025-09-03 10:31:52,135 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:38435'
2025-09-03 10:31:52,140 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:33071'
2025-09-03 10:31:52,144 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.30:37179'
2025-09-03 10:31:53,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:44505
2025-09-03 10:31:53,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:37743
2025-09-03 10:31:53,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:43921
2025-09-03 10:31:53,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:44757
2025-09-03 10:31:53,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:35595
2025-09-03 10:31:53,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:44505
2025-09-03 10:31:53,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:41983
2025-09-03 10:31:53,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:37743
2025-09-03 10:31:53,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:43921
2025-09-03 10:31:53,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:46269
2025-09-03 10:31:53,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:35045
2025-09-03 10:31:53,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:44329
2025-09-03 10:31:53,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:41951
2025-09-03 10:31:53,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:44757
2025-09-03 10:31:53,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:38971
2025-09-03 10:31:53,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:34547
2025-09-03 10:31:53,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:40551
2025-09-03 10:31:53,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:40375
2025-09-03 10:31:53,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:35595
2025-09-03 10:31:53,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:33485
2025-09-03 10:31:53,236 - distributed.worker - INFO -          dashboard at:          10.6.102.30:40927
2025-09-03 10:31:53,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:46673
2025-09-03 10:31:53,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:40635
2025-09-03 10:31:53,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:43137
2025-09-03 10:31:53,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:40635
2025-09-03 10:31:53,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:41983
2025-09-03 10:31:53,236 - distributed.worker - INFO -          dashboard at:          10.6.102.30:34259
2025-09-03 10:31:53,236 - distributed.worker - INFO -          dashboard at:          10.6.102.30:39719
2025-09-03 10:31:53,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:46269
2025-09-03 10:31:53,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:37961
2025-09-03 10:31:53,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:35045
2025-09-03 10:31:53,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:44329
2025-09-03 10:31:53,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:41951
2025-09-03 10:31:53,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:38437
2025-09-03 10:31:53,236 - distributed.worker - INFO -          dashboard at:          10.6.102.30:35643
2025-09-03 10:31:53,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:38065
2025-09-03 10:31:53,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:38971
2025-09-03 10:31:53,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:34547
2025-09-03 10:31:53,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:40551
2025-09-03 10:31:53,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:40375
2025-09-03 10:31:53,236 - distributed.worker - INFO -          dashboard at:          10.6.102.30:44199
2025-09-03 10:31:53,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:33485
2025-09-03 10:31:53,236 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:46673
2025-09-03 10:31:53,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:43137
2025-09-03 10:31:53,236 - distributed.worker - INFO -          dashboard at:          10.6.102.30:33817
2025-09-03 10:31:53,236 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,236 - distributed.worker - INFO -          dashboard at:          10.6.102.30:35059
2025-09-03 10:31:53,236 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,236 - distributed.worker - INFO -          dashboard at:          10.6.102.30:42425
2025-09-03 10:31:53,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:37961
2025-09-03 10:31:53,236 - distributed.worker - INFO -          dashboard at:          10.6.102.30:40671
2025-09-03 10:31:53,236 - distributed.worker - INFO -          dashboard at:          10.6.102.30:45603
2025-09-03 10:31:53,236 - distributed.worker - INFO -          dashboard at:          10.6.102.30:39867
2025-09-03 10:31:53,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:38437
2025-09-03 10:31:53,236 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:38065
2025-09-03 10:31:53,236 - distributed.worker - INFO -          dashboard at:          10.6.102.30:33107
2025-09-03 10:31:53,236 - distributed.worker - INFO -          dashboard at:          10.6.102.30:38701
2025-09-03 10:31:53,236 - distributed.worker - INFO -          dashboard at:          10.6.102.30:32827
2025-09-03 10:31:53,236 - distributed.worker - INFO -          dashboard at:          10.6.102.30:44931
2025-09-03 10:31:53,236 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,236 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO -          dashboard at:          10.6.102.30:35537
2025-09-03 10:31:53,237 - distributed.worker - INFO -          dashboard at:          10.6.102.30:43583
2025-09-03 10:31:53,237 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO -          dashboard at:          10.6.102.30:32783
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,237 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,237 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,237 - distributed.worker - INFO -          dashboard at:          10.6.102.30:33155
2025-09-03 10:31:53,237 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,237 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO -          dashboard at:          10.6.102.30:46779
2025-09-03 10:31:53,237 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,237 - distributed.worker - INFO -          dashboard at:          10.6.102.30:34731
2025-09-03 10:31:53,237 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,237 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,237 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,237 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,237 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,237 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,237 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,237 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,237 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,237 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,237 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,237 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,237 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,237 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,237 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,237 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,237 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,237 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,237 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,237 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,237 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-upviu_6w
2025-09-03 10:31:53,237 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,237 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,237 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,237 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-nvfbjms_
2025-09-03 10:31:53,237 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,237 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-lsfy9sy_
2025-09-03 10:31:53,237 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,237 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,237 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,237 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,237 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,237 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,237 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,237 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-tga4w_de
2025-09-03 10:31:53,237 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,237 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,237 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,237 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-bm57b4y2
2025-09-03 10:31:53,237 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,237 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,237 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-cg4ppjqh
2025-09-03 10:31:53,237 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,237 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-b2m5pnw7
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-5zly2ams
2025-09-03 10:31:53,237 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-yafvh60g
2025-09-03 10:31:53,237 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,237 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-j5k3tfft
2025-09-03 10:31:53,237 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-5niq58py
2025-09-03 10:31:53,237 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,237 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-f4yshvl6
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-bslb6nic
2025-09-03 10:31:53,237 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-9a32w_ua
2025-09-03 10:31:53,237 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-tvwtinza
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-48j9qbz5
2025-09-03 10:31:53,237 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-c62kvb9x
2025-09-03 10:31:53,237 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-762ehiej
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-wbreo8y5
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_vftv127
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-lmwow68x
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,238 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,469 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:44331
2025-09-03 10:31:53,469 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:44331
2025-09-03 10:31:53,469 - distributed.worker - INFO -          dashboard at:          10.6.102.30:41127
2025-09-03 10:31:53,469 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,469 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,469 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,469 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,469 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2013yve4
2025-09-03 10:31:53,469 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,511 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:46313
2025-09-03 10:31:53,511 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:46313
2025-09-03 10:31:53,511 - distributed.worker - INFO -          dashboard at:          10.6.102.30:36151
2025-09-03 10:31:53,511 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,511 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,511 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,511 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,511 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-icx2py2z
2025-09-03 10:31:53,511 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,528 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:42211
2025-09-03 10:31:53,528 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:42211
2025-09-03 10:31:53,528 - distributed.worker - INFO -          dashboard at:          10.6.102.30:43615
2025-09-03 10:31:53,528 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,528 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,528 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,528 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,528 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-xv_qij0g
2025-09-03 10:31:53,528 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,587 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:37659
2025-09-03 10:31:53,587 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:37659
2025-09-03 10:31:53,587 - distributed.worker - INFO -          dashboard at:          10.6.102.30:35649
2025-09-03 10:31:53,587 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,587 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,587 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,587 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,587 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-e86rr0hr
2025-09-03 10:31:53,587 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,624 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:46207
2025-09-03 10:31:53,624 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:46207
2025-09-03 10:31:53,624 - distributed.worker - INFO -          dashboard at:          10.6.102.30:33621
2025-09-03 10:31:53,624 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,624 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,624 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,624 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,624 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3gd07ruv
2025-09-03 10:31:53,624 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,628 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:37999
2025-09-03 10:31:53,628 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:37999
2025-09-03 10:31:53,628 - distributed.worker - INFO -          dashboard at:          10.6.102.30:45897
2025-09-03 10:31:53,628 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,629 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,629 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,629 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,629 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-sag25u57
2025-09-03 10:31:53,629 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,632 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:45255
2025-09-03 10:31:53,632 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:45255
2025-09-03 10:31:53,632 - distributed.worker - INFO -          dashboard at:          10.6.102.30:41375
2025-09-03 10:31:53,632 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,632 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,632 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,632 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,632 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-yf1mrp2b
2025-09-03 10:31:53,632 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,637 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:37031
2025-09-03 10:31:53,637 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:37031
2025-09-03 10:31:53,637 - distributed.worker - INFO -          dashboard at:          10.6.102.30:35451
2025-09-03 10:31:53,637 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,637 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,637 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,637 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,637 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-15sxb9jl
2025-09-03 10:31:53,637 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,646 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:33969
2025-09-03 10:31:53,646 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:33969
2025-09-03 10:31:53,646 - distributed.worker - INFO -          dashboard at:          10.6.102.30:43509
2025-09-03 10:31:53,646 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,646 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,646 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,646 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,646 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-c_hy6kvl
2025-09-03 10:31:53,646 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,649 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:39941
2025-09-03 10:31:53,649 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:39941
2025-09-03 10:31:53,649 - distributed.worker - INFO -          dashboard at:          10.6.102.30:39321
2025-09-03 10:31:53,649 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,649 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,649 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,649 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,649 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-e836qc6g
2025-09-03 10:31:53,649 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,652 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:34537
2025-09-03 10:31:53,652 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:34537
2025-09-03 10:31:53,652 - distributed.worker - INFO -          dashboard at:          10.6.102.30:37379
2025-09-03 10:31:53,652 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,652 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,652 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,652 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,652 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-1s0d33k4
2025-09-03 10:31:53,652 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,656 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:41825
2025-09-03 10:31:53,656 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:41825
2025-09-03 10:31:53,657 - distributed.worker - INFO -          dashboard at:          10.6.102.30:33077
2025-09-03 10:31:53,657 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,657 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,657 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,657 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,657 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-u53v1o0m
2025-09-03 10:31:53,657 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,663 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:45037
2025-09-03 10:31:53,663 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:45037
2025-09-03 10:31:53,663 - distributed.worker - INFO -          dashboard at:          10.6.102.30:38509
2025-09-03 10:31:53,663 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,663 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,663 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,663 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-1gb37143
2025-09-03 10:31:53,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,706 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:33187
2025-09-03 10:31:53,706 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:33187
2025-09-03 10:31:53,706 - distributed.worker - INFO -          dashboard at:          10.6.102.30:37283
2025-09-03 10:31:53,706 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,706 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,706 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,706 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,706 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-bjczmq6_
2025-09-03 10:31:53,706 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,731 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:35869
2025-09-03 10:31:53,731 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:35869
2025-09-03 10:31:53,731 - distributed.worker - INFO -          dashboard at:          10.6.102.30:40501
2025-09-03 10:31:53,731 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,731 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,731 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,731 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,731 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-l3t1f8i8
2025-09-03 10:31:53,731 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,930 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:45689
2025-09-03 10:31:53,931 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:45689
2025-09-03 10:31:53,931 - distributed.worker - INFO -          dashboard at:          10.6.102.30:35993
2025-09-03 10:31:53,931 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,931 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,931 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,931 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,931 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-g9ymttp9
2025-09-03 10:31:53,931 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,933 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:43233
2025-09-03 10:31:53,933 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:43233
2025-09-03 10:31:53,933 - distributed.worker - INFO -          dashboard at:          10.6.102.30:41361
2025-09-03 10:31:53,933 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,933 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,933 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,933 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,933 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-084la00q
2025-09-03 10:31:53,933 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,979 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:35897
2025-09-03 10:31:53,979 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:35897
2025-09-03 10:31:53,979 - distributed.worker - INFO -          dashboard at:          10.6.102.30:46337
2025-09-03 10:31:53,979 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,979 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,979 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:53,979 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:53,979 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-u3br87ga
2025-09-03 10:31:53,979 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,011 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,012 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,012 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,013 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,025 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,026 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,026 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,028 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,039 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,040 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,040 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,042 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,053 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,054 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,054 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,056 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,066 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,067 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,068 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,069 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,080 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,082 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,082 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,083 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,094 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,095 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,096 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,097 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,108 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,110 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,110 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,111 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,122 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,123 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,123 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,125 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,136 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,137 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,137 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,139 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,149 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,150 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,150 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,152 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,163 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,164 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,164 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,166 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,177 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,178 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,178 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,180 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,190 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,190 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,190 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,191 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,192 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:46339
2025-09-03 10:31:54,192 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:46339
2025-09-03 10:31:54,192 - distributed.worker - INFO -          dashboard at:          10.6.102.30:42457
2025-09-03 10:31:54,192 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,192 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,192 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,192 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-81aq71_b
2025-09-03 10:31:54,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,205 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,206 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,206 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,208 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,218 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,219 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,219 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,221 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,232 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,233 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,233 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,235 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:44909
2025-09-03 10:31:54,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:44909
2025-09-03 10:31:54,236 - distributed.worker - INFO -          dashboard at:          10.6.102.30:46685
2025-09-03 10:31:54,236 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,236 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,236 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,236 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,236 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-m6dokz97
2025-09-03 10:31:54,236 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,245 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,246 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,247 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,248 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,252 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:44039
2025-09-03 10:31:54,252 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:44039
2025-09-03 10:31:54,252 - distributed.worker - INFO -          dashboard at:          10.6.102.30:45619
2025-09-03 10:31:54,252 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,252 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,252 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,252 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,252 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3o2hw72j
2025-09-03 10:31:54,252 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,257 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:34085
2025-09-03 10:31:54,258 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:34085
2025-09-03 10:31:54,258 - distributed.worker - INFO -          dashboard at:          10.6.102.30:39947
2025-09-03 10:31:54,258 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,258 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,258 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,258 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,258 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-akj95_47
2025-09-03 10:31:54,258 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,259 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,261 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,261 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,262 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,275 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,276 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,277 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,277 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:41383
2025-09-03 10:31:54,277 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:41383
2025-09-03 10:31:54,277 - distributed.worker - INFO -          dashboard at:          10.6.102.30:36613
2025-09-03 10:31:54,277 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,277 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,278 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,278 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,278 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-wgbjchji
2025-09-03 10:31:54,278 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,280 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:43381
2025-09-03 10:31:54,280 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:43381
2025-09-03 10:31:54,280 - distributed.worker - INFO -          dashboard at:          10.6.102.30:45171
2025-09-03 10:31:54,280 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,280 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,280 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,280 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,280 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-5gind8r4
2025-09-03 10:31:54,280 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,280 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,281 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:33143
2025-09-03 10:31:54,281 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:33143
2025-09-03 10:31:54,281 - distributed.worker - INFO -          dashboard at:          10.6.102.30:34213
2025-09-03 10:31:54,281 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,281 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,282 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,282 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,282 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ukox775n
2025-09-03 10:31:54,282 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,288 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,290 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,291 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,294 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,299 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:46431
2025-09-03 10:31:54,299 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:46431
2025-09-03 10:31:54,299 - distributed.worker - INFO -          dashboard at:          10.6.102.30:44151
2025-09-03 10:31:54,299 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,299 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,299 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,299 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,299 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-isavw_cw
2025-09-03 10:31:54,299 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,301 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:45489
2025-09-03 10:31:54,302 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:45489
2025-09-03 10:31:54,302 - distributed.worker - INFO -          dashboard at:          10.6.102.30:45395
2025-09-03 10:31:54,302 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,302 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,302 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,302 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,302 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-mfs0u7cv
2025-09-03 10:31:54,302 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,311 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:42303
2025-09-03 10:31:54,311 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:42303
2025-09-03 10:31:54,311 - distributed.worker - INFO -          dashboard at:          10.6.102.30:36637
2025-09-03 10:31:54,311 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,311 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,311 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,311 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,311 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-u6i215mg
2025-09-03 10:31:54,311 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,324 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:39639
2025-09-03 10:31:54,324 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:39639
2025-09-03 10:31:54,324 - distributed.worker - INFO -          dashboard at:          10.6.102.30:39283
2025-09-03 10:31:54,324 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,324 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,324 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,324 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,324 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-k667oss3
2025-09-03 10:31:54,324 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,336 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:44443
2025-09-03 10:31:54,336 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:44443
2025-09-03 10:31:54,336 - distributed.worker - INFO -          dashboard at:          10.6.102.30:44859
2025-09-03 10:31:54,336 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,337 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,337 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,337 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,337 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-52bo4c95
2025-09-03 10:31:54,337 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,347 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:44501
2025-09-03 10:31:54,347 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:44501
2025-09-03 10:31:54,347 - distributed.worker - INFO -          dashboard at:          10.6.102.30:40929
2025-09-03 10:31:54,347 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,347 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,347 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,347 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,347 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-rm7p4nbr
2025-09-03 10:31:54,348 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,353 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:42855
2025-09-03 10:31:54,353 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:42855
2025-09-03 10:31:54,353 - distributed.worker - INFO -          dashboard at:          10.6.102.30:35287
2025-09-03 10:31:54,353 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,353 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,353 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,353 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,353 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-taacnen6
2025-09-03 10:31:54,354 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,358 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:40057
2025-09-03 10:31:54,358 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:40057
2025-09-03 10:31:54,358 - distributed.worker - INFO -          dashboard at:          10.6.102.30:33531
2025-09-03 10:31:54,358 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,358 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,358 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,358 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,358 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-8j7sm_2_
2025-09-03 10:31:54,358 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,391 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:43909
2025-09-03 10:31:54,391 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:43909
2025-09-03 10:31:54,391 - distributed.worker - INFO -          dashboard at:          10.6.102.30:44197
2025-09-03 10:31:54,391 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,391 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,391 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,391 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,391 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-b17hyuh4
2025-09-03 10:31:54,391 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,481 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:46857
2025-09-03 10:31:54,481 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:46857
2025-09-03 10:31:54,481 - distributed.worker - INFO -          dashboard at:          10.6.102.30:32777
2025-09-03 10:31:54,481 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,481 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,481 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,481 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,481 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-pbsae50m
2025-09-03 10:31:54,481 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,482 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,483 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,483 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,485 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,492 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:40937
2025-09-03 10:31:54,492 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:40937
2025-09-03 10:31:54,492 - distributed.worker - INFO -          dashboard at:          10.6.102.30:45277
2025-09-03 10:31:54,492 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,492 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,492 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,492 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,492 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-pze1m0pn
2025-09-03 10:31:54,492 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,496 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,497 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,497 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,499 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,517 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:46069
2025-09-03 10:31:54,517 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:46069
2025-09-03 10:31:54,517 - distributed.worker - INFO -          dashboard at:          10.6.102.30:40033
2025-09-03 10:31:54,517 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,517 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,517 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,517 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,517 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-p9vio1gt
2025-09-03 10:31:54,517 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,524 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:44765
2025-09-03 10:31:54,524 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:44765
2025-09-03 10:31:54,524 - distributed.worker - INFO -          dashboard at:          10.6.102.30:43095
2025-09-03 10:31:54,524 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,524 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,524 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,524 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-0seudsxu
2025-09-03 10:31:54,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,534 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:40711
2025-09-03 10:31:54,534 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:40711
2025-09-03 10:31:54,534 - distributed.worker - INFO -          dashboard at:          10.6.102.30:33497
2025-09-03 10:31:54,535 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,535 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,535 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,535 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,535 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-lw6s7twk
2025-09-03 10:31:54,535 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,555 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:35297
2025-09-03 10:31:54,555 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:35297
2025-09-03 10:31:54,555 - distributed.worker - INFO -          dashboard at:          10.6.102.30:38195
2025-09-03 10:31:54,555 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,555 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,555 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,555 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:44825
2025-09-03 10:31:54,555 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,555 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:44825
2025-09-03 10:31:54,555 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-lxlxfwt0
2025-09-03 10:31:54,555 - distributed.worker - INFO -          dashboard at:          10.6.102.30:33253
2025-09-03 10:31:54,555 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,555 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,555 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,555 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,555 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,555 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-zopyko8a
2025-09-03 10:31:54,555 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,556 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:43063
2025-09-03 10:31:54,556 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:43063
2025-09-03 10:31:54,556 - distributed.worker - INFO -          dashboard at:          10.6.102.30:33231
2025-09-03 10:31:54,556 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,557 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,557 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,557 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,557 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-yo1vvduk
2025-09-03 10:31:54,557 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,563 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:46425
2025-09-03 10:31:54,563 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:46425
2025-09-03 10:31:54,563 - distributed.worker - INFO -          dashboard at:          10.6.102.30:37455
2025-09-03 10:31:54,563 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,563 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,563 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,563 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,563 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-f7bxaas0
2025-09-03 10:31:54,563 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,564 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:40689
2025-09-03 10:31:54,565 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:40689
2025-09-03 10:31:54,565 - distributed.worker - INFO -          dashboard at:          10.6.102.30:46601
2025-09-03 10:31:54,565 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,565 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,565 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,565 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,565 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-m33bxgpg
2025-09-03 10:31:54,565 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,600 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:33975
2025-09-03 10:31:54,600 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:33975
2025-09-03 10:31:54,600 - distributed.worker - INFO -          dashboard at:          10.6.102.30:46755
2025-09-03 10:31:54,600 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,600 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,600 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,600 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,600 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-l4dr38dk
2025-09-03 10:31:54,600 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,601 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:42265
2025-09-03 10:31:54,601 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:42265
2025-09-03 10:31:54,601 - distributed.worker - INFO -          dashboard at:          10.6.102.30:37567
2025-09-03 10:31:54,601 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,601 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,601 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,601 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,601 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-y9lexe4l
2025-09-03 10:31:54,601 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,622 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,623 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,623 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,625 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,636 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,637 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,637 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,639 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,652 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:38225
2025-09-03 10:31:54,652 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:38225
2025-09-03 10:31:54,652 - distributed.worker - INFO -          dashboard at:          10.6.102.30:44875
2025-09-03 10:31:54,652 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,652 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,652 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,652 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,652 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-nkoir02i
2025-09-03 10:31:54,653 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,668 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:35491
2025-09-03 10:31:54,668 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:35491
2025-09-03 10:31:54,668 - distributed.worker - INFO -          dashboard at:          10.6.102.30:38523
2025-09-03 10:31:54,668 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,668 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,668 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,668 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,668 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-vw3p7imn
2025-09-03 10:31:54,668 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,689 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,690 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,690 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,691 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,702 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,704 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,704 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,705 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,731 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,732 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,732 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,734 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,742 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:35031
2025-09-03 10:31:54,742 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:35031
2025-09-03 10:31:54,742 - distributed.worker - INFO -          dashboard at:          10.6.102.30:44441
2025-09-03 10:31:54,742 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,742 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,742 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,742 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,742 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_xwehdgb
2025-09-03 10:31:54,742 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,764 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:46367
2025-09-03 10:31:54,764 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:46367
2025-09-03 10:31:54,764 - distributed.worker - INFO -          dashboard at:          10.6.102.30:44077
2025-09-03 10:31:54,764 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,764 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,764 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,764 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,764 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-kx23wv97
2025-09-03 10:31:54,764 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,774 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,774 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:44955
2025-09-03 10:31:54,775 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:44955
2025-09-03 10:31:54,775 - distributed.worker - INFO -          dashboard at:          10.6.102.30:36087
2025-09-03 10:31:54,775 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,775 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,775 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,775 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,775 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3vozo5pp
2025-09-03 10:31:54,775 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,775 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,775 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,777 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,808 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:35299
2025-09-03 10:31:54,808 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:35299
2025-09-03 10:31:54,808 - distributed.worker - INFO -          dashboard at:          10.6.102.30:44217
2025-09-03 10:31:54,808 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,808 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,808 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,808 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,808 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-o3nn6_y0
2025-09-03 10:31:54,808 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,872 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,874 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,874 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,875 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,886 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,887 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,887 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,889 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,900 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,901 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,902 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,903 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,914 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,916 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,916 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,918 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,928 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,929 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,930 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,931 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,931 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:42919
2025-09-03 10:31:54,931 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:42919
2025-09-03 10:31:54,931 - distributed.worker - INFO -          dashboard at:          10.6.102.30:33451
2025-09-03 10:31:54,931 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,931 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,931 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,931 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,932 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4g5pqwb5
2025-09-03 10:31:54,932 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,942 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,943 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,944 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,945 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,970 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,971 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,971 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,973 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,027 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,028 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:46047
2025-09-03 10:31:55,028 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:46047
2025-09-03 10:31:55,028 - distributed.worker - INFO -          dashboard at:          10.6.102.30:41395
2025-09-03 10:31:55,028 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,028 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,028 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,028 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,028 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-x2kv13sw
2025-09-03 10:31:55,028 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,028 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,028 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,030 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,033 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:43599
2025-09-03 10:31:55,034 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:43599
2025-09-03 10:31:55,034 - distributed.worker - INFO -          dashboard at:          10.6.102.30:46417
2025-09-03 10:31:55,034 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,034 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,034 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,034 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,034 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-1bta3f8b
2025-09-03 10:31:55,034 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,041 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,042 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,042 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,044 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,055 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,056 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,056 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,058 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,068 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,069 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,070 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,071 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,079 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:44967
2025-09-03 10:31:55,079 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:44967
2025-09-03 10:31:55,079 - distributed.worker - INFO -          dashboard at:          10.6.102.30:41115
2025-09-03 10:31:55,079 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,079 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,079 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,079 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,079 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-1hqfzo03
2025-09-03 10:31:55,079 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,082 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,083 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:42143
2025-09-03 10:31:55,083 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:42143
2025-09-03 10:31:55,083 - distributed.worker - INFO -          dashboard at:          10.6.102.30:44137
2025-09-03 10:31:55,083 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,083 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,083 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,083 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,083 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-wknhtlc8
2025-09-03 10:31:55,083 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,083 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,084 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,085 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,111 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,112 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,112 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,114 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,121 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:43939
2025-09-03 10:31:55,121 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:43939
2025-09-03 10:31:55,121 - distributed.worker - INFO -          dashboard at:          10.6.102.30:34539
2025-09-03 10:31:55,121 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,121 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:35073
2025-09-03 10:31:55,121 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,121 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,121 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:35073
2025-09-03 10:31:55,121 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,121 - distributed.worker - INFO -          dashboard at:          10.6.102.30:43205
2025-09-03 10:31:55,121 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-l_t4i_ya
2025-09-03 10:31:55,121 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,121 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,121 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,121 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,121 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,121 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-rvp0bu87
2025-09-03 10:31:55,121 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,125 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,126 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,126 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,128 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,139 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:40089
2025-09-03 10:31:55,139 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:40089
2025-09-03 10:31:55,139 - distributed.worker - INFO -          dashboard at:          10.6.102.30:33303
2025-09-03 10:31:55,139 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,139 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,139 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,139 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,139 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-z8o_o4hc
2025-09-03 10:31:55,140 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,145 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:36577
2025-09-03 10:31:55,145 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:36577
2025-09-03 10:31:55,145 - distributed.worker - INFO -          dashboard at:          10.6.102.30:41393
2025-09-03 10:31:55,145 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,145 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,145 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,145 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,145 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-avkgigj9
2025-09-03 10:31:55,145 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,145 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:36471
2025-09-03 10:31:55,146 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:36471
2025-09-03 10:31:55,146 - distributed.worker - INFO -          dashboard at:          10.6.102.30:37705
2025-09-03 10:31:55,146 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,146 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,146 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,146 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,146 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-61ueneei
2025-09-03 10:31:55,146 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,152 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,153 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:45119
2025-09-03 10:31:55,153 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:45119
2025-09-03 10:31:55,153 - distributed.worker - INFO -          dashboard at:          10.6.102.30:33027
2025-09-03 10:31:55,153 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,153 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,153 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,154 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,153 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,154 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-86vj7kjz
2025-09-03 10:31:55,154 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,154 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,155 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,158 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:36975
2025-09-03 10:31:55,158 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:36975
2025-09-03 10:31:55,158 - distributed.worker - INFO -          dashboard at:          10.6.102.30:41191
2025-09-03 10:31:55,158 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,158 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,158 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,158 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,158 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-f4kcqd76
2025-09-03 10:31:55,159 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,167 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,168 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,168 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,170 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,173 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:40637
2025-09-03 10:31:55,173 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:40637
2025-09-03 10:31:55,173 - distributed.worker - INFO -          dashboard at:          10.6.102.30:39599
2025-09-03 10:31:55,173 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,173 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,173 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,173 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-484dxzqz
2025-09-03 10:31:55,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,173 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:34143
2025-09-03 10:31:55,173 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:37965
2025-09-03 10:31:55,173 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:34143
2025-09-03 10:31:55,173 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:37965
2025-09-03 10:31:55,173 - distributed.worker - INFO -          dashboard at:          10.6.102.30:45769
2025-09-03 10:31:55,173 - distributed.worker - INFO -          dashboard at:          10.6.102.30:33267
2025-09-03 10:31:55,173 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,173 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,173 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,173 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,173 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,173 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,173 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-x6sj4_0l
2025-09-03 10:31:55,173 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2viw_9xt
2025-09-03 10:31:55,174 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,174 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,175 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:39841
2025-09-03 10:31:55,175 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:39841
2025-09-03 10:31:55,175 - distributed.worker - INFO -          dashboard at:          10.6.102.30:44933
2025-09-03 10:31:55,176 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,176 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,176 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,176 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,176 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-tkqmc45j
2025-09-03 10:31:55,176 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,176 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:33117
2025-09-03 10:31:55,176 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:33117
2025-09-03 10:31:55,176 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:35157
2025-09-03 10:31:55,176 - distributed.worker - INFO -          dashboard at:          10.6.102.30:34851
2025-09-03 10:31:55,176 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:35157
2025-09-03 10:31:55,176 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,176 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,176 - distributed.worker - INFO -          dashboard at:          10.6.102.30:39249
2025-09-03 10:31:55,176 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,176 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,176 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,176 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,176 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,176 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-s67dn09d
2025-09-03 10:31:55,176 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,177 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,177 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-airjleau
2025-09-03 10:31:55,177 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,178 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:33393
2025-09-03 10:31:55,178 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:33393
2025-09-03 10:31:55,178 - distributed.worker - INFO -          dashboard at:          10.6.102.30:41777
2025-09-03 10:31:55,178 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,178 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,178 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,178 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,178 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-1080skb6
2025-09-03 10:31:55,178 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,186 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:36407
2025-09-03 10:31:55,186 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:36407
2025-09-03 10:31:55,186 - distributed.worker - INFO -          dashboard at:          10.6.102.30:34571
2025-09-03 10:31:55,186 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,186 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,186 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,186 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,186 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ltpexj74
2025-09-03 10:31:55,186 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,188 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:38049
2025-09-03 10:31:55,189 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:38049
2025-09-03 10:31:55,189 - distributed.worker - INFO -          dashboard at:          10.6.102.30:46419
2025-09-03 10:31:55,189 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,189 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,189 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,189 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,189 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-jkg3mly1
2025-09-03 10:31:55,189 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,191 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:33287
2025-09-03 10:31:55,191 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:33287
2025-09-03 10:31:55,191 - distributed.worker - INFO -          dashboard at:          10.6.102.30:35409
2025-09-03 10:31:55,191 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,191 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,191 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,191 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,191 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-as5i1z0j
2025-09-03 10:31:55,191 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,195 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:41727
2025-09-03 10:31:55,195 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:41727
2025-09-03 10:31:55,195 - distributed.worker - INFO -          dashboard at:          10.6.102.30:34591
2025-09-03 10:31:55,195 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,195 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,195 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,195 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-d3e5k12b
2025-09-03 10:31:55,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,195 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:44593
2025-09-03 10:31:55,195 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:44593
2025-09-03 10:31:55,195 - distributed.worker - INFO -          dashboard at:          10.6.102.30:35039
2025-09-03 10:31:55,195 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,196 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,196 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,196 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-aq3guy0u
2025-09-03 10:31:55,196 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,200 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:35513
2025-09-03 10:31:55,200 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:35513
2025-09-03 10:31:55,200 - distributed.worker - INFO -          dashboard at:          10.6.102.30:43245
2025-09-03 10:31:55,200 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,200 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,200 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,200 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,200 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-fv7uomny
2025-09-03 10:31:55,200 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,204 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:34269
2025-09-03 10:31:55,204 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:34269
2025-09-03 10:31:55,204 - distributed.worker - INFO -          dashboard at:          10.6.102.30:42733
2025-09-03 10:31:55,204 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,204 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,204 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,204 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,204 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-bw11yf5g
2025-09-03 10:31:55,204 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,205 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:35353
2025-09-03 10:31:55,206 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:35353
2025-09-03 10:31:55,206 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:40645
2025-09-03 10:31:55,206 - distributed.worker - INFO -          dashboard at:          10.6.102.30:46391
2025-09-03 10:31:55,206 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:40645
2025-09-03 10:31:55,206 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,206 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,206 - distributed.worker - INFO -          dashboard at:          10.6.102.30:42385
2025-09-03 10:31:55,206 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,206 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,206 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,206 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,206 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,206 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4rtvik2v
2025-09-03 10:31:55,206 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,206 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,206 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-tqnzbryd
2025-09-03 10:31:55,206 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,207 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:40399
2025-09-03 10:31:55,207 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:40399
2025-09-03 10:31:55,207 - distributed.worker - INFO -          dashboard at:          10.6.102.30:42713
2025-09-03 10:31:55,207 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,207 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,207 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,207 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,207 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2rbdj0yx
2025-09-03 10:31:55,207 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,209 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:40837
2025-09-03 10:31:55,209 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:40837
2025-09-03 10:31:55,209 - distributed.worker - INFO -          dashboard at:          10.6.102.30:40169
2025-09-03 10:31:55,209 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,209 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,209 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,209 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,209 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-haua2w0u
2025-09-03 10:31:55,209 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,224 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.30:35925
2025-09-03 10:31:55,224 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.30:35925
2025-09-03 10:31:55,224 - distributed.worker - INFO -          dashboard at:          10.6.102.30:37883
2025-09-03 10:31:55,224 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,224 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,224 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:55,224 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:55,224 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-yqnogzwn
2025-09-03 10:31:55,224 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,344 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,345 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,346 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,347 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,443 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,444 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,444 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,446 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,483 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,484 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,484 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,486 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,577 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,581 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,581 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,585 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,586 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,586 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,587 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,588 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,599 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,600 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,600 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,602 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,613 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,614 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,614 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,616 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,627 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,628 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,628 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,630 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,641 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,642 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,642 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,644 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,655 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,657 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,657 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,659 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,669 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,670 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,671 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,673 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,683 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,684 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,684 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,686 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,895 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,897 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,897 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,899 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,911 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,912 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,913 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,914 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,925 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,926 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,926 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,928 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,939 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,940 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,940 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,942 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,953 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,954 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,954 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,956 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,564 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,565 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,566 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,567 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,579 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,580 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,580 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,582 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,671 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,672 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,672 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,674 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,686 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,687 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,688 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,689 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,701 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,703 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,703 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,705 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,715 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,716 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,716 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,718 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,745 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,746 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,747 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,748 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,760 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,761 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,761 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,763 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,774 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,775 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,775 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,777 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,788 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,789 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,789 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,791 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,801 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,802 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,803 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,804 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,815 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,817 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,817 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,819 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,830 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,831 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,831 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,833 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,844 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,845 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,846 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,847 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,858 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,859 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,860 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,862 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,872 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,873 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,874 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,876 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,887 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,888 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,888 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,890 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,901 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,902 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,902 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,904 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,915 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,916 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,916 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,918 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,929 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,931 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,931 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,933 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,943 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,944 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,944 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,946 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,957 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,959 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,959 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,961 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,971 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,972 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,972 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,974 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,986 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,987 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,987 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,990 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,999 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,001 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,001 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,003 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,014 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,015 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,016 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,018 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,029 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,030 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,030 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,032 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,825 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,827 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,827 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,829 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,841 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,842 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,843 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,844 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,855 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,857 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,857 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,858 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:03,036 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:03,037 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:03,038 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:03,039 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:17,342 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:17,344 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:17,344 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:17,346 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:17,358 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:17,360 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:17,360 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:17,362 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:17,374 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:17,375 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:17,376 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:17,377 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:17,390 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:17,392 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:17,392 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:17,394 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:17,407 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:17,408 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:17,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:17,410 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:17,423 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:17,425 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:17,425 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:17,427 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:17,439 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:17,441 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:17,441 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:17,442 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:24,933 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:25,208 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:25,210 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:25,212 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:25,225 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:25,225 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:25,238 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:25,251 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:25,628 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:25,642 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:25,920 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:26,591 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:26,593 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:26,593 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:26,595 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:26,915 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:26,930 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:26,944 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:26,959 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:27,794 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:27,809 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:27,820 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:27,836 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:55,034 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:55,309 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:55,311 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:55,380 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:55,381 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:55,381 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:55,383 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:55,511 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:55,512 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:55,512 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:55,514 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:55,529 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:55,530 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:55,530 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:55,532 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:57,596 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:58,794 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:58,808 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:58,821 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:58,835 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:35:51,375 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,377 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,644 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,649 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,793 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,794 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,885 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,886 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,972 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,973 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,982 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,984 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,048 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,054 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,129 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,131 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,173 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,174 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,092 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,092 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,094 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,099 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,140 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,145 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,164 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,165 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,168 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,169 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,171 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,175 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,421 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,422 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,424 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,426 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,450 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,452 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,470 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,471 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,574 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,581 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,897 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,899 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,981 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,982 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,986 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,991 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,993 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,995 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,045 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,046 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,114 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,120 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,245 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,247 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,259 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,265 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,265 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,267 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,278 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,280 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,293 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,295 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,371 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,372 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,389 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,390 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,418 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,421 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,474 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,480 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,903 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,910 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,927 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,933 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,120 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,126 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,153 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,158 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,163 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,164 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,240 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,245 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,271 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,273 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,362 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,363 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,386 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,392 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,485 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,491 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,044 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,051 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,136 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,139 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,286 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,291 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,291 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,297 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,304 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,310 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,385 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,391 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,631 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,637 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,709 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,712 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,770 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,776 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,785 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,791 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,795 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,801 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,918 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,919 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,957 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,961 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,017 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,022 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,156 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,158 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,256 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,261 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,281 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,282 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,523 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,524 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,534 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,539 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,540 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,542 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,621 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,628 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,637 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,639 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,709 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,710 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,715 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,716 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,783 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,785 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,062 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,064 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,077 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,079 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,430 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,431 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,456 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,457 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,530 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,535 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,628 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,629 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,737 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,739 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,826 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,829 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,882 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,886 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,060 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,065 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,362 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,368 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,149 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,151 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,233 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,234 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,448 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,454 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,493 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,494 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,622 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,624 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,965 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,966 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,972 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,978 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,323 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,325 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,428 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,430 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,436 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,438 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,571 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,577 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,712 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,715 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,904 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,906 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,919 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,920 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,022 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,027 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,163 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,169 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,239 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,241 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,619 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,620 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,919 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,922 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:03,519 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:03,521 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:03,793 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:03,795 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:07,397 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:07,403 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:12,527 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:12,533 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:22,884 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,886 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,887 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,885 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,889 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,890 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,892 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,894 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,901 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,901 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,903 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,903 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,907 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,910 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,909 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,910 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,912 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,912 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,919 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,921 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,924 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,926 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,927 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,927 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,927 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,929 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,929 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,930 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,934 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,936 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,388 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,391 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,391 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,393 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,393 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,395 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,409 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,412 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,495 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,497 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,506 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,508 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,509 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,510 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,511 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,513 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,515 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,522 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,522 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,522 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,524 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,523 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,527 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,530 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,530 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,531 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,530 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,533 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,533 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,533 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,535 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,535 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,535 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,536 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,537 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,538 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,538 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,539 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,540 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,541 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,558 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,561 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,802 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,805 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,843 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,845 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,846 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,847 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,846 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,847 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,849 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,849 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,860 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,862 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,866 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,869 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,874 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,876 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,877 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,879 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,894 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,894 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,896 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,897 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,899 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,904 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,910 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,912 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,912 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,913 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,914 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,914 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,098 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,100 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,099 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,101 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,103 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,103 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,122 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,125 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,128 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,130 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,333 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,335 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,337 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,339 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,342 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,344 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,390 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,395 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,400 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,403 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,405 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,406 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,407 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,408 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,410 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,412 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,432 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,433 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,434 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,435 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,453 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,457 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,458 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,459 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,508 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,511 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,513 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,515 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,627 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,629 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,643 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,648 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,660 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,662 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,663 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,664 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,665 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,670 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,669 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,674 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,688 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,690 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,750 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,752 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,858 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,860 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,860 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,862 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,862 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,864 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,923 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,925 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,928 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,928 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,930 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,930 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,946 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,948 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,967 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,968 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,969 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,970 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,987 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,989 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,016 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,018 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,026 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,029 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,039 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,041 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,041 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,043 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,089 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,091 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,254 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,255 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,272 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,274 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,319 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,321 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,423 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,425 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,485 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,490 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,835 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,837 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,900 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,902 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,080 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,082 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,227 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,229 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,428 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,430 - distributed.utils - INFO - Reload module qme_vars from .py file
