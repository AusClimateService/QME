Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-10 15:43:16,196 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:43655'
2025-09-10 15:43:16,204 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:38601'
2025-09-10 15:43:16,210 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:41045'
2025-09-10 15:43:16,214 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:43377'
2025-09-10 15:43:16,218 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:38215'
2025-09-10 15:43:16,222 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:44263'
2025-09-10 15:43:16,226 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:33525'
2025-09-10 15:43:16,230 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:36237'
2025-09-10 15:43:16,234 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:43147'
2025-09-10 15:43:16,239 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:39539'
2025-09-10 15:43:16,243 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:35503'
2025-09-10 15:43:16,246 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:41355'
2025-09-10 15:43:16,252 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:43685'
2025-09-10 15:43:16,256 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:34603'
2025-09-10 15:43:16,260 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:41771'
2025-09-10 15:43:16,264 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:33753'
2025-09-10 15:43:16,268 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:39625'
2025-09-10 15:43:16,273 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:39741'
2025-09-10 15:43:16,277 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:35999'
2025-09-10 15:43:16,282 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:33155'
2025-09-10 15:43:16,405 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:42347'
2025-09-10 15:43:16,409 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:44515'
2025-09-10 15:43:16,414 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:45081'
2025-09-10 15:43:16,419 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:34973'
2025-09-10 15:43:16,424 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:33113'
2025-09-10 15:43:16,429 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:37713'
2025-09-10 15:43:16,434 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:42131'
2025-09-10 15:43:16,438 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:45547'
2025-09-10 15:43:16,443 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:34911'
2025-09-10 15:43:16,448 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:38325'
2025-09-10 15:43:16,452 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:39395'
2025-09-10 15:43:16,456 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:45757'
2025-09-10 15:43:16,461 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:41847'
2025-09-10 15:43:16,466 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:33625'
2025-09-10 15:43:16,470 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:36295'
2025-09-10 15:43:16,474 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:36137'
2025-09-10 15:43:16,478 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:35989'
2025-09-10 15:43:16,482 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:45305'
2025-09-10 15:43:16,488 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:42977'
2025-09-10 15:43:16,493 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:40649'
2025-09-10 15:43:16,497 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:36961'
2025-09-10 15:43:16,501 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:44149'
2025-09-10 15:43:16,505 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:43373'
2025-09-10 15:43:16,510 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:45873'
2025-09-10 15:43:16,514 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:39367'
2025-09-10 15:43:16,519 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:34013'
2025-09-10 15:43:16,523 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:37143'
2025-09-10 15:43:16,528 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:39581'
2025-09-10 15:43:16,532 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:44321'
2025-09-10 15:43:16,536 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:43653'
2025-09-10 15:43:16,541 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:45527'
2025-09-10 15:43:16,545 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.36:43867'
2025-09-10 15:43:17,314 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:45277
2025-09-10 15:43:17,314 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:36045
2025-09-10 15:43:17,314 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:45315
2025-09-10 15:43:17,314 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:44371
2025-09-10 15:43:17,314 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:39005
2025-09-10 15:43:17,314 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:44357
2025-09-10 15:43:17,314 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:46217
2025-09-10 15:43:17,314 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:45277
2025-09-10 15:43:17,314 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:40401
2025-09-10 15:43:17,314 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:36045
2025-09-10 15:43:17,314 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:33115
2025-09-10 15:43:17,314 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:45315
2025-09-10 15:43:17,314 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:44371
2025-09-10 15:43:17,314 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:39005
2025-09-10 15:43:17,314 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:43937
2025-09-10 15:43:17,314 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:44357
2025-09-10 15:43:17,314 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:35823
2025-09-10 15:43:17,314 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:33555
2025-09-10 15:43:17,314 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:46217
2025-09-10 15:43:17,314 - distributed.worker - INFO -          dashboard at:          10.6.102.36:43843
2025-09-10 15:43:17,314 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:40401
2025-09-10 15:43:17,314 - distributed.worker - INFO -          dashboard at:          10.6.102.36:33827
2025-09-10 15:43:17,314 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:35745
2025-09-10 15:43:17,314 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:33115
2025-09-10 15:43:17,314 - distributed.worker - INFO -          dashboard at:          10.6.102.36:34337
2025-09-10 15:43:17,314 - distributed.worker - INFO -          dashboard at:          10.6.102.36:43951
2025-09-10 15:43:17,314 - distributed.worker - INFO -          dashboard at:          10.6.102.36:44329
2025-09-10 15:43:17,315 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:43937
2025-09-10 15:43:17,315 - distributed.worker - INFO -          dashboard at:          10.6.102.36:36125
2025-09-10 15:43:17,315 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:35823
2025-09-10 15:43:17,315 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:33555
2025-09-10 15:43:17,315 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,315 - distributed.worker - INFO -          dashboard at:          10.6.102.36:42721
2025-09-10 15:43:17,315 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,315 - distributed.worker - INFO -          dashboard at:          10.6.102.36:40185
2025-09-10 15:43:17,315 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:35745
2025-09-10 15:43:17,315 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,315 - distributed.worker - INFO -          dashboard at:          10.6.102.36:42945
2025-09-10 15:43:17,315 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,315 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,315 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,315 - distributed.worker - INFO -          dashboard at:          10.6.102.36:43093
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO -          dashboard at:          10.6.102.36:38525
2025-09-10 15:43:17,315 - distributed.worker - INFO -          dashboard at:          10.6.102.36:45509
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,315 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO -          dashboard at:          10.6.102.36:39471
2025-09-10 15:43:17,315 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,315 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,315 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,315 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,315 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,315 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,315 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,315 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,315 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,315 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,315 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,315 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-4454i_au
2025-09-10 15:43:17,315 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,315 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,315 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,315 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,315 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-gfu_ygtz
2025-09-10 15:43:17,315 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,315 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,315 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,315 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,315 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,315 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-uy6z1aoi
2025-09-10 15:43:17,315 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-zlbaws4z
2025-09-10 15:43:17,315 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-we04vzcp
2025-09-10 15:43:17,315 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-y96pjl8y
2025-09-10 15:43:17,315 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,315 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,315 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-usciteda
2025-09-10 15:43:17,315 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-tk0xejtk
2025-09-10 15:43:17,315 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,315 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-xzhh0af3
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-kmkmkhm_
2025-09-10 15:43:17,315 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-cam8aeql
2025-09-10 15:43:17,315 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-hwzpiu5e
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-guxka088
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,318 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:40623
2025-09-10 15:43:17,318 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:40623
2025-09-10 15:43:17,318 - distributed.worker - INFO -          dashboard at:          10.6.102.36:34809
2025-09-10 15:43:17,318 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,318 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,318 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,318 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,318 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-kjjp7jwk
2025-09-10 15:43:17,318 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,323 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:44545
2025-09-10 15:43:17,323 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:44545
2025-09-10 15:43:17,323 - distributed.worker - INFO -          dashboard at:          10.6.102.36:39557
2025-09-10 15:43:17,323 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,323 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,323 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,323 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,323 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-h3avemkc
2025-09-10 15:43:17,323 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,324 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:42311
2025-09-10 15:43:17,324 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:42311
2025-09-10 15:43:17,324 - distributed.worker - INFO -          dashboard at:          10.6.102.36:42525
2025-09-10 15:43:17,324 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,324 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,324 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,324 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,324 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-vercp4n1
2025-09-10 15:43:17,324 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,326 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:43279
2025-09-10 15:43:17,326 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:43279
2025-09-10 15:43:17,326 - distributed.worker - INFO -          dashboard at:          10.6.102.36:43929
2025-09-10 15:43:17,326 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,326 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,326 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,326 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,326 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-103eh4v0
2025-09-10 15:43:17,326 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,352 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:33269
2025-09-10 15:43:17,352 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:33269
2025-09-10 15:43:17,352 - distributed.worker - INFO -          dashboard at:          10.6.102.36:43243
2025-09-10 15:43:17,352 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,352 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,353 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,353 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,353 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-wzyo5nb8
2025-09-10 15:43:17,353 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,392 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,393 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,393 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,401 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,403 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,403 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,404 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,408 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,409 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,409 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,411 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,414 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,416 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,416 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,417 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,421 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,422 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,422 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,424 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,427 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,428 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,428 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,430 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,433 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,434 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,434 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,436 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,439 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,440 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,440 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,442 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,444 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,445 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,446 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,447 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,450 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,451 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,451 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,453 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,455 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,455 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,455 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,456 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,461 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,462 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,462 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,464 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,466 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,467 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,467 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,469 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,471 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,473 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,473 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,474 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,477 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,478 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,478 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,480 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,482 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,483 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,483 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,485 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,487 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,488 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,488 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,490 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,495 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,496 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,496 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,498 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,532 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:42165
2025-09-10 15:43:17,532 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:42165
2025-09-10 15:43:17,532 - distributed.worker - INFO -          dashboard at:          10.6.102.36:43887
2025-09-10 15:43:17,532 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,532 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,532 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,532 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,532 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-ina3n_th
2025-09-10 15:43:17,533 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,536 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:34093
2025-09-10 15:43:17,537 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:34093
2025-09-10 15:43:17,537 - distributed.worker - INFO -          dashboard at:          10.6.102.36:46349
2025-09-10 15:43:17,537 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,537 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,537 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,537 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,537 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-xzrbp35m
2025-09-10 15:43:17,537 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,552 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:36265
2025-09-10 15:43:17,553 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:36265
2025-09-10 15:43:17,553 - distributed.worker - INFO -          dashboard at:          10.6.102.36:36669
2025-09-10 15:43:17,553 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,553 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,553 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,553 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,553 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-64qk9z1a
2025-09-10 15:43:17,553 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,558 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:32991
2025-09-10 15:43:17,558 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:32991
2025-09-10 15:43:17,558 - distributed.worker - INFO -          dashboard at:          10.6.102.36:37565
2025-09-10 15:43:17,558 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,558 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,558 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,558 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,558 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-0m7i9aes
2025-09-10 15:43:17,558 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,564 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:38293
2025-09-10 15:43:17,564 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:38293
2025-09-10 15:43:17,564 - distributed.worker - INFO -          dashboard at:          10.6.102.36:40523
2025-09-10 15:43:17,564 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,564 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,564 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,564 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,564 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-_imm0hdz
2025-09-10 15:43:17,564 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,565 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,566 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,566 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,567 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:38871
2025-09-10 15:43:17,567 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:38871
2025-09-10 15:43:17,567 - distributed.worker - INFO -          dashboard at:          10.6.102.36:32841
2025-09-10 15:43:17,567 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,567 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,567 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,567 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,567 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-6eki8eey
2025-09-10 15:43:17,567 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,568 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,577 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,578 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,578 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,580 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,602 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:44953
2025-09-10 15:43:17,602 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:44953
2025-09-10 15:43:17,602 - distributed.worker - INFO -          dashboard at:          10.6.102.36:34887
2025-09-10 15:43:17,602 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,602 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,602 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,602 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,602 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-lfxe_u_k
2025-09-10 15:43:17,602 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,603 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,604 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,604 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,606 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,611 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,612 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,612 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,614 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,618 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,619 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,619 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,621 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,623 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,624 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,624 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,626 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,644 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:44971
2025-09-10 15:43:17,644 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:44971
2025-09-10 15:43:17,644 - distributed.worker - INFO -          dashboard at:          10.6.102.36:42813
2025-09-10 15:43:17,644 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,644 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,644 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,644 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,644 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-dixgl8wx
2025-09-10 15:43:17,644 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,647 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,648 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,648 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,650 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,664 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:40857
2025-09-10 15:43:17,664 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:40857
2025-09-10 15:43:17,664 - distributed.worker - INFO -          dashboard at:          10.6.102.36:43629
2025-09-10 15:43:17,664 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,664 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,664 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,664 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,664 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-w235jxqx
2025-09-10 15:43:17,664 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,667 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:40493
2025-09-10 15:43:17,667 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:40493
2025-09-10 15:43:17,667 - distributed.worker - INFO -          dashboard at:          10.6.102.36:42325
2025-09-10 15:43:17,667 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,667 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,667 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,667 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,667 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-thls5iit
2025-09-10 15:43:17,667 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,673 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,674 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:44561
2025-09-10 15:43:17,674 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:44561
2025-09-10 15:43:17,674 - distributed.worker - INFO -          dashboard at:          10.6.102.36:39291
2025-09-10 15:43:17,674 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,674 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,674 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,674 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,674 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,674 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-0_30fmkc
2025-09-10 15:43:17,674 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,674 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,675 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,702 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,703 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,703 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,705 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,711 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,712 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,712 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,714 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,718 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,719 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,719 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,721 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,823 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:39281
2025-09-10 15:43:17,823 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:39281
2025-09-10 15:43:17,823 - distributed.worker - INFO -          dashboard at:          10.6.102.36:40385
2025-09-10 15:43:17,823 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,823 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,823 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,823 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,823 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-yehm4jt5
2025-09-10 15:43:17,823 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,833 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:40817
2025-09-10 15:43:17,833 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:40817
2025-09-10 15:43:17,833 - distributed.worker - INFO -          dashboard at:          10.6.102.36:41517
2025-09-10 15:43:17,833 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,833 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,833 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,833 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,833 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-uandythv
2025-09-10 15:43:17,833 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,839 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:46739
2025-09-10 15:43:17,839 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:46739
2025-09-10 15:43:17,839 - distributed.worker - INFO -          dashboard at:          10.6.102.36:42929
2025-09-10 15:43:17,839 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,839 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,839 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,839 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,839 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-2y5ej5oc
2025-09-10 15:43:17,839 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,842 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:33045
2025-09-10 15:43:17,843 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:33045
2025-09-10 15:43:17,843 - distributed.worker - INFO -          dashboard at:          10.6.102.36:36545
2025-09-10 15:43:17,843 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,843 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,843 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,843 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,843 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-x4m8f13h
2025-09-10 15:43:17,843 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,843 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:35965
2025-09-10 15:43:17,843 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:35965
2025-09-10 15:43:17,843 - distributed.worker - INFO -          dashboard at:          10.6.102.36:33861
2025-09-10 15:43:17,843 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,843 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,843 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,843 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,843 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-0xdxq7qv
2025-09-10 15:43:17,844 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,846 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:41409
2025-09-10 15:43:17,846 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:41409
2025-09-10 15:43:17,846 - distributed.worker - INFO -          dashboard at:          10.6.102.36:44251
2025-09-10 15:43:17,846 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,846 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,846 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,846 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,846 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-b5lq2eg1
2025-09-10 15:43:17,846 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,846 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:39355
2025-09-10 15:43:17,847 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:39355
2025-09-10 15:43:17,847 - distributed.worker - INFO -          dashboard at:          10.6.102.36:37959
2025-09-10 15:43:17,847 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,847 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,847 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,847 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,847 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-s08lj91w
2025-09-10 15:43:17,847 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,853 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:42139
2025-09-10 15:43:17,853 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:42139
2025-09-10 15:43:17,853 - distributed.worker - INFO -          dashboard at:          10.6.102.36:41695
2025-09-10 15:43:17,853 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,853 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,853 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,853 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,853 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-jn5ircky
2025-09-10 15:43:17,853 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,855 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:39907
2025-09-10 15:43:17,855 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:39907
2025-09-10 15:43:17,855 - distributed.worker - INFO -          dashboard at:          10.6.102.36:36781
2025-09-10 15:43:17,855 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:32911
2025-09-10 15:43:17,855 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,855 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,855 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:32911
2025-09-10 15:43:17,855 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,855 - distributed.worker - INFO -          dashboard at:          10.6.102.36:35115
2025-09-10 15:43:17,855 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,855 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,855 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-62091cu3
2025-09-10 15:43:17,855 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,855 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,855 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,855 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,855 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-e01vzcur
2025-09-10 15:43:17,855 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,857 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:40887
2025-09-10 15:43:17,857 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:40887
2025-09-10 15:43:17,857 - distributed.worker - INFO -          dashboard at:          10.6.102.36:41655
2025-09-10 15:43:17,857 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,857 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,857 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,857 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,857 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-eibeooec
2025-09-10 15:43:17,857 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,858 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,859 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,859 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,859 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:33677
2025-09-10 15:43:17,859 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:33677
2025-09-10 15:43:17,859 - distributed.worker - INFO -          dashboard at:          10.6.102.36:39733
2025-09-10 15:43:17,859 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,859 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,859 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,859 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,859 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-zad75wuv
2025-09-10 15:43:17,860 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,860 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:46061
2025-09-10 15:43:17,860 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:46061
2025-09-10 15:43:17,860 - distributed.worker - INFO -          dashboard at:          10.6.102.36:36885
2025-09-10 15:43:17,860 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,860 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,860 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,860 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,860 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-m97dx4r6
2025-09-10 15:43:17,860 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,861 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,862 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:41125
2025-09-10 15:43:17,862 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:41125
2025-09-10 15:43:17,862 - distributed.worker - INFO -          dashboard at:          10.6.102.36:36403
2025-09-10 15:43:17,862 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,862 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,862 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,862 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,862 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-tym4u9da
2025-09-10 15:43:17,862 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,863 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:36859
2025-09-10 15:43:17,863 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:36859
2025-09-10 15:43:17,863 - distributed.worker - INFO -          dashboard at:          10.6.102.36:44925
2025-09-10 15:43:17,863 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,863 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,863 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,863 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,863 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-kklflid_
2025-09-10 15:43:17,863 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,864 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:35403
2025-09-10 15:43:17,864 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:35403
2025-09-10 15:43:17,864 - distributed.worker - INFO -          dashboard at:          10.6.102.36:37457
2025-09-10 15:43:17,864 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,864 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,864 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,864 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,864 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-50mbz3th
2025-09-10 15:43:17,864 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,866 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:41085
2025-09-10 15:43:17,866 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:41085
2025-09-10 15:43:17,866 - distributed.worker - INFO -          dashboard at:          10.6.102.36:33437
2025-09-10 15:43:17,866 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,866 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,866 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,866 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,866 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-jn0f4d78
2025-09-10 15:43:17,866 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,866 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:44391
2025-09-10 15:43:17,866 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:44391
2025-09-10 15:43:17,866 - distributed.worker - INFO -          dashboard at:          10.6.102.36:44573
2025-09-10 15:43:17,866 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,867 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,867 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,867 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,867 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-v39d3e3k
2025-09-10 15:43:17,867 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,869 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,870 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,870 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,871 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,873 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:37659
2025-09-10 15:43:17,873 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:37659
2025-09-10 15:43:17,873 - distributed.worker - INFO -          dashboard at:          10.6.102.36:38185
2025-09-10 15:43:17,873 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,873 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,873 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,873 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,873 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-3qaehooq
2025-09-10 15:43:17,873 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,875 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:38167
2025-09-10 15:43:17,875 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:38167
2025-09-10 15:43:17,875 - distributed.worker - INFO -          dashboard at:          10.6.102.36:34415
2025-09-10 15:43:17,875 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,875 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,875 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,875 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,875 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-f3fseapz
2025-09-10 15:43:17,875 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,878 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:43961
2025-09-10 15:43:17,878 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:43961
2025-09-10 15:43:17,878 - distributed.worker - INFO -          dashboard at:          10.6.102.36:44707
2025-09-10 15:43:17,878 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,878 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,878 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,878 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,878 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-ma28jqeo
2025-09-10 15:43:17,878 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,878 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:40017
2025-09-10 15:43:17,879 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:40017
2025-09-10 15:43:17,879 - distributed.worker - INFO -          dashboard at:          10.6.102.36:37919
2025-09-10 15:43:17,879 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,879 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,879 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,879 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,879 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-p9zvkruj
2025-09-10 15:43:17,879 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,880 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,881 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,881 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,882 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.36:37955
2025-09-10 15:43:17,882 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.36:37955
2025-09-10 15:43:17,882 - distributed.worker - INFO -          dashboard at:          10.6.102.36:43259
2025-09-10 15:43:17,882 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,882 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,882 - distributed.worker - INFO -               Threads:                          2
2025-09-10 15:43:17,882 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-10 15:43:17,882 - distributed.worker - INFO -       Local Directory: /jobfs/149313772.gadi-pbs/dask-scratch-space/worker-a7zzxzun
2025-09-10 15:43:17,882 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,883 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,886 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,886 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,886 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,887 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,893 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,894 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,894 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,895 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,898 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,899 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,899 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,900 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,903 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,904 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,904 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,906 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,907 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,908 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,908 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,909 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,916 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,917 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,917 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,919 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,921 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,922 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,924 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,926 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,926 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,927 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,928 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,932 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,933 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,933 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,934 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,938 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,939 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,939 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,940 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,943 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,944 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,945 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,946 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,948 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,949 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,949 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,951 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,953 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,954 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,954 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,955 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,958 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,959 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,959 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,960 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,963 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,964 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,964 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,966 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,967 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,968 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,968 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,970 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,972 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,973 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,973 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,975 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,976 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,977 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,977 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,979 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,981 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,982 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,982 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,984 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:43:17,986 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-10 15:43:17,987 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.19:8761
2025-09-10 15:43:17,987 - distributed.worker - INFO - -------------------------------------------------
2025-09-10 15:43:17,989 - distributed.core - INFO - Starting established connection to tcp://10.6.82.19:8761
2025-09-10 15:44:05,939 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,939 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,939 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,939 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,940 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,940 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,940 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,940 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,941 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,940 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,941 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,941 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,941 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,941 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,941 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,942 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,942 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,942 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,942 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,942 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,942 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,942 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,943 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,942 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,943 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,943 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,943 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,943 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,943 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,943 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,943 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,943 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,943 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,943 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,943 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,943 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,943 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,943 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,944 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,944 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,944 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,944 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,944 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,943 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,944 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,942 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,944 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,944 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,944 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,944 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,944 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,943 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,944 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,944 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,943 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,945 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,945 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,945 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,945 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,943 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,945 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,945 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,945 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,946 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,946 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,946 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,946 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,946 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,946 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,944 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,944 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,946 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,945 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,947 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,947 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,945 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,947 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,947 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,947 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,947 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,945 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,947 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,948 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,948 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,947 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,947 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,947 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,949 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,948 - distributed.worker - INFO - Starting Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:44:05,950 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,951 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,952 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,952 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,953 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,953 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,953 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,954 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,955 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,955 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,955 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,956 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,957 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,958 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:05,957 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-10 15:44:10,607 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,607 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,607 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,607 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,607 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,607 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,607 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,607 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,607 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,608 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,608 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,608 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,608 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,608 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,609 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,608 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,609 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,609 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,609 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,609 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,609 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,610 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,610 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,610 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,610 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,610 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,610 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,610 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,610 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,611 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,610 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,611 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,611 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,610 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,611 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,611 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,610 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,611 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,611 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,608 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,610 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,611 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,611 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,611 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,611 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,611 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,611 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,610 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,612 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,611 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,612 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,612 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,612 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,611 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,611 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,611 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,612 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,612 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,612 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,612 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,612 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,612 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,612 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,612 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,611 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,612 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,612 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,612 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,613 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,610 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,612 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,613 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,613 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,611 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,613 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,613 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,613 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,614 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,613 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,614 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,614 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,614 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,614 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,615 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,615 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,615 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,615 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,615 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,615 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,615 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,615 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,615 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,613 - distributed.worker - INFO - Starting Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:44:10,616 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,616 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,616 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,616 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,616 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,616 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,616 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,616 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,616 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,617 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:10,618 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-10 15:44:12,140 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,140 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,140 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,140 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,141 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,141 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,141 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,141 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,141 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,141 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,141 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,142 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,142 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,142 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,142 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,142 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,142 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,142 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,142 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,142 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,143 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,143 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,143 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,143 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,143 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,143 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,143 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,143 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,144 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,144 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,144 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,144 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,144 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,144 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,144 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,144 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,145 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,145 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,145 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,145 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,145 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,145 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,145 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,145 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,145 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,146 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,146 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,146 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,146 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,146 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,146 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,146 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,146 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,146 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,146 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,146 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,146 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,146 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,146 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,146 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,146 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,146 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,146 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,146 - distributed.worker - INFO - Starting Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:44:12,147 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,147 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,147 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,147 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,148 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,148 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,148 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,148 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,148 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,148 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,148 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,148 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,149 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,149 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,149 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,149 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,149 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,149 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,149 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,149 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,149 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,149 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,149 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,149 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,150 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,150 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,150 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,150 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,150 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,150 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,150 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,150 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,150 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,150 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,150 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,150 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,150 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,151 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,151 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:12,153 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-10 15:44:13,683 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,683 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,683 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,683 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,683 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,684 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,684 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,684 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,684 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,684 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,684 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,685 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,685 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,685 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,685 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,685 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,685 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,685 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,685 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,685 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,685 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,686 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,686 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,686 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,686 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,686 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,686 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,686 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,686 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,686 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,686 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,686 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,686 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,686 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,686 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,686 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,686 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,686 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,686 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,686 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,686 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,687 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,687 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,687 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,687 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,687 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,687 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,687 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,687 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,687 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,687 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,687 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,687 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,687 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,687 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,688 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,688 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,688 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,688 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,688 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,688 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,688 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,688 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,688 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,688 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,688 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,688 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,688 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,688 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,689 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,689 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,689 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,689 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,689 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,689 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,689 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,689 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,689 - distributed.worker - INFO - Starting Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:44:13,689 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,689 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,689 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,689 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,689 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,690 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,690 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,690 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,690 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,690 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,691 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,691 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,691 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,691 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,691 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,692 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,692 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,692 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,692 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,692 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,692 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,692 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,692 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,692 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,692 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:44:13,700 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-10 15:58:08,314 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:38167. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:43937. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:35823. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:43279. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,318 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:40623. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,316 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:33115. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,316 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,316 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:44357. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,315 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,316 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:44545. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,316 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,316 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,316 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,316 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:33555. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,316 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,316 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:40401. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,316 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,316 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:39005. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,316 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:46217. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,316 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:35745. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,317 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:40887. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,317 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:45277. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,317 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:44561. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,317 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:37659. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,317 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:42311. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,318 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:33269. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,318 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:32991. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,318 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:45315. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,318 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:44971. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,318 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:36045. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,318 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:38293. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,318 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:42165. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,318 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:40857. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,319 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:38871. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,319 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:39355. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,319 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:46739. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,319 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:39907. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,314 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.36:48578 remote=tcp://10.6.82.19:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-10 15:58:08,319 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:40493. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,319 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:39281. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,319 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:44953. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,320 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:34093. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,320 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:42139. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,320 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:32911. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,320 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:36265. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,320 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:44371. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,320 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:40817. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,315 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.36:48490 remote=tcp://10.6.82.19:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-10 15:58:08,315 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.36:48506 remote=tcp://10.6.82.19:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-10 15:58:08,321 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,314 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.36:48600 remote=tcp://10.6.82.19:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-10 15:58:08,315 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.36:48520 remote=tcp://10.6.82.19:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-10 15:58:08,314 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.36:48622 remote=tcp://10.6.82.19:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-10 15:58:08,321 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:43961. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,322 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,317 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.36:48536 remote=tcp://10.6.82.19:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-10 15:58:08,322 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,322 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:35965. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,322 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,314 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.36:48610 remote=tcp://10.6.82.19:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-10 15:58:08,314 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.36:48592 remote=tcp://10.6.82.19:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-10 15:58:08,322 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,314 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.36:48552 remote=tcp://10.6.82.19:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-10 15:58:08,315 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.36:48486 remote=tcp://10.6.82.19:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-10 15:58:08,315 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.36:48532 remote=tcp://10.6.82.19:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-10 15:58:08,322 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:33045. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,322 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:41125. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,322 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,315 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.36:48548 remote=tcp://10.6.82.19:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-10 15:58:08,314 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.36:48568 remote=tcp://10.6.82.19:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-10 15:58:08,323 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:44391. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,323 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,323 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,323 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:36859. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,323 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,324 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,324 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,324 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:37955. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,324 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:33677. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,324 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,324 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:46061. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,324 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:41409. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,324 - distributed.core - INFO - Connection to tcp://10.6.82.19:8761 has been closed.
2025-09-10 15:58:08,324 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:35403. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,324 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:41085. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,325 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.36:40017. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,328 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:39395'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,324 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.36:48626 remote=tcp://10.6.82.19:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-10 15:58:08,331 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,331 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,331 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,331 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,331 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,335 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,336 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:43377'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,337 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:39539'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,337 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,337 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,337 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,337 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,337 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,338 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,338 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,338 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,338 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,338 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,342 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,344 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,351 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:44263'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,351 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:41771'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,352 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:35503'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,352 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,352 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:43147'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,352 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,352 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,352 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:33753'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,352 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,352 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,352 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,352 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,352 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:39625'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,352 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,352 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,353 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,353 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:39741'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,353 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,353 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,353 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,353 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,353 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:45547'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,353 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,353 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,353 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,353 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,353 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:33525'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,353 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,353 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,353 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,353 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,353 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:34603'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,353 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,353 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,353 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,354 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,354 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,354 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:45527'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,354 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,354 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,354 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,354 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,354 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:44515'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,354 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,354 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,354 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,354 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,354 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,354 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,354 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:43373'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,354 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,354 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,355 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,354 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,355 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,354 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,355 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,355 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,355 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,355 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:35999'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,355 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,355 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,355 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,355 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,355 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,355 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,355 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:33155'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,355 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,355 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,355 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,355 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,355 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:43685'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,355 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,355 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,355 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:41355'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,355 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,356 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:41045'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,356 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,355 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,356 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,356 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,356 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:43867'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,356 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,356 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,356 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:43655'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,356 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,356 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,356 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,356 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,356 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,356 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:36237'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,356 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,356 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,356 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,356 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,356 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,357 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:37713'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,357 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,357 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,357 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,357 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:38601'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,357 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,357 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,357 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,357 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,357 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,357 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:40649'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,357 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,357 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,357 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,357 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,357 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,357 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,357 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:34973'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,357 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,357 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,357 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,357 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,357 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,357 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,357 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:34911'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,357 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,358 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,358 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,358 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,358 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,358 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,358 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,358 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,358 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:33113'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,358 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,358 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,358 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,358 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,358 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,358 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,358 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:44321'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,358 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,358 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,358 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,358 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,358 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,358 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,358 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:34013'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,358 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,358 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,358 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,358 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,358 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,358 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,359 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:44149'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,359 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,359 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,359 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,359 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,359 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,359 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:42347'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,359 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,359 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,359 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,359 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,359 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,359 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,359 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:39581'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,359 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,359 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,359 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,359 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,359 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,359 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,359 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,360 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:45305'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,360 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,360 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,360 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,360 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,360 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,360 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:45081'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,360 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,360 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,360 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,360 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,360 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,360 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:43653'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,360 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,361 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,361 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,361 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:39367'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,361 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,361 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,361 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,361 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,361 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,361 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,361 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,361 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:38215'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,361 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,361 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,361 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,361 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,361 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,361 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:38325'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,361 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,361 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,361 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,361 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:36137'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,361 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,361 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,361 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,361 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,362 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:37143'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,361 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,362 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,362 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,362 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,362 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,362 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,362 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:45873'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,362 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,362 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,362 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,362 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,362 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,362 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,362 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:45757'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,362 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,362 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,362 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,362 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,362 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,362 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,362 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,363 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,363 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,363 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,363 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,363 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,363 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,363 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,363 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,363 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,363 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,363 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,363 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,363 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,363 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,363 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,363 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,363 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,363 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,363 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,363 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,364 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,364 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,364 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,364 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,364 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,364 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,364 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,364 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,364 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,364 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,364 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,364 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,364 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,364 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,364 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,364 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,365 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,365 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,365 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,365 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,365 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,366 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,366 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:42131'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,366 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,366 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,367 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:33625'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,367 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:41847'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,367 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,367 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:36295'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,367 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,367 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,367 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:36961'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,367 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,367 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:35989'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,367 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,368 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,368 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.36:42977'. Reason: worker-handle-scheduler-connection-broken
2025-09-10 15:58:08,368 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,368 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,368 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,368 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,368 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,368 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,368 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,368 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,368 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,368 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,368 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,368 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,368 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,368 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-10 15:58:08,368 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,368 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,369 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,369 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,369 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,369 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,369 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,369 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,369 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,369 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,369 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,369 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,369 - distributed.worker - INFO - Removing Worker plugin qme_utils.py96c847bb-585e-42ac-8f79-02bebf6778dd
2025-09-10 15:58:08,369 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,369 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,369 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,369 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,369 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,369 - distributed.worker - INFO - Removing Worker plugin qme_vars.py2f698db5-c8ca-42e5-adc1-a7e0210df4b9
2025-09-10 15:58:08,369 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,369 - distributed.worker - INFO - Removing Worker plugin qme_train.py6cadc8d9-a063-423d-9b99-d59abfca6207
2025-09-10 15:58:08,369 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,370 - distributed.worker - INFO - Removing Worker plugin qme_apply.py854535ea-71d8-4901-8b14-b7584a259002
2025-09-10 15:58:08,370 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,370 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,370 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,370 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,371 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,371 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,371 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,371 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,371 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,372 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,375 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,375 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,376 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,375 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,376 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,376 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:08,376 - distributed.nanny - INFO - Worker closed
2025-09-10 15:58:10,339 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,346 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,349 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,361 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,362 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,363 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,363 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,363 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,363 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,364 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,364 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,365 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,365 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,366 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,366 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,366 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,367 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,367 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,368 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,368 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,369 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,369 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,370 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,371 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,371 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,371 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,372 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,372 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,372 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,373 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,373 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,374 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,374 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,374 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,374 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,375 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,375 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,375 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,376 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,378 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,379 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,379 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,379 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,380 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-10 15:58:10,683 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:37713'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,686 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:39395'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,694 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:37713' closed.
2025-09-10 15:58:10,697 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:39395' closed.
2025-09-10 15:58:10,697 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:35999'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,699 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:35999' closed.
2025-09-10 15:58:10,703 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:40649'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,704 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:39367'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,706 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:33753'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,707 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:40649' closed.
2025-09-10 15:58:10,708 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:43147'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,709 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:39367' closed.
2025-09-10 15:58:10,710 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:35989'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,711 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:33753' closed.
2025-09-10 15:58:10,711 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:45305'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,712 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:43147' closed.
2025-09-10 15:58:10,713 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:43655'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,713 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:35989' closed.
2025-09-10 15:58:10,714 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:45305' closed.
2025-09-10 15:58:10,714 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:43655' closed.
2025-09-10 15:58:10,716 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:39539'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,717 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:42131'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,717 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:39539' closed.
2025-09-10 15:58:10,718 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:42131' closed.
2025-09-10 15:58:10,725 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:43867'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,726 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:43867' closed.
2025-09-10 15:58:10,727 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:37143'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,727 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:37143' closed.
2025-09-10 15:58:10,735 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:34911'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,740 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:34911' closed.
2025-09-10 15:58:10,741 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:36137'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,741 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:38601'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,742 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:36137' closed.
2025-09-10 15:58:10,742 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:38601' closed.
2025-09-10 15:58:10,743 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:35503'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,744 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:35503' closed.
2025-09-10 15:58:10,746 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:36961'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,748 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:36237'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,750 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:36961' closed.
2025-09-10 15:58:10,750 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:38215'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,753 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:43685'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,753 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:42977'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,754 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:36237' closed.
2025-09-10 15:58:10,754 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:41355'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,754 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:42347'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,755 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:38215' closed.
2025-09-10 15:58:10,755 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:43685' closed.
2025-09-10 15:58:10,755 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:42977' closed.
2025-09-10 15:58:10,755 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:41355' closed.
2025-09-10 15:58:10,756 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:42347' closed.
2025-09-10 15:58:10,757 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:33625'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,757 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:33625' closed.
2025-09-10 15:58:10,761 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:36295'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,762 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:33525'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,762 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:36295' closed.
2025-09-10 15:58:10,763 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:33525' closed.
2025-09-10 15:58:10,764 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:43653'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,764 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:43653' closed.
2025-09-10 15:58:10,767 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:39581'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,769 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:34013'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,771 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:41045'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,772 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:44263'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,774 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:44149'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,774 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:45527'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,775 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:39581' closed.
2025-09-10 15:58:10,775 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:44321'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,776 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:34973'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,776 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:34013' closed.
2025-09-10 15:58:10,777 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:41045' closed.
2025-09-10 15:58:10,777 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:44263' closed.
2025-09-10 15:58:10,777 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:41847'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,778 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:44149' closed.
2025-09-10 15:58:10,778 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:45527' closed.
2025-09-10 15:58:10,778 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:44321' closed.
2025-09-10 15:58:10,779 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:34973' closed.
2025-09-10 15:58:10,779 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:41847' closed.
2025-09-10 15:58:10,783 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:43377'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,783 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:43377' closed.
2025-09-10 15:58:10,784 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:45081'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,785 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:45081' closed.
2025-09-10 15:58:10,795 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:44515'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,796 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:44515' closed.
2025-09-10 15:58:10,810 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:33113'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,811 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:33113' closed.
2025-09-10 15:58:10,861 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:34603'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,862 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:34603' closed.
2025-09-10 15:58:10,864 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:45757'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,864 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:45757' closed.
2025-09-10 15:58:10,875 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:33155'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,876 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:39741'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,877 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:33155' closed.
2025-09-10 15:58:10,877 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:39741' closed.
2025-09-10 15:58:10,880 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:45873'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,883 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:39625'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,884 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:43373'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,884 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:41771'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,885 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:45873' closed.
2025-09-10 15:58:10,888 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:39625' closed.
2025-09-10 15:58:10,888 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:43373' closed.
2025-09-10 15:58:10,889 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:41771' closed.
2025-09-10 15:58:10,904 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:45547'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,905 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:45547' closed.
2025-09-10 15:58:10,908 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.36:38325'. Reason: nanny-close-gracefully
2025-09-10 15:58:10,908 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.36:38325' closed.
2025-09-10 15:58:10,911 - distributed.dask_worker - INFO - End worker
