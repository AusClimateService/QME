Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 10:52:52,054 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:34707'
2025-09-03 10:52:52,063 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:40981'
2025-09-03 10:52:52,066 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:44237'
2025-09-03 10:52:52,073 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:36527'
2025-09-03 10:52:52,077 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:33763'
2025-09-03 10:52:52,139 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:33157'
2025-09-03 10:52:52,144 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:44461'
2025-09-03 10:52:52,148 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:35121'
2025-09-03 10:52:52,153 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:33445'
2025-09-03 10:52:52,159 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:37929'
2025-09-03 10:52:52,163 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:36157'
2025-09-03 10:52:52,171 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:36793'
2025-09-03 10:52:52,175 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:36933'
2025-09-03 10:52:52,178 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:40787'
2025-09-03 10:52:52,182 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:44953'
2025-09-03 10:52:52,186 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:38635'
2025-09-03 10:52:52,189 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:36597'
2025-09-03 10:52:52,194 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:45477'
2025-09-03 10:52:52,199 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:36365'
2025-09-03 10:52:52,204 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:41921'
2025-09-03 10:52:52,209 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:38161'
2025-09-03 10:52:52,213 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:41481'
2025-09-03 10:52:52,218 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:38809'
2025-09-03 10:52:52,222 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:35601'
2025-09-03 10:52:52,228 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:46877'
2025-09-03 10:52:52,232 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:36433'
2025-09-03 10:52:52,237 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:43321'
2025-09-03 10:52:52,242 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:36847'
2025-09-03 10:52:52,246 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:38607'
2025-09-03 10:52:52,250 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:35847'
2025-09-03 10:52:52,255 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:36679'
2025-09-03 10:52:52,260 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:37617'
2025-09-03 10:52:52,263 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:33595'
2025-09-03 10:52:52,268 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:39099'
2025-09-03 10:52:52,273 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:36619'
2025-09-03 10:52:52,277 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:39435'
2025-09-03 10:52:52,282 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:42631'
2025-09-03 10:52:52,288 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:41941'
2025-09-03 10:52:52,292 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:35693'
2025-09-03 10:52:52,297 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:43469'
2025-09-03 10:52:52,301 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:35937'
2025-09-03 10:52:52,440 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:43251'
2025-09-03 10:52:52,445 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:33929'
2025-09-03 10:52:52,449 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:34399'
2025-09-03 10:52:52,455 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:44185'
2025-09-03 10:52:52,459 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:36003'
2025-09-03 10:52:52,464 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:32987'
2025-09-03 10:52:52,468 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:33339'
2025-09-03 10:52:52,474 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:34843'
2025-09-03 10:52:52,477 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:45439'
2025-09-03 10:52:52,482 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:32791'
2025-09-03 10:52:52,486 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:35125'
2025-09-03 10:52:52,492 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:39807'
2025-09-03 10:52:52,496 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:33769'
2025-09-03 10:52:52,499 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:34997'
2025-09-03 10:52:52,504 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:40349'
2025-09-03 10:52:52,510 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:37385'
2025-09-03 10:52:52,514 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:39693'
2025-09-03 10:52:52,517 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:41091'
2025-09-03 10:52:52,520 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:36663'
2025-09-03 10:52:52,523 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:38823'
2025-09-03 10:52:52,528 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:42007'
2025-09-03 10:52:52,532 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:34929'
2025-09-03 10:52:52,537 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:34695'
2025-09-03 10:52:52,543 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:42951'
2025-09-03 10:52:52,547 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:32847'
2025-09-03 10:52:52,552 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:45177'
2025-09-03 10:52:52,556 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:44413'
2025-09-03 10:52:52,560 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:36807'
2025-09-03 10:52:52,566 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:37137'
2025-09-03 10:52:52,570 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:45891'
2025-09-03 10:52:52,576 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:44933'
2025-09-03 10:52:52,580 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:43589'
2025-09-03 10:52:52,583 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:37127'
2025-09-03 10:52:52,587 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:43985'
2025-09-03 10:52:52,591 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:44081'
2025-09-03 10:52:52,594 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:37489'
2025-09-03 10:52:52,598 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:42865'
2025-09-03 10:52:52,603 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:43843'
2025-09-03 10:52:52,607 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:37491'
2025-09-03 10:52:52,611 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:40717'
2025-09-03 10:52:52,614 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:43925'
2025-09-03 10:52:52,617 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:38117'
2025-09-03 10:52:52,621 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:42653'
2025-09-03 10:52:52,626 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:43389'
2025-09-03 10:52:52,630 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:37927'
2025-09-03 10:52:52,650 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:34665'
2025-09-03 10:52:52,656 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:32879'
2025-09-03 10:52:52,661 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:33389'
2025-09-03 10:52:52,666 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:45405'
2025-09-03 10:52:52,670 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:46163'
2025-09-03 10:52:52,674 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:39263'
2025-09-03 10:52:52,679 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:43341'
2025-09-03 10:52:52,684 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:45061'
2025-09-03 10:52:52,691 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:40595'
2025-09-03 10:52:52,694 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:33493'
2025-09-03 10:52:52,699 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:34311'
2025-09-03 10:52:52,704 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:45087'
2025-09-03 10:52:52,708 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:35049'
2025-09-03 10:52:52,711 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:35591'
2025-09-03 10:52:52,717 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:39213'
2025-09-03 10:52:52,722 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:34943'
2025-09-03 10:52:52,727 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:33843'
2025-09-03 10:52:52,734 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.16:46111'
2025-09-03 10:52:54,332 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:34735
2025-09-03 10:52:54,332 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:43405
2025-09-03 10:52:54,332 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:35367
2025-09-03 10:52:54,332 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:34301
2025-09-03 10:52:54,333 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:40695
2025-09-03 10:52:54,333 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:45413
2025-09-03 10:52:54,333 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:44733
2025-09-03 10:52:54,333 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:38525
2025-09-03 10:52:54,333 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:38787
2025-09-03 10:52:54,333 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:46593
2025-09-03 10:52:54,333 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:43711
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:34735
2025-09-03 10:52:54,333 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:36795
2025-09-03 10:52:54,333 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:40213
2025-09-03 10:52:54,333 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:39575
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:43405
2025-09-03 10:52:54,333 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:45113
2025-09-03 10:52:54,333 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:33145
2025-09-03 10:52:54,333 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:44489
2025-09-03 10:52:54,333 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:32867
2025-09-03 10:52:54,333 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:46627
2025-09-03 10:52:54,333 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:44617
2025-09-03 10:52:54,333 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:38133
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:35367
2025-09-03 10:52:54,333 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:44847
2025-09-03 10:52:54,333 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:43615
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:34301
2025-09-03 10:52:54,333 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:39251
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:40695
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:45413
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:44733
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:38525
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:39251
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:38787
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:41171
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:46593
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:43711
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:44857
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:36795
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:40213
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:39575
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:40603
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:45113
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:33145
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:44489
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:32867
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:46627
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:44617
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:38133
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:45171
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:44847
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:43615
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:34531
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:45323
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:34737
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:33219
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:35633
2025-09-03 10:52:54,333 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:40865
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:45951
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:40117
2025-09-03 10:52:54,333 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:42831
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:35555
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:42299
2025-09-03 10:52:54,333 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:38265
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:44539
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:36113
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:44485
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:43351
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:38119
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:35395
2025-09-03 10:52:54,333 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:36983
2025-09-03 10:52:54,334 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,333 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.16:34775
2025-09-03 10:52:54,334 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,334 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,334 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,334 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,334 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,334 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,334 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,334 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,334 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,334 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,334 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_zewih0h
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-gnzprnzh
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-7r2zzmm5
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-k_0gllmf
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-8h76nr8q
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-tr386uvd
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-rle71j9_
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-h2dp0kor
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-oj9m3224
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-wswmp34d
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-a7bol1yq
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-t3f3grm2
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-cnxha9hq
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-mvg67dp0
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-gfqvrqzv
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-0wpokb9t
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-vtq9mwbk
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-t3w8zdt_
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_98np1z2
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-cy6fe3jw
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-49qzxv19
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9a4uqixl
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_rzfcnl8
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-zubhgilc
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,335 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,335 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,335 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,335 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,335 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,335 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,335 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,335 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,335 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,335 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,335 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,335 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,335 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,336 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:43909
2025-09-03 10:52:54,336 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:43909
2025-09-03 10:52:54,336 - distributed.worker - INFO -          dashboard at:          10.6.105.16:41585
2025-09-03 10:52:54,336 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,336 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,336 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,336 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,336 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-1nej0x1m
2025-09-03 10:52:54,336 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,339 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:42895
2025-09-03 10:52:54,339 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:42895
2025-09-03 10:52:54,339 - distributed.worker - INFO -          dashboard at:          10.6.105.16:43859
2025-09-03 10:52:54,340 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,340 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,340 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,340 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,340 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-dgxw7_9u
2025-09-03 10:52:54,340 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,363 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:35729
2025-09-03 10:52:54,363 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:35729
2025-09-03 10:52:54,363 - distributed.worker - INFO -          dashboard at:          10.6.105.16:44011
2025-09-03 10:52:54,364 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,364 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,364 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,364 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,364 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-812jafpl
2025-09-03 10:52:54,364 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,394 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:37751
2025-09-03 10:52:54,394 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:37751
2025-09-03 10:52:54,394 - distributed.worker - INFO -          dashboard at:          10.6.105.16:42715
2025-09-03 10:52:54,394 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,394 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,394 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,394 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,394 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-y2wii_q_
2025-09-03 10:52:54,394 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,395 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:45131
2025-09-03 10:52:54,396 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:45131
2025-09-03 10:52:54,396 - distributed.worker - INFO -          dashboard at:          10.6.105.16:35453
2025-09-03 10:52:54,396 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,396 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,396 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,396 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,396 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-qnuwwdgm
2025-09-03 10:52:54,396 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,420 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:44753
2025-09-03 10:52:54,420 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:44753
2025-09-03 10:52:54,420 - distributed.worker - INFO -          dashboard at:          10.6.105.16:42537
2025-09-03 10:52:54,420 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,420 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,420 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,420 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,420 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9p07w6ze
2025-09-03 10:52:54,421 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,567 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:36357
2025-09-03 10:52:54,567 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:36357
2025-09-03 10:52:54,567 - distributed.worker - INFO -          dashboard at:          10.6.105.16:36865
2025-09-03 10:52:54,567 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,567 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,567 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,567 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,567 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-gylgkh7n
2025-09-03 10:52:54,567 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,574 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:46771
2025-09-03 10:52:54,574 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:46771
2025-09-03 10:52:54,574 - distributed.worker - INFO -          dashboard at:          10.6.105.16:42327
2025-09-03 10:52:54,574 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,574 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,574 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,574 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,574 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-eq45v80n
2025-09-03 10:52:54,574 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,579 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:42521
2025-09-03 10:52:54,579 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:42521
2025-09-03 10:52:54,579 - distributed.worker - INFO -          dashboard at:          10.6.105.16:37425
2025-09-03 10:52:54,579 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,579 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,579 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,579 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,579 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-i42ornlm
2025-09-03 10:52:54,579 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,579 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:36779
2025-09-03 10:52:54,579 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:36779
2025-09-03 10:52:54,579 - distributed.worker - INFO -          dashboard at:          10.6.105.16:40775
2025-09-03 10:52:54,579 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,579 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,579 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,579 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,579 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-u3_msf40
2025-09-03 10:52:54,579 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,582 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:39411
2025-09-03 10:52:54,582 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:39411
2025-09-03 10:52:54,582 - distributed.worker - INFO -          dashboard at:          10.6.105.16:44335
2025-09-03 10:52:54,582 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,582 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,582 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,582 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,582 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-n3et6uyu
2025-09-03 10:52:54,582 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,586 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:37889
2025-09-03 10:52:54,586 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:37889
2025-09-03 10:52:54,586 - distributed.worker - INFO -          dashboard at:          10.6.105.16:45879
2025-09-03 10:52:54,586 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,586 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,586 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,586 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,586 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-hk79x8wh
2025-09-03 10:52:54,586 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,673 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:38159
2025-09-03 10:52:54,673 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:38159
2025-09-03 10:52:54,673 - distributed.worker - INFO -          dashboard at:          10.6.105.16:36947
2025-09-03 10:52:54,673 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,673 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,673 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,673 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ek4tu9i_
2025-09-03 10:52:54,674 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,690 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:45939
2025-09-03 10:52:54,691 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:45939
2025-09-03 10:52:54,691 - distributed.worker - INFO -          dashboard at:          10.6.105.16:35561
2025-09-03 10:52:54,691 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,691 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,691 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,691 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,691 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ooo94q8_
2025-09-03 10:52:54,691 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,721 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:42907
2025-09-03 10:52:54,721 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:42907
2025-09-03 10:52:54,721 - distributed.worker - INFO -          dashboard at:          10.6.105.16:35559
2025-09-03 10:52:54,721 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,721 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,721 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,721 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,721 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ex6rdhet
2025-09-03 10:52:54,721 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,762 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:41339
2025-09-03 10:52:54,762 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:41339
2025-09-03 10:52:54,762 - distributed.worker - INFO -          dashboard at:          10.6.105.16:39731
2025-09-03 10:52:54,762 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,762 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,763 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,763 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,763 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-2sv0q8wo
2025-09-03 10:52:54,763 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,775 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:36525
2025-09-03 10:52:54,775 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:36525
2025-09-03 10:52:54,775 - distributed.worker - INFO -          dashboard at:          10.6.105.16:34373
2025-09-03 10:52:54,775 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,775 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,775 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,775 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,775 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-gezp4mlt
2025-09-03 10:52:54,775 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,782 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:41971
2025-09-03 10:52:54,782 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:41971
2025-09-03 10:52:54,782 - distributed.worker - INFO -          dashboard at:          10.6.105.16:41379
2025-09-03 10:52:54,782 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,782 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,782 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,782 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,782 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-dfgv1f5b
2025-09-03 10:52:54,782 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,968 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:43447
2025-09-03 10:52:54,969 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:43447
2025-09-03 10:52:54,969 - distributed.worker - INFO -          dashboard at:          10.6.105.16:40069
2025-09-03 10:52:54,969 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,969 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,969 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,969 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,969 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-0_xfy_xl
2025-09-03 10:52:54,969 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,992 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:38229
2025-09-03 10:52:54,992 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:38229
2025-09-03 10:52:54,992 - distributed.worker - INFO -          dashboard at:          10.6.105.16:40283
2025-09-03 10:52:54,992 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,992 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,992 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,992 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,992 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-gbdus3hq
2025-09-03 10:52:54,992 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,002 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:46181
2025-09-03 10:52:55,002 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:46181
2025-09-03 10:52:55,002 - distributed.worker - INFO -          dashboard at:          10.6.105.16:33459
2025-09-03 10:52:55,002 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,002 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,002 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,002 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,002 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-nn9r11zm
2025-09-03 10:52:55,002 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,006 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:42861
2025-09-03 10:52:55,006 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:42861
2025-09-03 10:52:55,006 - distributed.worker - INFO -          dashboard at:          10.6.105.16:39485
2025-09-03 10:52:55,006 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,006 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,006 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,006 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,006 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-bu4wmxa5
2025-09-03 10:52:55,006 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,012 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:37713
2025-09-03 10:52:55,013 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:37713
2025-09-03 10:52:55,013 - distributed.worker - INFO -          dashboard at:          10.6.105.16:43381
2025-09-03 10:52:55,013 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,013 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,013 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,013 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,013 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9tx29_y6
2025-09-03 10:52:55,013 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,015 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:35513
2025-09-03 10:52:55,015 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:35513
2025-09-03 10:52:55,015 - distributed.worker - INFO -          dashboard at:          10.6.105.16:42459
2025-09-03 10:52:55,015 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,015 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,015 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,016 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,016 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-vwnq6_y8
2025-09-03 10:52:55,016 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,022 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:34031
2025-09-03 10:52:55,022 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:34031
2025-09-03 10:52:55,022 - distributed.worker - INFO -          dashboard at:          10.6.105.16:41675
2025-09-03 10:52:55,022 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,022 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,022 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,022 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,022 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-bas1un_j
2025-09-03 10:52:55,022 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,061 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:42191
2025-09-03 10:52:55,061 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:42191
2025-09-03 10:52:55,061 - distributed.worker - INFO -          dashboard at:          10.6.105.16:35115
2025-09-03 10:52:55,061 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,061 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,061 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,062 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,062 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-8tt942ac
2025-09-03 10:52:55,062 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,184 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:38743
2025-09-03 10:52:55,184 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:38743
2025-09-03 10:52:55,184 - distributed.worker - INFO -          dashboard at:          10.6.105.16:43865
2025-09-03 10:52:55,184 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,184 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,184 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,184 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,184 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-yelq9p33
2025-09-03 10:52:55,184 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,193 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:39717
2025-09-03 10:52:55,193 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:39717
2025-09-03 10:52:55,193 - distributed.worker - INFO -          dashboard at:          10.6.105.16:40655
2025-09-03 10:52:55,193 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,193 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,193 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,193 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,193 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-2pbr2b2v
2025-09-03 10:52:55,193 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,212 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:38951
2025-09-03 10:52:55,212 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:38951
2025-09-03 10:52:55,212 - distributed.worker - INFO -          dashboard at:          10.6.105.16:36413
2025-09-03 10:52:55,212 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,212 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,212 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,212 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,212 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-cgatnsie
2025-09-03 10:52:55,212 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,279 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:45515
2025-09-03 10:52:55,280 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:45515
2025-09-03 10:52:55,280 - distributed.worker - INFO -          dashboard at:          10.6.105.16:36391
2025-09-03 10:52:55,280 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,280 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,280 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,280 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,280 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-21wmfapt
2025-09-03 10:52:55,280 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,308 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:36989
2025-09-03 10:52:55,308 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:36989
2025-09-03 10:52:55,308 - distributed.worker - INFO -          dashboard at:          10.6.105.16:38499
2025-09-03 10:52:55,308 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,308 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,308 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,308 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-23pcdrt2
2025-09-03 10:52:55,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,344 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:39715
2025-09-03 10:52:55,344 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:39715
2025-09-03 10:52:55,344 - distributed.worker - INFO -          dashboard at:          10.6.105.16:41663
2025-09-03 10:52:55,344 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,344 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,344 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,344 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,344 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-k89hgdib
2025-09-03 10:52:55,344 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,347 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:37365
2025-09-03 10:52:55,347 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:37365
2025-09-03 10:52:55,347 - distributed.worker - INFO -          dashboard at:          10.6.105.16:42717
2025-09-03 10:52:55,347 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,347 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,347 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,347 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,347 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-srmpq_lz
2025-09-03 10:52:55,347 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,374 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:42335
2025-09-03 10:52:55,375 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:42335
2025-09-03 10:52:55,375 - distributed.worker - INFO -          dashboard at:          10.6.105.16:37659
2025-09-03 10:52:55,375 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,375 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,375 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,375 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,375 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ebvduni4
2025-09-03 10:52:55,375 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,421 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:45771
2025-09-03 10:52:55,421 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:45771
2025-09-03 10:52:55,421 - distributed.worker - INFO -          dashboard at:          10.6.105.16:42819
2025-09-03 10:52:55,421 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,421 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,422 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,422 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,422 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-09r9nsgw
2025-09-03 10:52:55,422 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,456 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:42397
2025-09-03 10:52:55,457 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:42397
2025-09-03 10:52:55,457 - distributed.worker - INFO -          dashboard at:          10.6.105.16:40097
2025-09-03 10:52:55,457 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,457 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,457 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,457 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,457 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-x0s6dgrs
2025-09-03 10:52:55,457 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,487 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:33079
2025-09-03 10:52:55,488 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:33079
2025-09-03 10:52:55,488 - distributed.worker - INFO -          dashboard at:          10.6.105.16:41161
2025-09-03 10:52:55,488 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,488 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,488 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,488 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,488 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-plnyf0u7
2025-09-03 10:52:55,488 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,508 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:45993
2025-09-03 10:52:55,508 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:45993
2025-09-03 10:52:55,508 - distributed.worker - INFO -          dashboard at:          10.6.105.16:33669
2025-09-03 10:52:55,508 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,508 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,508 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,508 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,508 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-xub62zm9
2025-09-03 10:52:55,508 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,515 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:43681
2025-09-03 10:52:55,515 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:43681
2025-09-03 10:52:55,515 - distributed.worker - INFO -          dashboard at:          10.6.105.16:42105
2025-09-03 10:52:55,515 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,515 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,515 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,515 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,515 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-br5epkk1
2025-09-03 10:52:55,515 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,566 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:39949
2025-09-03 10:52:55,566 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:39949
2025-09-03 10:52:55,567 - distributed.worker - INFO -          dashboard at:          10.6.105.16:46637
2025-09-03 10:52:55,567 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,567 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,567 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,567 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,567 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-jlbawka9
2025-09-03 10:52:55,567 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,582 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:45859
2025-09-03 10:52:55,582 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:45859
2025-09-03 10:52:55,582 - distributed.worker - INFO -          dashboard at:          10.6.105.16:33015
2025-09-03 10:52:55,582 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,582 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,582 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,582 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,583 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-64coumea
2025-09-03 10:52:55,583 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,624 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:39851
2025-09-03 10:52:55,625 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:39851
2025-09-03 10:52:55,625 - distributed.worker - INFO -          dashboard at:          10.6.105.16:44787
2025-09-03 10:52:55,625 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,625 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,625 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,625 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,625 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-t7zjdopd
2025-09-03 10:52:55,625 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,629 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:41913
2025-09-03 10:52:55,629 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:41913
2025-09-03 10:52:55,629 - distributed.worker - INFO -          dashboard at:          10.6.105.16:39377
2025-09-03 10:52:55,629 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,629 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,629 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,629 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,629 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-1xnt0rdj
2025-09-03 10:52:55,629 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,648 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:38029
2025-09-03 10:52:55,649 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:38029
2025-09-03 10:52:55,649 - distributed.worker - INFO -          dashboard at:          10.6.105.16:37457
2025-09-03 10:52:55,649 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,649 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,649 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,649 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,649 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ozpfqzit
2025-09-03 10:52:55,649 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,742 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:45615
2025-09-03 10:52:55,743 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:45615
2025-09-03 10:52:55,743 - distributed.worker - INFO -          dashboard at:          10.6.105.16:38323
2025-09-03 10:52:55,743 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,743 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,743 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,743 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,743 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-cb_wy30o
2025-09-03 10:52:55,743 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,788 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:46327
2025-09-03 10:52:55,788 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:46327
2025-09-03 10:52:55,788 - distributed.worker - INFO -          dashboard at:          10.6.105.16:43961
2025-09-03 10:52:55,788 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,788 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,788 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,788 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,789 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-0b_b1h_s
2025-09-03 10:52:55,789 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,847 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:38045
2025-09-03 10:52:55,848 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:38045
2025-09-03 10:52:55,848 - distributed.worker - INFO -          dashboard at:          10.6.105.16:43179
2025-09-03 10:52:55,848 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,848 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,848 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,848 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,848 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-eu975deg
2025-09-03 10:52:55,848 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,945 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:35881
2025-09-03 10:52:55,945 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:35881
2025-09-03 10:52:55,945 - distributed.worker - INFO -          dashboard at:          10.6.105.16:40565
2025-09-03 10:52:55,945 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,945 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,945 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,945 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,945 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_0_2fptv
2025-09-03 10:52:55,945 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,950 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:45929
2025-09-03 10:52:55,950 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:45929
2025-09-03 10:52:55,950 - distributed.worker - INFO -          dashboard at:          10.6.105.16:42417
2025-09-03 10:52:55,950 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,950 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,950 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,950 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,950 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-mxfl6q9p
2025-09-03 10:52:55,950 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,955 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:39887
2025-09-03 10:52:55,955 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:39887
2025-09-03 10:52:55,955 - distributed.worker - INFO -          dashboard at:          10.6.105.16:42421
2025-09-03 10:52:55,955 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,955 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,955 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,955 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,955 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-fup29cxd
2025-09-03 10:52:55,955 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,965 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:39437
2025-09-03 10:52:55,965 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:39437
2025-09-03 10:52:55,965 - distributed.worker - INFO -          dashboard at:          10.6.105.16:37587
2025-09-03 10:52:55,965 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,965 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,965 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,965 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,965 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-t2wwc3at
2025-09-03 10:52:55,965 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,969 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:43019
2025-09-03 10:52:55,969 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:43019
2025-09-03 10:52:55,969 - distributed.worker - INFO -          dashboard at:          10.6.105.16:38367
2025-09-03 10:52:55,969 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,969 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,969 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,969 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,969 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-eukgvl5m
2025-09-03 10:52:55,969 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,979 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:36123
2025-09-03 10:52:55,979 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:36123
2025-09-03 10:52:55,979 - distributed.worker - INFO -          dashboard at:          10.6.105.16:43141
2025-09-03 10:52:55,979 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,979 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,979 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,979 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,979 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ixdhi61m
2025-09-03 10:52:55,979 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,995 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:44253
2025-09-03 10:52:55,995 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:44253
2025-09-03 10:52:55,995 - distributed.worker - INFO -          dashboard at:          10.6.105.16:41341
2025-09-03 10:52:55,995 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,995 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,995 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,995 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,995 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-koin018a
2025-09-03 10:52:55,995 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,032 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:46171
2025-09-03 10:52:56,032 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:46171
2025-09-03 10:52:56,032 - distributed.worker - INFO -          dashboard at:          10.6.105.16:46461
2025-09-03 10:52:56,032 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,032 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,032 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,032 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,032 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-jzdu97ca
2025-09-03 10:52:56,032 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,039 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:36427
2025-09-03 10:52:56,039 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:36427
2025-09-03 10:52:56,039 - distributed.worker - INFO -          dashboard at:          10.6.105.16:45045
2025-09-03 10:52:56,039 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,039 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,039 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,040 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,040 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-23qv9g_i
2025-09-03 10:52:56,040 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,063 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:42699
2025-09-03 10:52:56,063 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:42699
2025-09-03 10:52:56,063 - distributed.worker - INFO -          dashboard at:          10.6.105.16:43119
2025-09-03 10:52:56,063 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,063 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,063 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,063 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,063 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-glh_sjg6
2025-09-03 10:52:56,063 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,077 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:39073
2025-09-03 10:52:56,078 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:39073
2025-09-03 10:52:56,078 - distributed.worker - INFO -          dashboard at:          10.6.105.16:42695
2025-09-03 10:52:56,078 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,078 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,078 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,078 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,078 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-lhbennt8
2025-09-03 10:52:56,078 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,090 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:44563
2025-09-03 10:52:56,090 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:44563
2025-09-03 10:52:56,090 - distributed.worker - INFO -          dashboard at:          10.6.105.16:35225
2025-09-03 10:52:56,090 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,090 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,090 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,090 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,090 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-28sim4wz
2025-09-03 10:52:56,090 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,151 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:43853
2025-09-03 10:52:56,151 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:43853
2025-09-03 10:52:56,151 - distributed.worker - INFO -          dashboard at:          10.6.105.16:45251
2025-09-03 10:52:56,151 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,151 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,151 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,151 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,151 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ephdkzai
2025-09-03 10:52:56,151 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,158 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:36887
2025-09-03 10:52:56,158 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:36887
2025-09-03 10:52:56,158 - distributed.worker - INFO -          dashboard at:          10.6.105.16:45083
2025-09-03 10:52:56,158 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,158 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,158 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,158 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,158 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-v15wom8y
2025-09-03 10:52:56,158 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,164 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:44921
2025-09-03 10:52:56,164 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:44921
2025-09-03 10:52:56,164 - distributed.worker - INFO -          dashboard at:          10.6.105.16:33435
2025-09-03 10:52:56,164 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,164 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,164 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,164 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,164 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-f2wv5wa2
2025-09-03 10:52:56,164 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,164 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:38369
2025-09-03 10:52:56,164 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:38369
2025-09-03 10:52:56,164 - distributed.worker - INFO -          dashboard at:          10.6.105.16:39909
2025-09-03 10:52:56,164 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:45517
2025-09-03 10:52:56,164 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,164 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,164 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:45517
2025-09-03 10:52:56,164 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,164 - distributed.worker - INFO -          dashboard at:          10.6.105.16:37535
2025-09-03 10:52:56,165 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,165 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,165 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-rblxcjvl
2025-09-03 10:52:56,165 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,165 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,165 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,165 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,165 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-11a5iv9o
2025-09-03 10:52:56,165 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,165 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:41763
2025-09-03 10:52:56,165 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:41763
2025-09-03 10:52:56,165 - distributed.worker - INFO -          dashboard at:          10.6.105.16:39269
2025-09-03 10:52:56,165 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,165 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,165 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,165 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,165 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-xhbb4cjc
2025-09-03 10:52:56,165 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,170 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:33311
2025-09-03 10:52:56,170 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:33311
2025-09-03 10:52:56,170 - distributed.worker - INFO -          dashboard at:          10.6.105.16:33049
2025-09-03 10:52:56,170 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,170 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,170 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,170 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,170 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_gt_icl1
2025-09-03 10:52:56,170 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,171 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:45449
2025-09-03 10:52:56,172 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:45449
2025-09-03 10:52:56,172 - distributed.worker - INFO -          dashboard at:          10.6.105.16:45261
2025-09-03 10:52:56,172 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,172 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,172 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,172 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,172 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-g8s49nwv
2025-09-03 10:52:56,172 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,173 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:44587
2025-09-03 10:52:56,173 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:44587
2025-09-03 10:52:56,173 - distributed.worker - INFO -          dashboard at:          10.6.105.16:33347
2025-09-03 10:52:56,173 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,173 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,173 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,173 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-94bwkcdd
2025-09-03 10:52:56,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,173 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:45335
2025-09-03 10:52:56,173 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:45335
2025-09-03 10:52:56,173 - distributed.worker - INFO -          dashboard at:          10.6.105.16:46821
2025-09-03 10:52:56,173 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,173 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,174 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,174 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-5lozfs73
2025-09-03 10:52:56,174 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,175 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:46051
2025-09-03 10:52:56,175 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:46051
2025-09-03 10:52:56,175 - distributed.worker - INFO -          dashboard at:          10.6.105.16:34799
2025-09-03 10:52:56,175 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,176 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,176 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,176 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,176 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-5nhsz1ec
2025-09-03 10:52:56,176 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,179 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:40671
2025-09-03 10:52:56,179 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:40671
2025-09-03 10:52:56,179 - distributed.worker - INFO -          dashboard at:          10.6.105.16:41167
2025-09-03 10:52:56,179 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,179 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,179 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,179 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,179 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-6dz2u_j8
2025-09-03 10:52:56,179 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,182 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:44419
2025-09-03 10:52:56,182 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:44419
2025-09-03 10:52:56,182 - distributed.worker - INFO -          dashboard at:          10.6.105.16:35785
2025-09-03 10:52:56,182 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,182 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,182 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,182 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,182 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-g_urdvg8
2025-09-03 10:52:56,182 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,184 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:46177
2025-09-03 10:52:56,184 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:34161
2025-09-03 10:52:56,185 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:46177
2025-09-03 10:52:56,185 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:34161
2025-09-03 10:52:56,185 - distributed.worker - INFO -          dashboard at:          10.6.105.16:41163
2025-09-03 10:52:56,185 - distributed.worker - INFO -          dashboard at:          10.6.105.16:37251
2025-09-03 10:52:56,185 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,185 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,185 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,185 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,185 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,185 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,185 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,185 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,185 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ip9oq2f2
2025-09-03 10:52:56,185 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_tc8mwzg
2025-09-03 10:52:56,185 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,185 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,196 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:33689
2025-09-03 10:52:56,196 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:33689
2025-09-03 10:52:56,196 - distributed.worker - INFO -          dashboard at:          10.6.105.16:34669
2025-09-03 10:52:56,196 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,196 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,196 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,196 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,196 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-k_2jzxew
2025-09-03 10:52:56,196 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,198 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:41289
2025-09-03 10:52:56,198 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:41289
2025-09-03 10:52:56,198 - distributed.worker - INFO -          dashboard at:          10.6.105.16:42225
2025-09-03 10:52:56,198 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,198 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,198 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,198 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ppv51anc
2025-09-03 10:52:56,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,203 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:43515
2025-09-03 10:52:56,203 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:43515
2025-09-03 10:52:56,203 - distributed.worker - INFO -          dashboard at:          10.6.105.16:35419
2025-09-03 10:52:56,203 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,203 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,203 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,203 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,204 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-n9lm74xc
2025-09-03 10:52:56,204 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,204 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:35343
2025-09-03 10:52:56,204 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:35343
2025-09-03 10:52:56,204 - distributed.worker - INFO -          dashboard at:          10.6.105.16:40585
2025-09-03 10:52:56,204 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,204 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,204 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,204 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,204 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-0bsip_o5
2025-09-03 10:52:56,204 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,207 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:35563
2025-09-03 10:52:56,207 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:35563
2025-09-03 10:52:56,207 - distributed.worker - INFO -          dashboard at:          10.6.105.16:41547
2025-09-03 10:52:56,207 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,207 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,207 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,207 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,207 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-nqiymu4c
2025-09-03 10:52:56,207 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,211 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.16:34575
2025-09-03 10:52:56,211 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.16:34575
2025-09-03 10:52:56,211 - distributed.worker - INFO -          dashboard at:          10.6.105.16:35317
2025-09-03 10:52:56,211 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,211 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,211 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,211 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,211 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-8ey6jwkh
2025-09-03 10:52:56,211 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,537 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,539 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,539 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,541 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,554 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,555 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,556 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,557 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,571 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,572 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,572 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,574 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,587 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,589 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,589 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,591 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,608 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,612 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,612 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,617 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,624 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,627 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,627 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,629 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,636 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,637 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,637 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,639 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,652 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,654 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,654 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,656 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,669 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,670 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,670 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,672 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,689 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,693 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,693 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,699 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,701 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,702 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,702 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,704 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,718 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,719 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,719 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,721 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,734 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,736 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,736 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,738 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,751 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,752 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,753 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,755 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,768 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,769 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,769 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,771 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,783 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,785 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,785 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,787 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,800 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,801 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,802 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,804 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,816 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,818 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,818 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,820 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,832 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,834 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,834 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,836 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,849 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,851 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,851 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,853 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,865 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,866 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,867 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,868 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,882 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,883 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,884 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,886 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,898 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,899 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,899 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,901 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,914 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,916 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,916 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,918 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,947 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,949 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,949 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,951 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,981 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,984 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,984 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,989 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,207 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,208 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,208 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,210 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,388 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,389 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,390 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,391 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,421 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,422 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,423 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,425 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,707 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,708 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,708 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,710 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,799 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,800 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,800 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,802 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,017 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,018 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,018 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,020 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,033 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,034 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,034 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,036 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,049 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,051 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,051 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,053 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,066 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,068 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,068 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,070 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,082 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,083 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,084 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,085 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,099 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,100 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,100 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,103 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,199 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,200 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,200 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,202 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,215 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,217 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,217 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,219 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,351 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,353 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,353 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,355 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,484 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,485 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,485 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,487 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,502 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,503 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,504 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,505 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,518 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,520 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,520 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,522 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,806 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,808 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,808 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,810 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,824 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,825 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,825 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,827 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,840 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,842 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,842 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,844 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,857 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,858 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,859 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,861 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,874 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,875 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,875 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,877 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,890 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,892 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,892 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,894 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,907 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,909 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,909 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,911 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,924 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,925 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,926 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,928 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,941 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,942 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,942 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,944 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,958 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,959 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,959 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,961 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,975 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,976 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,976 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,978 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,991 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,993 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,993 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,995 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:59,008 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:59,009 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:59,009 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:59,011 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:59,025 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:59,026 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:59,027 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:59,029 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:59,042 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:59,043 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:59,043 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:59,045 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:59,058 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:59,060 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:59,060 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:59,062 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:59,075 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:59,077 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:59,077 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:59,079 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:59,092 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:59,093 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:59,093 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:59,095 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:59,108 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:59,110 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:59,110 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:59,112 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:59,125 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:59,127 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:59,127 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:59,129 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:59,142 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:59,144 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:59,144 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:59,146 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:59,247 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:59,249 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:59,249 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:59,251 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:59,266 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:59,267 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:59,267 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:59,269 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:59,318 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:59,319 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:59,319 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:59,321 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:59,334 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:59,335 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:59,336 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:59,337 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:59,351 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:59,352 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:59,352 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:59,354 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:00,325 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:00,326 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:00,326 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:00,329 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:00,342 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:00,343 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:00,343 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:00,345 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:00,359 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:00,360 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:00,360 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:00,363 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,228 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,230 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,230 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,232 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,713 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,715 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,715 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,717 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,730 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,731 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,731 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,733 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:04,822 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:04,823 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:04,824 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:04,825 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:06,885 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:06,887 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:06,887 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:06,889 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:06,904 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:06,905 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:06,906 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:06,907 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:06,920 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:06,922 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:06,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:06,924 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:07,155 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:07,156 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:07,157 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:07,158 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:07,172 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:07,174 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:07,174 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:07,176 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:07,189 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:07,191 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:07,191 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:07,193 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:07,206 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:07,208 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:07,208 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:07,210 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:07,223 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:07,225 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:07,225 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:07,227 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:10,829 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:10,830 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:10,831 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:10,832 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:10,853 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:10,858 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:10,859 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:10,864 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:10,866 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:10,867 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:10,867 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:10,869 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:10,883 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:10,884 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:10,884 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:10,886 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:24,783 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:24,970 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:24,993 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,004 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,007 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,014 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,017 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,023 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,423 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,458 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,744 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,790 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,981 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,997 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:26,033 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:26,041 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:27,546 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:27,559 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:27,742 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:27,757 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:28,247 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:28,249 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:28,249 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:28,251 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:28,265 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:28,267 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:28,267 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:28,269 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:28,284 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:28,285 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:28,285 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:28,287 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:28,302 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:28,303 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:28,304 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:28,305 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:28,320 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:28,322 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:28,322 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:28,323 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:28,338 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:28,340 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:28,340 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:28,342 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:28,356 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:28,358 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:28,358 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:28,360 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:28,804 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:29,023 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:29,037 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:29,829 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:29,847 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:29,862 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:29,880 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:29,897 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:29,964 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:30,322 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:30,341 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:30,356 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:34,074 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:34,075 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:34,075 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:34,077 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:34,093 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:34,096 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:34,096 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:34,101 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:34,237 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:37,892 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:37,911 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:37,928 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:38,162 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:38,179 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:38,194 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:38,214 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:46,601 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:46,602 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:46,602 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:46,604 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,599 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,600 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,600 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,602 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,622 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,624 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,624 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,625 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,642 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,643 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,644 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,645 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,661 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,663 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,664 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,680 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,682 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,682 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,684 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,706 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,716 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,716 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,722 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:56:38,093 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,095 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,254 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,261 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,694 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,695 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,703 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,704 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,751 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,752 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,766 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,768 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,802 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,807 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,057 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,062 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,097 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,106 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,306 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,312 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,335 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,336 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,713 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,718 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,806 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,810 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,957 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,963 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,240 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,243 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,303 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,308 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,072 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,078 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,391 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,397 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,510 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,515 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,690 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,695 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,861 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,867 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,145 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,147 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,224 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,229 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,280 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,286 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,316 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,321 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,528 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,529 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,533 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,534 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,561 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,563 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,752 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,754 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,764 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,769 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,778 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,782 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,945 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,946 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,975 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,976 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,136 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,142 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,512 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,513 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,770 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,771 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,820 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,821 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,824 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,826 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,109 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,110 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,241 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,242 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,546 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,552 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,568 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,570 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,587 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,594 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,708 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,710 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,751 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,755 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,805 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,811 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,995 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,001 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,005 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,010 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,020 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,023 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,252 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,253 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,359 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,364 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,404 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,405 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,428 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,431 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,489 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,493 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,611 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,616 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,392 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,394 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,822 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,827 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,205 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,211 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,525 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,526 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,606 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,607 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,612 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,617 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,690 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,695 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,243 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,246 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,636 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,640 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,087 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,092 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,191 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,192 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,246 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,251 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,601 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,603 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,935 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,940 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,959 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,964 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,150 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,155 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,507 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,515 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,536 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,542 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,685 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,690 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,896 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,897 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,945 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,946 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,961 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,966 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,992 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,997 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,155 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,158 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,795 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,797 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,906 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,912 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,993 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,994 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,038 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,039 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,281 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,283 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,589 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,593 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,118 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,120 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,702 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,703 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,807 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,816 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,250 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,255 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,376 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,377 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,446 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,451 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:56,635 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:56,641 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:56,656 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:56,657 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:57,701 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:57,707 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:58,897 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:58,903 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:59,633 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:59,638 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:00,080 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:00,082 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:02,119 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:02,120 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:02,991 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:02,993 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:03,251 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:03,253 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:03,607 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:03,612 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:03,617 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:03,621 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:04,002 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:04,003 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:05,268 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:05,275 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:09,996 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:09,998 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:09,998 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,001 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,015 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,017 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,017 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,019 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,021 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,022 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,330 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,330 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,332 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,333 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,337 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,340 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,353 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,355 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,385 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,386 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,387 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,387 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,388 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,388 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,389 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,390 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,492 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,494 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,507 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,509 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,509 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,511 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,531 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,535 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,546 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,551 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,624 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,626 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,644 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,646 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,658 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,660 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,731 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,733 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,739 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,741 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,748 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,750 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,751 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,753 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,756 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,758 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,802 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,804 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,874 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,877 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,897 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,905 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,912 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,914 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,920 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,924 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,120 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,122 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,127 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,130 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,169 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,171 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,260 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,261 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,264 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,282 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,284 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,284 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,290 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,299 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,301 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,312 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,314 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,372 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,374 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,392 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,395 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,397 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,399 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,399 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,405 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,407 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,418 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,421 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,484 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,486 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,485 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,487 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,490 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,492 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,504 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,504 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,506 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,506 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,517 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,519 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,537 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,539 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,549 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,551 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,573 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,575 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,574 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,580 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,595 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,597 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,603 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,605 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,607 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,609 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,616 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,630 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,634 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,637 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,647 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,652 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,681 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,684 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,694 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,696 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,759 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,761 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,767 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,769 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,856 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,858 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,920 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,922 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,954 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,959 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,978 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,982 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,999 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,002 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,015 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,018 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,054 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,057 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,090 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,092 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,161 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,163 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,167 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,169 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,191 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,193 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,301 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,303 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,331 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,333 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,418 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,420 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,433 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,435 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,461 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,463 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,512 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,513 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,516 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,518 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,566 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,568 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,581 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,583 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,596 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,603 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,683 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,685 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,700 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,703 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,865 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,867 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,907 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,910 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,925 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,927 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,955 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,958 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,979 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,981 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,095 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,096 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,111 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,112 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,141 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,142 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,174 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,175 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,183 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,185 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,247 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,249 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,309 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,311 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,503 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,504 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,798 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,800 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:15,510 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:15,513 - distributed.utils - INFO - Reload module qme_vars from .py file
