Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-10-15 01:18:58,297 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:41329'
2025-10-15 01:18:58,306 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:36901'
2025-10-15 01:18:58,313 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:40033'
2025-10-15 01:18:58,318 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:43341'
2025-10-15 01:18:58,322 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:36749'
2025-10-15 01:18:58,326 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:34047'
2025-10-15 01:18:58,331 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:35865'
2025-10-15 01:18:58,335 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:39443'
2025-10-15 01:18:58,343 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:37417'
2025-10-15 01:18:58,347 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:41705'
2025-10-15 01:18:58,352 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:40617'
2025-10-15 01:18:58,356 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:46871'
2025-10-15 01:18:58,360 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:34103'
2025-10-15 01:18:58,365 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:39349'
2025-10-15 01:18:58,369 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:45743'
2025-10-15 01:18:58,374 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:35209'
2025-10-15 01:18:58,379 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:42469'
2025-10-15 01:18:58,383 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:39581'
2025-10-15 01:18:58,387 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:39861'
2025-10-15 01:18:58,391 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:33767'
2025-10-15 01:18:58,396 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:40329'
2025-10-15 01:18:58,515 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:37747'
2025-10-15 01:18:58,520 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:39953'
2025-10-15 01:18:58,524 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:38225'
2025-10-15 01:18:58,528 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:37821'
2025-10-15 01:18:58,532 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:43181'
2025-10-15 01:18:58,537 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:34565'
2025-10-15 01:18:58,541 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:37681'
2025-10-15 01:18:58,544 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:43991'
2025-10-15 01:18:58,549 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:42257'
2025-10-15 01:18:58,554 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:46133'
2025-10-15 01:18:58,559 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:39901'
2025-10-15 01:18:58,563 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:39669'
2025-10-15 01:18:58,568 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:36993'
2025-10-15 01:18:58,572 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:38999'
2025-10-15 01:18:58,577 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:35801'
2025-10-15 01:18:58,582 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:35169'
2025-10-15 01:18:58,585 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:38519'
2025-10-15 01:18:58,588 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:33697'
2025-10-15 01:18:58,593 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:43165'
2025-10-15 01:18:58,597 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:35815'
2025-10-15 01:18:58,603 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:45947'
2025-10-15 01:18:58,607 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:37843'
2025-10-15 01:18:58,611 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:45287'
2025-10-15 01:18:58,614 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:39945'
2025-10-15 01:18:58,618 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:43163'
2025-10-15 01:18:58,622 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:37913'
2025-10-15 01:18:58,626 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:37767'
2025-10-15 01:18:59,704 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:45397
2025-10-15 01:18:59,704 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:40629
2025-10-15 01:18:59,704 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:45397
2025-10-15 01:18:59,704 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:40629
2025-10-15 01:18:59,704 - distributed.worker - INFO -          dashboard at:            10.6.5.29:46191
2025-10-15 01:18:59,704 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:43593
2025-10-15 01:18:59,704 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:41791
2025-10-15 01:18:59,704 - distributed.worker - INFO -          dashboard at:            10.6.5.29:37579
2025-10-15 01:18:59,704 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,704 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:43593
2025-10-15 01:18:59,704 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:41791
2025-10-15 01:18:59,705 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,705 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,705 - distributed.worker - INFO -          dashboard at:            10.6.5.29:37637
2025-10-15 01:18:59,705 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:18:59,705 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,705 - distributed.worker - INFO -          dashboard at:            10.6.5.29:39089
2025-10-15 01:18:59,705 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,705 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:18:59,705 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:18:59,705 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,705 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,705 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-imh594mw
2025-10-15 01:18:59,705 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:18:59,705 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,705 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:18:59,705 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,705 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:18:59,705 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-jhvcp1b1
2025-10-15 01:18:59,705 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:18:59,705 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:18:59,705 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,705 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-v6eg6qy1
2025-10-15 01:18:59,705 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-944mh7vo
2025-10-15 01:18:59,705 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,705 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,707 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:34461
2025-10-15 01:18:59,707 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:34461
2025-10-15 01:18:59,707 - distributed.worker - INFO -          dashboard at:            10.6.5.29:36699
2025-10-15 01:18:59,707 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,707 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,707 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:18:59,707 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:18:59,707 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-f_xn_9qo
2025-10-15 01:18:59,707 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,714 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:35143
2025-10-15 01:18:59,714 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:35143
2025-10-15 01:18:59,714 - distributed.worker - INFO -          dashboard at:            10.6.5.29:41411
2025-10-15 01:18:59,714 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,714 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,714 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:18:59,715 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:18:59,715 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-eenj2l20
2025-10-15 01:18:59,715 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,715 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:36307
2025-10-15 01:18:59,715 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:36307
2025-10-15 01:18:59,715 - distributed.worker - INFO -          dashboard at:            10.6.5.29:36571
2025-10-15 01:18:59,715 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,715 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,715 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:18:59,715 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:18:59,715 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-xtng3hsf
2025-10-15 01:18:59,715 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,719 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:18:59,720 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,720 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,726 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:18:59,726 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,726 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,727 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:18:59,727 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:18:59,728 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:18:59,728 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,728 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,729 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:18:59,730 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:18:59,730 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,730 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,731 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:18:59,731 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:18:59,732 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,732 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,733 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:18:59,733 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:18:59,733 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,734 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,734 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:18:59,735 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:18:59,736 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,736 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,736 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:18:59,927 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:37435
2025-10-15 01:18:59,927 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:37435
2025-10-15 01:18:59,927 - distributed.worker - INFO -          dashboard at:            10.6.5.29:46613
2025-10-15 01:18:59,927 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,927 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,927 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:18:59,927 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:18:59,927 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-x20he9dz
2025-10-15 01:18:59,927 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,929 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:37451
2025-10-15 01:18:59,930 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:37451
2025-10-15 01:18:59,930 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:38607
2025-10-15 01:18:59,930 - distributed.worker - INFO -          dashboard at:            10.6.5.29:35559
2025-10-15 01:18:59,930 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:38607
2025-10-15 01:18:59,930 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,930 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,930 - distributed.worker - INFO -          dashboard at:            10.6.5.29:46327
2025-10-15 01:18:59,930 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:18:59,930 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,930 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,930 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:18:59,930 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:18:59,930 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-fnahvmvl
2025-10-15 01:18:59,930 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:18:59,930 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,930 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-lv_d514_
2025-10-15 01:18:59,930 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,932 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:36615
2025-10-15 01:18:59,932 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:36615
2025-10-15 01:18:59,932 - distributed.worker - INFO -          dashboard at:            10.6.5.29:35807
2025-10-15 01:18:59,932 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,932 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,932 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:18:59,932 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:18:59,932 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-xpoypyy_
2025-10-15 01:18:59,932 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,932 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:42199
2025-10-15 01:18:59,932 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:42199
2025-10-15 01:18:59,932 - distributed.worker - INFO -          dashboard at:            10.6.5.29:42061
2025-10-15 01:18:59,933 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,933 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,933 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:18:59,933 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:18:59,933 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-p4ekpluv
2025-10-15 01:18:59,933 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,938 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:39347
2025-10-15 01:18:59,938 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:39347
2025-10-15 01:18:59,938 - distributed.worker - INFO -          dashboard at:            10.6.5.29:40691
2025-10-15 01:18:59,938 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,938 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,938 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:18:59,938 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:18:59,938 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-qtzd97g7
2025-10-15 01:18:59,939 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,939 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:18:59,939 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,940 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,940 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:18:59,942 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:18:59,942 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,942 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,943 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:18:59,944 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:18:59,944 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,944 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,945 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:18:59,947 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:46711
2025-10-15 01:18:59,947 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:44715
2025-10-15 01:18:59,947 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:46711
2025-10-15 01:18:59,947 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:44715
2025-10-15 01:18:59,948 - distributed.worker - INFO -          dashboard at:            10.6.5.29:40805
2025-10-15 01:18:59,948 - distributed.worker - INFO -          dashboard at:            10.6.5.29:33169
2025-10-15 01:18:59,948 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,948 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,948 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,948 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,948 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:18:59,948 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:18:59,948 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:18:59,948 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:18:59,948 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-f7ztbfqm
2025-10-15 01:18:59,948 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-hy2eu3sh
2025-10-15 01:18:59,948 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,948 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,949 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:18:59,950 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,950 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,951 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:18:59,952 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:34227
2025-10-15 01:18:59,952 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:34227
2025-10-15 01:18:59,952 - distributed.worker - INFO -          dashboard at:            10.6.5.29:38111
2025-10-15 01:18:59,952 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,952 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,952 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:18:59,952 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:18:59,952 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-69tphasu
2025-10-15 01:18:59,952 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,953 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:18:59,954 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,954 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,955 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:18:59,955 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:18:59,956 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,956 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,957 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:18:59,959 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:18:59,959 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,959 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,960 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:18:59,962 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:18:59,963 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,963 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,964 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:18:59,964 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:45797
2025-10-15 01:18:59,964 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:45797
2025-10-15 01:18:59,964 - distributed.worker - INFO -          dashboard at:            10.6.5.29:34037
2025-10-15 01:18:59,964 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,964 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,964 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:18:59,964 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:18:59,964 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-zoxfoz9w
2025-10-15 01:18:59,964 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,968 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:44967
2025-10-15 01:18:59,968 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:44967
2025-10-15 01:18:59,968 - distributed.worker - INFO -          dashboard at:            10.6.5.29:46733
2025-10-15 01:18:59,968 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,968 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,968 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:18:59,968 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:18:59,968 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-d_j2ax61
2025-10-15 01:18:59,968 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:18:59,968 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,969 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,969 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,970 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:18:59,975 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:18:59,976 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,976 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,977 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:18:59,980 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:44071
2025-10-15 01:18:59,981 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:44071
2025-10-15 01:18:59,981 - distributed.worker - INFO -          dashboard at:            10.6.5.29:36379
2025-10-15 01:18:59,981 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,981 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,981 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:18:59,981 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:18:59,981 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-zvzs1kyd
2025-10-15 01:18:59,981 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,988 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:18:59,989 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,989 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,990 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:18:59,995 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:18:59,996 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:18:59,996 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:18:59,997 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,002 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:37299
2025-10-15 01:19:00,002 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:37299
2025-10-15 01:19:00,002 - distributed.worker - INFO -          dashboard at:            10.6.5.29:37719
2025-10-15 01:19:00,003 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,003 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,003 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,003 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,003 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-48t3n41_
2025-10-15 01:19:00,003 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,023 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:43329
2025-10-15 01:19:00,023 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:43329
2025-10-15 01:19:00,023 - distributed.worker - INFO -          dashboard at:            10.6.5.29:41021
2025-10-15 01:19:00,023 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,023 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,024 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,024 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,024 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-jake_f5a
2025-10-15 01:19:00,024 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,024 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,025 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,025 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,026 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,035 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,036 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,036 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,036 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,073 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:45253
2025-10-15 01:19:00,074 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:45253
2025-10-15 01:19:00,074 - distributed.worker - INFO -          dashboard at:            10.6.5.29:41711
2025-10-15 01:19:00,074 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,074 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,074 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,074 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,074 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-dleapk6e
2025-10-15 01:19:00,074 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,082 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:34877
2025-10-15 01:19:00,082 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:34877
2025-10-15 01:19:00,082 - distributed.worker - INFO -          dashboard at:            10.6.5.29:36097
2025-10-15 01:19:00,082 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,082 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,082 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,082 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,082 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-zr2riw_u
2025-10-15 01:19:00,082 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,086 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,087 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,087 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,087 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,101 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,102 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,102 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,103 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,123 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:42903
2025-10-15 01:19:00,123 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:42903
2025-10-15 01:19:00,123 - distributed.worker - INFO -          dashboard at:            10.6.5.29:37377
2025-10-15 01:19:00,123 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,123 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,123 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,123 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,124 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-mwuh_ak5
2025-10-15 01:19:00,124 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,133 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:37261
2025-10-15 01:19:00,133 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:37261
2025-10-15 01:19:00,133 - distributed.worker - INFO -          dashboard at:            10.6.5.29:41923
2025-10-15 01:19:00,133 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,133 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,134 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,134 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,134 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-byz466o2
2025-10-15 01:19:00,134 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,144 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,145 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,145 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,146 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,146 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,146 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,146 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:41275
2025-10-15 01:19:00,147 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,147 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:41275
2025-10-15 01:19:00,147 - distributed.worker - INFO -          dashboard at:            10.6.5.29:45321
2025-10-15 01:19:00,147 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,147 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,147 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,147 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,147 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-u9x2j94u
2025-10-15 01:19:00,147 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,147 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,160 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:35955
2025-10-15 01:19:00,160 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:35955
2025-10-15 01:19:00,161 - distributed.worker - INFO -          dashboard at:            10.6.5.29:44611
2025-10-15 01:19:00,161 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,161 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,161 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,161 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,161 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-yfc7dcwj
2025-10-15 01:19:00,161 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,167 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,168 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,168 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,169 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,171 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:45267
2025-10-15 01:19:00,171 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:45267
2025-10-15 01:19:00,171 - distributed.worker - INFO -          dashboard at:            10.6.5.29:43627
2025-10-15 01:19:00,171 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,171 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,171 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,171 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,171 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-280xvrnu
2025-10-15 01:19:00,171 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,172 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:44055
2025-10-15 01:19:00,172 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:44055
2025-10-15 01:19:00,172 - distributed.worker - INFO -          dashboard at:            10.6.5.29:45995
2025-10-15 01:19:00,172 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,172 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,172 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,172 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,172 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-cu_i5syf
2025-10-15 01:19:00,172 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,172 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,173 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,173 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,174 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,182 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,182 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,182 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,183 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,187 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:44033
2025-10-15 01:19:00,187 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:44033
2025-10-15 01:19:00,187 - distributed.worker - INFO -          dashboard at:            10.6.5.29:33445
2025-10-15 01:19:00,187 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,187 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,187 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,187 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,187 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-nrvprosj
2025-10-15 01:19:00,188 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,191 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:44565
2025-10-15 01:19:00,191 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:44565
2025-10-15 01:19:00,191 - distributed.worker - INFO -          dashboard at:            10.6.5.29:46145
2025-10-15 01:19:00,191 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,191 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,192 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,192 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,192 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-4xr90dul
2025-10-15 01:19:00,192 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,192 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,192 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,192 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,193 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,198 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:36821
2025-10-15 01:19:00,198 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:36821
2025-10-15 01:19:00,198 - distributed.worker - INFO -          dashboard at:            10.6.5.29:34555
2025-10-15 01:19:00,198 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,198 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,199 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,199 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,199 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-7awdnob6
2025-10-15 01:19:00,199 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,199 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,199 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,199 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,200 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,205 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:45631
2025-10-15 01:19:00,205 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:45631
2025-10-15 01:19:00,205 - distributed.worker - INFO -          dashboard at:            10.6.5.29:44035
2025-10-15 01:19:00,205 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,206 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,206 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,206 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,206 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-ku_utlr9
2025-10-15 01:19:00,206 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,210 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,211 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,211 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,212 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,213 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:34435
2025-10-15 01:19:00,213 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:34435
2025-10-15 01:19:00,214 - distributed.worker - INFO -          dashboard at:            10.6.5.29:37047
2025-10-15 01:19:00,214 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,214 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,214 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,214 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,214 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-dvzmd495
2025-10-15 01:19:00,214 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,218 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,219 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,219 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,221 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:44947
2025-10-15 01:19:00,221 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,221 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:44947
2025-10-15 01:19:00,221 - distributed.worker - INFO -          dashboard at:            10.6.5.29:45441
2025-10-15 01:19:00,221 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,221 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,221 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,221 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,221 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-t83oa2tb
2025-10-15 01:19:00,221 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,223 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:33815
2025-10-15 01:19:00,223 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:33815
2025-10-15 01:19:00,223 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:46331
2025-10-15 01:19:00,223 - distributed.worker - INFO -          dashboard at:            10.6.5.29:33075
2025-10-15 01:19:00,223 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:46331
2025-10-15 01:19:00,223 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,223 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,223 - distributed.worker - INFO -          dashboard at:            10.6.5.29:39639
2025-10-15 01:19:00,223 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,223 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,223 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,223 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,224 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,224 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-2nht8qlb
2025-10-15 01:19:00,224 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,224 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,224 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-2awlhsfz
2025-10-15 01:19:00,224 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,224 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:41163
2025-10-15 01:19:00,224 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:41163
2025-10-15 01:19:00,224 - distributed.worker - INFO -          dashboard at:            10.6.5.29:43957
2025-10-15 01:19:00,224 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,224 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,224 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,224 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,224 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-usfgy1if
2025-10-15 01:19:00,224 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,224 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:42955
2025-10-15 01:19:00,224 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:42955
2025-10-15 01:19:00,224 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:46407
2025-10-15 01:19:00,225 - distributed.worker - INFO -          dashboard at:            10.6.5.29:40005
2025-10-15 01:19:00,225 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:46407
2025-10-15 01:19:00,225 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,225 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:41893
2025-10-15 01:19:00,225 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,225 - distributed.worker - INFO -          dashboard at:            10.6.5.29:43071
2025-10-15 01:19:00,225 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,225 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:41893
2025-10-15 01:19:00,225 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,225 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,225 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,225 - distributed.worker - INFO -          dashboard at:            10.6.5.29:45203
2025-10-15 01:19:00,225 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,225 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-40s_yau2
2025-10-15 01:19:00,225 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,225 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,225 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,225 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,225 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,225 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-jytg6w4a
2025-10-15 01:19:00,225 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,225 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,225 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-fgof9wxg
2025-10-15 01:19:00,225 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,225 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:38535
2025-10-15 01:19:00,225 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:38535
2025-10-15 01:19:00,225 - distributed.worker - INFO -          dashboard at:            10.6.5.29:45433
2025-10-15 01:19:00,226 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,226 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,226 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,226 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,226 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-zetgo9zy
2025-10-15 01:19:00,226 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,226 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,227 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,227 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,228 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,228 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:44507
2025-10-15 01:19:00,228 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:44507
2025-10-15 01:19:00,229 - distributed.worker - INFO -          dashboard at:            10.6.5.29:36923
2025-10-15 01:19:00,229 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,229 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,229 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,229 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,229 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-k46_2bgu
2025-10-15 01:19:00,229 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,232 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,233 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,233 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,234 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,234 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,234 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,234 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,235 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,237 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,237 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,237 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,238 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,239 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,240 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,240 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,241 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,241 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:35113
2025-10-15 01:19:00,241 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:35113
2025-10-15 01:19:00,241 - distributed.worker - INFO -          dashboard at:            10.6.5.29:35661
2025-10-15 01:19:00,241 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,241 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,241 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,241 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,241 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-l4luje7h
2025-10-15 01:19:00,241 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,245 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,246 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,246 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,247 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,251 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,251 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:46703
2025-10-15 01:19:00,251 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:46703
2025-10-15 01:19:00,251 - distributed.worker - INFO -          dashboard at:            10.6.5.29:36391
2025-10-15 01:19:00,251 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,251 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,251 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,251 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,251 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-hjhdre3k
2025-10-15 01:19:00,251 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,252 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,252 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,252 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,253 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,253 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,253 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,253 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,254 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:35915
2025-10-15 01:19:00,254 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:35915
2025-10-15 01:19:00,254 - distributed.worker - INFO -          dashboard at:            10.6.5.29:44969
2025-10-15 01:19:00,254 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,254 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,254 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,254 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,254 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-u2ojjx0c
2025-10-15 01:19:00,254 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,254 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,254 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,255 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,255 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,255 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,256 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,256 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,256 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,256 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:35985
2025-10-15 01:19:00,256 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:35985
2025-10-15 01:19:00,256 - distributed.worker - INFO -          dashboard at:            10.6.5.29:39799
2025-10-15 01:19:00,256 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,257 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,257 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,257 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,257 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,257 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,257 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-jdc4ztlt
2025-10-15 01:19:00,257 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,257 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,258 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,259 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,260 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,260 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,261 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,265 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,266 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,266 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,266 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,273 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,274 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,274 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,275 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,277 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,278 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,278 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,279 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:00,288 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:36731
2025-10-15 01:19:00,288 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:36731
2025-10-15 01:19:00,288 - distributed.worker - INFO -          dashboard at:            10.6.5.29:38721
2025-10-15 01:19:00,288 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,288 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,288 - distributed.worker - INFO -               Threads:                          1
2025-10-15 01:19:00,288 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-15 01:19:00,288 - distributed.worker - INFO -       Local Directory: /jobfs/152303358.gadi-pbs/dask-scratch-space/worker-o0v2hhwp
2025-10-15 01:19:00,288 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,298 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-15 01:19:00,299 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8799
2025-10-15 01:19:00,299 - distributed.worker - INFO - -------------------------------------------------
2025-10-15 01:19:00,299 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8799
2025-10-15 01:19:07,262 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,262 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,262 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,263 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,264 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,264 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,264 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,264 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,265 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,265 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,264 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,265 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,265 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,265 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,265 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,266 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,266 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,266 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,266 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,266 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,266 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,267 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,266 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,267 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,267 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,267 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,267 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,267 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,268 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,268 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,268 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,269 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,269 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,269 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,269 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,269 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,270 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,270 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,270 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,270 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,270 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,270 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,271 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,271 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,271 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,271 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,271 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,272 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,272 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,271 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,272 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,272 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,272 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,272 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,272 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,271 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,273 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,273 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,273 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,273 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,274 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,274 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,274 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,274 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,274 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,274 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,275 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,275 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,275 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,275 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,276 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,276 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,276 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,276 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,276 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,277 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,277 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,277 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,277 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,277 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,278 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,278 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,278 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,279 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,280 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,280 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,281 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,281 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,281 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,281 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,282 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,282 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,282 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,282 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:07,287 - distributed.worker - INFO - Starting Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 01:19:07,291 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-15 01:19:09,814 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,814 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,814 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,815 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,815 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,815 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,815 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,816 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,815 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,816 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,815 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,816 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,816 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,816 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,816 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,817 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,816 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,817 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,816 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,817 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,817 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,817 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,817 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,817 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,817 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,817 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,817 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,818 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,818 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,818 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,816 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,818 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,818 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,818 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,818 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,818 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,819 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,819 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,819 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,819 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,819 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,819 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,819 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,819 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,820 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,819 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,819 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,820 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,820 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,819 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,820 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,820 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,820 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,820 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,820 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,821 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,820 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,821 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,821 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,821 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,821 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,821 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,821 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,822 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,822 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,821 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,822 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,822 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,821 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,822 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,822 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,822 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,822 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,821 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,822 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,822 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,823 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,823 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,823 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,823 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,824 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,824 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,824 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,823 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,824 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,823 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,824 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,824 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,822 - distributed.worker - INFO - Starting Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 01:19:09,824 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,825 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,825 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,825 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,825 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,825 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,826 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-15 01:19:09,914 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,914 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,916 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,916 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,916 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,917 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,917 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,917 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,917 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,918 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,918 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,918 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,918 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,918 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,919 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,919 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,919 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,919 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,919 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,919 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,919 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,920 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,920 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,920 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,920 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,920 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,920 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,920 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,920 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,920 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,921 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,921 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,921 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,921 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,921 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,921 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,921 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,921 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,922 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,922 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,922 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,922 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,922 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,922 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,922 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,922 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,922 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,922 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,922 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,923 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,923 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,923 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,923 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,923 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,923 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,923 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,923 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,924 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,924 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,924 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,924 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,924 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,924 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,924 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,924 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,924 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,924 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,925 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,925 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,925 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,925 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,925 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,925 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,925 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,925 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,926 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,926 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,926 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,926 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,926 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,926 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,926 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,926 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,927 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,927 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,927 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,927 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,927 - distributed.worker - INFO - Starting Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 01:19:09,928 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,928 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,928 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,928 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,928 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,928 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,929 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:09,930 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-15 01:19:10,021 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,021 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,021 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,022 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,022 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,022 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,022 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,022 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,022 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,022 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,022 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,022 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,023 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,023 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,023 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,023 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,023 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,023 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,023 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,023 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,023 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,023 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,024 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,024 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,024 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,024 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,024 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,024 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,024 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,024 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,024 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,024 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,024 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,025 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,025 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,025 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,025 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,025 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,025 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,025 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,025 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,025 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,025 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,025 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,026 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,026 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,026 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,026 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,026 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,026 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,026 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,026 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,026 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,027 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,027 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,027 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,027 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,027 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,027 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,027 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,027 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,027 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,027 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,028 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,028 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,027 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,028 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,028 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,028 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,028 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,028 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,028 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,029 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,029 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,029 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,029 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,029 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,029 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,029 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,029 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,029 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,029 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,029 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,029 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,030 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,030 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,030 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,030 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,030 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,031 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,031 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,030 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,031 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,031 - distributed.worker - INFO - Starting Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 01:19:10,034 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:19:10,034 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-15 01:20:37,453 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-t83oa2tb/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-jdc4ztlt/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-zoxfoz9w/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-xpoypyy_/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-xtng3hsf/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-fgof9wxg/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-cu_i5syf/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
2025-10-15 01:22:32,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-fnahvmvl/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-hjhdre3k/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-48t3n41_/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-jake_f5a/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-f7ztbfqm/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-hy2eu3sh/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-o0v2hhwp/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-d_j2ax61/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-280xvrnu/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-jytg6w4a/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-mwuh_ak5/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-jhvcp1b1/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-x20he9dz/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-2nht8qlb/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-dleapk6e/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-k46_2bgu/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-ku_utlr9/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-v6eg6qy1/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-69tphasu/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-zetgo9zy/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-qtzd97g7/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-40s_yau2/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-yfc7dcwj/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-u9x2j94u/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-dvzmd495/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-944mh7vo/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-u2ojjx0c/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-byz466o2/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-imh594mw/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-nrvprosj/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-zvzs1kyd/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-l4luje7h/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-7awdnob6/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-usfgy1if/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-4xr90dul/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-eenj2l20/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-lv_d514_/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-2awlhsfz/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-f_xn_9qo/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-zr2riw_u/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
/jobfs/152303358.gadi-pbs/dask-scratch-space/worker-p4ekpluv/qme_utils.py:96: RuntimeWarning: invalid value encountered in cast
  return rounded.astype(int)
2025-10-15 04:15:22,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-15 04:15:38,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-15 04:16:15,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-15 04:16:32,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-15 04:16:47,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-15 04:17:03,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-15 04:17:21,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-15 04:17:32,309 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-15 04:18:26,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-15 04:19:12,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-15 04:20:11,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-15 04:20:37,738 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-15 04:20:59,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-15 07:21:56,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-15 07:22:05,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-15 09:29:24,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-15 09:54:23,208 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,208 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,209 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:46711. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,209 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:35915. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,208 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,209 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:44565. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,208 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,208 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,208 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,208 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,208 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,210 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:42955. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,210 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:35985. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,210 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:34435. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,211 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:35113. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,208 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,211 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:36731. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,208 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,208 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,206 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:40268 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,211 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:45631. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,205 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45310 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,205 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45298 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,206 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:40110 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,211 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:38535. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,211 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:41893. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,208 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:55318 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,206 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:40082 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,208 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:55186 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,211 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,213 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:44033. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,208 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,212 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,213 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:41275. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,213 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:41163. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,212 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,208 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:55204 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,212 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,212 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,214 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:43593. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,206 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:40108 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,206 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:40190 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,214 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:40629. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,214 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:44715. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,213 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,214 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:36615. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,208 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,207 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:40164 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,215 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:46407. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,206 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:40212 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,206 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:40174 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,213 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,208 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,208 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:55298 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,208 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,214 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,216 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:36821. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,216 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:42199. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,216 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:44947. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,216 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:33815. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,208 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,206 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:40224 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,207 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:40098 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,214 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,207 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:37836 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,208 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,217 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:35143. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,217 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:46703. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,209 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:55172 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,216 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,215 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,210 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:55238 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,214 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,218 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:43329. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,206 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:40134 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,216 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,216 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,218 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:34227. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,218 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:45397. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,218 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:42903. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,218 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:45797. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,218 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:44507. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,207 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:40256 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,214 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,218 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,219 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:44071. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,219 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:37451. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,205 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:45324 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,208 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:55208 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,217 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,220 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:46331. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,218 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,218 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,218 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,208 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:55284 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,220 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:37261. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,220 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:41791. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,221 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:36307. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,219 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,221 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:39347. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,220 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,220 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,222 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:44055. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,222 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:38607. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,209 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:55272 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,221 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,221 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,225 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:37299. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,225 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:45253. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,223 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,226 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:44967. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,227 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:55296 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,232 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:39443'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,233 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:35815'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,233 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,233 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:42257'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,233 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,233 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 1, 9, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,233 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:35169'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,234 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,234 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,234 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,234 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,234 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:37681'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,234 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 1, 7, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,234 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,234 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,234 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,234 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,234 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,234 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:43991'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,234 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,234 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 1, 15, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,231 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,234 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:38225'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,235 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,235 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,235 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,235 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,235 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:37913'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,235 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:34877. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,235 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,235 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 0, 9, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,235 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,235 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:45287'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,235 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,235 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,235 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,235 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,235 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:33697'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,235 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,236 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,236 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,236 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,235 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,236 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,236 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,236 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,236 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:43165'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,236 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,236 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,236 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,236 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,236 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:39669'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,236 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,236 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,236 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,236 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,236 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,236 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:43341'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,236 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,236 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 1, 6, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,236 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,237 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,237 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,237 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:36749'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,237 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,237 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,237 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,237 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,237 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,237 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:45743'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,237 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,237 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,237 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,237 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,237 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,237 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:43163'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,237 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 1, 14, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,238 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:37417'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,238 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:37843'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,238 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:34565'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,238 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,239 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,239 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,239 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,239 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 0, 14, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,239 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 0, 4, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,239 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 1, 13, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,239 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,239 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,239 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,239 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,239 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,240 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,240 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,240 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,240 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,240 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,240 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,240 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,240 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,240 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,240 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,245 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:33767'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,245 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:36993'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,246 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:40033'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,246 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:38519'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,246 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,246 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:38999'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,246 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 1, 18, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,246 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,246 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:37747'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,239 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x148430d37550>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-15 09:54:23,246 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,247 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,247 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,247 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:37821'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,247 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,247 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,247 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 0, 12, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,247 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,247 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:36901'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,247 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,247 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,247 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:42469'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,247 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 0, 10, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,247 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,247 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,247 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,247 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,247 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,247 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,247 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 0, 13, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,247 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:41705'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,247 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,247 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,247 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,248 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,248 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,248 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:46133'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,248 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,248 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,248 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,248 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,248 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,248 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,248 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:39349'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,248 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,248 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,248 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 1, 2, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,248 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,248 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,248 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,248 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,248 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,248 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,248 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,248 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 1, 4, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,249 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,249 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,249 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,249 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,249 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,249 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,249 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,249 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,249 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,249 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,249 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,249 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,249 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,249 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,249 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,249 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,249 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,250 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,250 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,250 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,250 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,250 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,250 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,250 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,253 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:23,255 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:39861'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,256 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:35801'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,256 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:37767'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,256 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 1, 16, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,256 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,256 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:39901'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,256 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,256 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,256 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,256 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:35865'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,257 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,257 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,257 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,257 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,257 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,257 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,257 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:34047'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,257 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,257 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,257 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 0, 3, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,257 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,257 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:35209'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,257 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,257 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,257 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,257 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:45947'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,257 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,257 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,257 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,257 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:23,257 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:39581'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,258 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,257 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,258 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:23,258 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:40617'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,258 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,258 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:23,258 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,258 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 0, 11, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,258 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:40329'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,258 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,258 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 1, 20, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,258 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,258 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,258 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,250 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14d7b09e7410>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-15 09:54:23,258 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:34103'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,258 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,258 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,259 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,259 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,259 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,259 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,259 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,259 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,259 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,259 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 1, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,259 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 1, 3, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,259 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,259 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,259 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,259 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,259 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,259 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,260 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,260 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 0, 2, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,260 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,260 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,260 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-xarray-wbgt-transpose-909331efae4f79d60cd2bbf1e6b63ec1', 0, 16, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,260 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,227 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:40102 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,260 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:43181'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,260 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,250 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14f371400390>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-15 09:54:23,253 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14b4d8ef3b90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-15 09:54:23,260 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,260 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,260 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,260 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,261 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('vectorize_apply-vectorize_apply_0-store-map-c4d21664f1cd7ffb14c3fd0ad841f69d', 13, 19, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,261 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,261 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,261 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,262 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,262 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,262 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,262 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,262 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,262 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,262 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,262 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,262 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,255 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14a9d2ca9350>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-15 09:54:23,262 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,263 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,254 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14c3af86b650>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-15 09:54:23,263 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:23,264 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:23,260 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14e1c695fe10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-15 09:54:23,268 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:23,266 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x148d32cfae10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-15 09:54:23,260 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,275 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:34461. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,287 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:23,291 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:23,296 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:23,297 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:23,303 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:23,310 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:23,311 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:23,314 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:23,316 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:23,317 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:23,317 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:23,317 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:23,359 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:23,423 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,431 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:45267. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,355 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:40238 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,468 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,472 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:35955. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,598 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:41329'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,606 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('vectorize_apply-vectorize_apply_0-store-map-c4d21664f1cd7ffb14c3fd0ad841f69d', 10, 9, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,612 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,614 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,614 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,620 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,621 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,672 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:39945'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,674 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('vectorize_apply-vectorize_apply_0-store-map-c4d21664f1cd7ffb14c3fd0ad841f69d', 13, 7, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,674 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,675 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,675 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,675 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,675 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,681 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15159c573350>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-15 09:54:23,699 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:23,627 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:37840 remote=tcp://10.6.5.28:8799>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-15 09:54:23,728 - distributed.core - INFO - Connection to tcp://10.6.5.28:8799 has been closed.
2025-10-15 09:54:23,731 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:37435. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,741 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:46871'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,742 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('vectorize_apply-vectorize_apply_0-store-map-c4d21664f1cd7ffb14c3fd0ad841f69d', 11, 17, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,742 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,742 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,742 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,742 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,742 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,787 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:39953'. Reason: worker-handle-scheduler-connection-broken
2025-10-15 09:54:23,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('vectorize_apply-vectorize_apply_0-store-map-c4d21664f1cd7ffb14c3fd0ad841f69d', 11, 11, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-15 09:54:23,810 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-15 09:54:23,811 - distributed.worker - INFO - Removing Worker plugin qme_utils.py0b3c6152-4126-418d-8998-0c1ced909a5f
2025-10-15 09:54:23,811 - distributed.worker - INFO - Removing Worker plugin qme_vars.py657553fd-ecea-4df2-ac28-c947c2091ae7
2025-10-15 09:54:23,817 - distributed.worker - INFO - Removing Worker plugin qme_train.py6ad3b222-1455-4359-b9fd-837866539024
2025-10-15 09:54:23,818 - distributed.worker - INFO - Removing Worker plugin qme_apply.py48270bef-fad5-49f9-8fa2-1c7e5d764fc4
2025-10-15 09:54:23,927 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:24,246 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:24,370 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:24,719 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:25,256 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-15 09:54:25,260 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-15 09:54:25,261 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-15 09:54:25,261 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-15 09:54:25,266 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-15 09:54:25,267 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-15 09:54:25,290 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-15 09:54:25,296 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-15 09:54:25,300 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-15 09:54:25,318 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-15 09:54:25,318 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-15 09:54:25,320 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-15 09:54:25,322 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-15 09:54:25,367 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-15 09:54:26,375 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-15 09:54:26,438 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:43165'. Reason: nanny-close-gracefully
2025-10-15 09:54:26,441 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:43165' closed.
2025-10-15 09:54:26,443 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:37913'. Reason: nanny-close-gracefully
2025-10-15 09:54:26,446 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:42469'. Reason: nanny-close-gracefully
2025-10-15 09:54:26,447 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:39443'. Reason: nanny-close-gracefully
2025-10-15 09:54:26,447 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:37913' closed.
2025-10-15 09:54:26,449 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:42469' closed.
2025-10-15 09:54:26,450 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:39443' closed.
2025-10-15 09:54:26,574 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:46133'. Reason: nanny-close-gracefully
2025-10-15 09:54:26,575 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:46133' closed.
2025-10-15 09:54:26,599 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:38999'. Reason: nanny-close-gracefully
2025-10-15 09:54:26,600 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:38999' closed.
2025-10-15 09:54:26,690 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:33697'. Reason: nanny-close-gracefully
2025-10-15 09:54:26,691 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:33697' closed.
2025-10-15 09:54:26,722 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-15 09:54:27,091 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:39945'. Reason: nanny-close-gracefully
2025-10-15 09:54:27,092 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:39945' closed.
2025-10-15 09:54:27,218 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:37821'. Reason: nanny-close-gracefully
2025-10-15 09:54:27,221 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:37821' closed.
2025-10-15 09:54:27,261 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:45743'. Reason: nanny-close-gracefully
2025-10-15 09:54:27,267 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:45743' closed.
2025-10-15 09:54:27,292 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:43341'. Reason: nanny-close-gracefully
2025-10-15 09:54:27,293 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:43341' closed.
2025-10-15 09:54:27,330 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:36749'. Reason: nanny-close-gracefully
2025-10-15 09:54:27,332 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:36749' closed.
2025-10-15 09:54:27,377 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:35865'. Reason: nanny-close-gracefully
2025-10-15 09:54:27,377 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:35865' closed.
2025-10-15 09:54:27,399 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:35801'. Reason: nanny-close-gracefully
2025-10-15 09:54:27,400 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:35801' closed.
2025-10-15 09:54:27,431 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:38225'. Reason: nanny-close-gracefully
2025-10-15 09:54:27,433 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:38225' closed.
2025-10-15 09:54:27,677 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:43181'. Reason: nanny-close-gracefully
2025-10-15 09:54:27,677 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:43181' closed.
2025-10-15 09:54:27,683 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:37681'. Reason: nanny-close-gracefully
2025-10-15 09:54:27,684 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:37681' closed.
2025-10-15 09:54:28,139 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:46871'. Reason: nanny-close-gracefully
2025-10-15 09:54:28,140 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:46871' closed.
2025-10-15 09:54:28,177 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:39901'. Reason: nanny-close-gracefully
2025-10-15 09:54:28,177 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:39901' closed.
2025-10-15 09:54:28,189 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:39349'. Reason: nanny-close-gracefully
2025-10-15 09:54:28,190 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:39349' closed.
2025-10-15 09:54:28,288 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:34047'. Reason: nanny-close-gracefully
2025-10-15 09:54:28,289 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:34047' closed.
2025-10-15 09:54:28,346 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:41329'. Reason: nanny-close-gracefully
2025-10-15 09:54:28,347 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:41329' closed.
2025-10-15 09:54:28,451 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:39953'. Reason: nanny-close-gracefully
2025-10-15 09:54:28,452 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:39953' closed.
2025-10-15 09:54:28,467 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:40033'. Reason: nanny-close-gracefully
2025-10-15 09:54:28,470 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:40033' closed.
2025-10-15 09:54:28,482 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:37843'. Reason: nanny-close-gracefully
2025-10-15 09:54:28,483 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:37843' closed.
2025-10-15 09:54:29,982 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:29,982 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:29,982 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:29,983 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:29,983 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:29,986 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:29,986 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:29,988 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:29,990 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:29,993 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:30,007 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:30,018 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:30,024 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:30,031 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:30,033 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:30,034 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:30,036 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:30,039 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:30,042 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:30,042 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:30,042 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:30,045 - distributed.nanny - INFO - Worker closed
2025-10-15 09:54:30,055 - distributed.nanny - INFO - Worker closed
