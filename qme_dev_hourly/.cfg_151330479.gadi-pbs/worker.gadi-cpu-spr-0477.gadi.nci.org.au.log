Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-10-01 11:43:21,341 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:35961'
2025-10-01 11:43:21,348 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:33025'
2025-10-01 11:43:21,351 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:42917'
2025-10-01 11:43:21,356 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:43103'
2025-10-01 11:43:21,360 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:41789'
2025-10-01 11:43:21,364 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:37063'
2025-10-01 11:43:21,368 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:42173'
2025-10-01 11:43:21,372 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:39779'
2025-10-01 11:43:21,376 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:44393'
2025-10-01 11:43:21,381 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:39877'
2025-10-01 11:43:21,385 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:40007'
2025-10-01 11:43:21,389 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:36549'
2025-10-01 11:43:21,393 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:42645'
2025-10-01 11:43:21,397 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:39003'
2025-10-01 11:43:21,401 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:40785'
2025-10-01 11:43:21,406 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:39271'
2025-10-01 11:43:21,411 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:38585'
2025-10-01 11:43:21,416 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:35077'
2025-10-01 11:43:21,420 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:46505'
2025-10-01 11:43:21,424 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:44157'
2025-10-01 11:43:21,509 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:41971'
2025-10-01 11:43:21,512 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:35797'
2025-10-01 11:43:21,517 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:34189'
2025-10-01 11:43:21,521 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:46125'
2025-10-01 11:43:21,525 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:46843'
2025-10-01 11:43:21,529 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:36071'
2025-10-01 11:43:21,534 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:41277'
2025-10-01 11:43:21,539 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:45303'
2025-10-01 11:43:21,543 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:43441'
2025-10-01 11:43:21,547 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:37047'
2025-10-01 11:43:21,552 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:39553'
2025-10-01 11:43:21,555 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:40061'
2025-10-01 11:43:21,560 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:33621'
2025-10-01 11:43:21,564 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:46003'
2025-10-01 11:43:21,569 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:34215'
2025-10-01 11:43:21,574 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:35055'
2025-10-01 11:43:21,579 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:40179'
2025-10-01 11:43:21,584 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:44603'
2025-10-01 11:43:21,589 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:41755'
2025-10-01 11:43:21,593 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:37989'
2025-10-01 11:43:21,597 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:40355'
2025-10-01 11:43:21,601 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:38003'
2025-10-01 11:43:21,606 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:43877'
2025-10-01 11:43:21,611 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:41069'
2025-10-01 11:43:21,616 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:37695'
2025-10-01 11:43:21,620 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:34635'
2025-10-01 11:43:21,625 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:40987'
2025-10-01 11:43:21,628 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:34219'
2025-10-01 11:43:21,632 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:41371'
2025-10-01 11:43:21,638 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:33197'
2025-10-01 11:43:21,647 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:36307'
2025-10-01 11:43:21,651 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.45:43289'
2025-10-01 11:43:22,630 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:46523
2025-10-01 11:43:22,630 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:42901
2025-10-01 11:43:22,631 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:40733
2025-10-01 11:43:22,631 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:46801
2025-10-01 11:43:22,631 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:33845
2025-10-01 11:43:22,631 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:32999
2025-10-01 11:43:22,631 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:46523
2025-10-01 11:43:22,631 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:37059
2025-10-01 11:43:22,631 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:42901
2025-10-01 11:43:22,631 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:39241
2025-10-01 11:43:22,631 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:46227
2025-10-01 11:43:22,631 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:37929
2025-10-01 11:43:22,631 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:40733
2025-10-01 11:43:22,631 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:41877
2025-10-01 11:43:22,631 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:34849
2025-10-01 11:43:22,631 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:33235
2025-10-01 11:43:22,631 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:46801
2025-10-01 11:43:22,631 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:33845
2025-10-01 11:43:22,631 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:37609
2025-10-01 11:43:22,631 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:32999
2025-10-01 11:43:22,631 - distributed.worker - INFO -          dashboard at:          10.6.102.45:34077
2025-10-01 11:43:22,631 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:37059
2025-10-01 11:43:22,631 - distributed.worker - INFO -          dashboard at:          10.6.102.45:41985
2025-10-01 11:43:22,631 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:39241
2025-10-01 11:43:22,631 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:46227
2025-10-01 11:43:22,631 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:37929
2025-10-01 11:43:22,631 - distributed.worker - INFO -          dashboard at:          10.6.102.45:41483
2025-10-01 11:43:22,631 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:41877
2025-10-01 11:43:22,631 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:34849
2025-10-01 11:43:22,631 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:33235
2025-10-01 11:43:22,631 - distributed.worker - INFO -          dashboard at:          10.6.102.45:39019
2025-10-01 11:43:22,631 - distributed.worker - INFO -          dashboard at:          10.6.102.45:42551
2025-10-01 11:43:22,631 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:37609
2025-10-01 11:43:22,631 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,631 - distributed.worker - INFO -          dashboard at:          10.6.102.45:37691
2025-10-01 11:43:22,631 - distributed.worker - INFO -          dashboard at:          10.6.102.45:43271
2025-10-01 11:43:22,631 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,631 - distributed.worker - INFO -          dashboard at:          10.6.102.45:45251
2025-10-01 11:43:22,631 - distributed.worker - INFO -          dashboard at:          10.6.102.45:35699
2025-10-01 11:43:22,631 - distributed.worker - INFO -          dashboard at:          10.6.102.45:44301
2025-10-01 11:43:22,631 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,631 - distributed.worker - INFO -          dashboard at:          10.6.102.45:35735
2025-10-01 11:43:22,631 - distributed.worker - INFO -          dashboard at:          10.6.102.45:35377
2025-10-01 11:43:22,631 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,631 - distributed.worker - INFO -          dashboard at:          10.6.102.45:44155
2025-10-01 11:43:22,631 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,631 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,631 - distributed.worker - INFO -          dashboard at:          10.6.102.45:43255
2025-10-01 11:43:22,631 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,631 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,631 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,631 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,631 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,631 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,631 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,631 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,631 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,631 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,631 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,631 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,631 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,631 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,631 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,631 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,631 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,631 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,631 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,631 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,631 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,631 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,631 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,631 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,631 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,631 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,631 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,631 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,631 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,631 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,631 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,631 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,631 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-bghnd8fp
2025-10-01 11:43:22,631 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,631 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,631 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,631 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,631 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,631 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,631 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,631 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,631 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,631 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,631 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,631 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-z_pa_sh6
2025-10-01 11:43:22,631 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,631 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,631 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,632 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,631 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-w_udhxcl
2025-10-01 11:43:22,632 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,632 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,632 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-77pa1v99
2025-10-01 11:43:22,632 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,632 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-3n6vhtua
2025-10-01 11:43:22,632 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,632 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-_zc1wprz
2025-10-01 11:43:22,632 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-cp4yn34e
2025-10-01 11:43:22,632 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,632 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-ocg0zbtl
2025-10-01 11:43:22,632 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-zwus1y_m
2025-10-01 11:43:22,632 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-cmnczksq
2025-10-01 11:43:22,632 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,632 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-ctfl92qq
2025-10-01 11:43:22,632 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-pyv6do6_
2025-10-01 11:43:22,632 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,632 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-zjgevs_g
2025-10-01 11:43:22,632 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,632 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-99ccgeoy
2025-10-01 11:43:22,632 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,632 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,632 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,632 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,632 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,632 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,632 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,632 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,632 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,634 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:36153
2025-10-01 11:43:22,634 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:36153
2025-10-01 11:43:22,634 - distributed.worker - INFO -          dashboard at:          10.6.102.45:45871
2025-10-01 11:43:22,634 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,634 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,634 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,634 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,634 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-0l1i0kk3
2025-10-01 11:43:22,634 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,636 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:38395
2025-10-01 11:43:22,636 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:38395
2025-10-01 11:43:22,636 - distributed.worker - INFO -          dashboard at:          10.6.102.45:41015
2025-10-01 11:43:22,636 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,636 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,636 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,636 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,636 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-34e8prk_
2025-10-01 11:43:22,637 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,637 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:44325
2025-10-01 11:43:22,637 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:41897
2025-10-01 11:43:22,637 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:44325
2025-10-01 11:43:22,637 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:41897
2025-10-01 11:43:22,638 - distributed.worker - INFO -          dashboard at:          10.6.102.45:44605
2025-10-01 11:43:22,638 - distributed.worker - INFO -          dashboard at:          10.6.102.45:38919
2025-10-01 11:43:22,638 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,638 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,638 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,638 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,638 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,638 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,638 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,638 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,638 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-xv8aq65y
2025-10-01 11:43:22,638 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-vwx5ubz0
2025-10-01 11:43:22,638 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,638 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,653 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,653 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,653 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,653 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:44933
2025-10-01 11:43:22,653 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:44933
2025-10-01 11:43:22,654 - distributed.worker - INFO -          dashboard at:          10.6.102.45:43075
2025-10-01 11:43:22,654 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,654 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,654 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,654 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,654 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-5c1sml9u
2025-10-01 11:43:22,654 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,654 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,656 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,657 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,657 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,657 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,659 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,660 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,660 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,661 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,663 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,664 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,664 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,665 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,666 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,667 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,667 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,668 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:43793
2025-10-01 11:43:22,668 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:43793
2025-10-01 11:43:22,668 - distributed.worker - INFO -          dashboard at:          10.6.102.45:46165
2025-10-01 11:43:22,668 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,668 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,668 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,668 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,668 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-24s6wiaz
2025-10-01 11:43:22,668 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,669 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,669 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,670 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,671 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,671 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,672 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,672 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,672 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,673 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,674 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,674 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,675 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,675 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,676 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,677 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,677 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,677 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,678 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,678 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,678 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,679 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,680 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,680 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,680 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,681 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,682 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,682 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,682 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,683 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,683 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,684 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,684 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,684 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,685 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,685 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,686 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,686 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,686 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:39587
2025-10-01 11:43:22,686 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:39587
2025-10-01 11:43:22,686 - distributed.worker - INFO -          dashboard at:          10.6.102.45:39135
2025-10-01 11:43:22,686 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,686 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,686 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,686 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,686 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-142vt02h
2025-10-01 11:43:22,686 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,687 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,687 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,687 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,687 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,688 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,688 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,688 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,689 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,689 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,689 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,690 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,690 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,691 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,691 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,691 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,693 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,705 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,706 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,706 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,707 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,708 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,709 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,709 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,710 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,711 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,711 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,711 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,712 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,719 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:38151
2025-10-01 11:43:22,719 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:38151
2025-10-01 11:43:22,719 - distributed.worker - INFO -          dashboard at:          10.6.102.45:33841
2025-10-01 11:43:22,719 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,719 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,719 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,719 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,719 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-5ifo58gz
2025-10-01 11:43:22,719 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,742 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,743 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,743 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,744 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,744 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:37749
2025-10-01 11:43:22,744 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:37749
2025-10-01 11:43:22,744 - distributed.worker - INFO -          dashboard at:          10.6.102.45:36695
2025-10-01 11:43:22,744 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,744 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,744 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,744 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,745 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-b7cm6z8h
2025-10-01 11:43:22,745 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,767 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:43971
2025-10-01 11:43:22,767 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:43971
2025-10-01 11:43:22,767 - distributed.worker - INFO -          dashboard at:          10.6.102.45:34271
2025-10-01 11:43:22,767 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,767 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,767 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,767 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,767 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-y3jcukyd
2025-10-01 11:43:22,767 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,767 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,768 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,768 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,770 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,790 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,791 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,791 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,793 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,798 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:41165
2025-10-01 11:43:22,798 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:41165
2025-10-01 11:43:22,798 - distributed.worker - INFO -          dashboard at:          10.6.102.45:42141
2025-10-01 11:43:22,798 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,798 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,799 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,799 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,799 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-h0qi62ql
2025-10-01 11:43:22,799 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,820 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,821 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,821 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,823 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,855 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:34427
2025-10-01 11:43:22,856 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:34427
2025-10-01 11:43:22,856 - distributed.worker - INFO -          dashboard at:          10.6.102.45:35647
2025-10-01 11:43:22,856 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,856 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,856 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,856 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,856 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-6_58n1an
2025-10-01 11:43:22,856 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,868 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:35491
2025-10-01 11:43:22,868 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:35491
2025-10-01 11:43:22,868 - distributed.worker - INFO -          dashboard at:          10.6.102.45:42387
2025-10-01 11:43:22,868 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,868 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,868 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,868 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,868 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-_zk1yaw3
2025-10-01 11:43:22,868 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,878 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,879 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,879 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,880 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,889 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:40779
2025-10-01 11:43:22,889 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:40779
2025-10-01 11:43:22,890 - distributed.worker - INFO -          dashboard at:          10.6.102.45:42687
2025-10-01 11:43:22,890 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,890 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,890 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,890 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,890 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-m60q2tnh
2025-10-01 11:43:22,890 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,891 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,891 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,892 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,892 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:34809
2025-10-01 11:43:22,892 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:34809
2025-10-01 11:43:22,892 - distributed.worker - INFO -          dashboard at:          10.6.102.45:35199
2025-10-01 11:43:22,892 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,892 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,892 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,892 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,892 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-ro6a7va0
2025-10-01 11:43:22,892 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,893 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,905 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:36659
2025-10-01 11:43:22,905 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:36659
2025-10-01 11:43:22,905 - distributed.worker - INFO -          dashboard at:          10.6.102.45:44083
2025-10-01 11:43:22,905 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,905 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,905 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,905 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,905 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-rchdn94c
2025-10-01 11:43:22,905 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,911 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,912 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,912 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,913 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,915 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,916 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,916 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,918 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,927 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,928 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,928 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,929 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,943 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:46593
2025-10-01 11:43:22,943 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:46593
2025-10-01 11:43:22,943 - distributed.worker - INFO -          dashboard at:          10.6.102.45:33913
2025-10-01 11:43:22,943 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,943 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,943 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,943 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,943 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-y4azv49t
2025-10-01 11:43:22,943 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,948 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:35439
2025-10-01 11:43:22,948 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:35439
2025-10-01 11:43:22,948 - distributed.worker - INFO -          dashboard at:          10.6.102.45:46063
2025-10-01 11:43:22,948 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,948 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,948 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,948 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,948 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-ira_evb2
2025-10-01 11:43:22,949 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,951 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:35649
2025-10-01 11:43:22,951 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:35649
2025-10-01 11:43:22,951 - distributed.worker - INFO -          dashboard at:          10.6.102.45:38741
2025-10-01 11:43:22,951 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,951 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,951 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,951 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,951 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-drp915zh
2025-10-01 11:43:22,951 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,957 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,957 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,957 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,957 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,963 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,963 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,963 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,964 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,969 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:22,970 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,970 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,972 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:22,996 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:45697
2025-10-01 11:43:22,996 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:45697
2025-10-01 11:43:22,996 - distributed.worker - INFO -          dashboard at:          10.6.102.45:40263
2025-10-01 11:43:22,996 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:22,996 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:22,996 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:22,996 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:22,996 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-49rk2_3u
2025-10-01 11:43:22,996 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,019 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:23,020 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,020 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,022 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:23,042 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:38569
2025-10-01 11:43:23,042 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:38569
2025-10-01 11:43:23,042 - distributed.worker - INFO -          dashboard at:          10.6.102.45:46707
2025-10-01 11:43:23,042 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,042 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,042 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:23,042 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:23,042 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-664ldx57
2025-10-01 11:43:23,042 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,053 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:38359
2025-10-01 11:43:23,053 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:38359
2025-10-01 11:43:23,053 - distributed.worker - INFO -          dashboard at:          10.6.102.45:40801
2025-10-01 11:43:23,053 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,053 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,053 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:23,053 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:23,053 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-22xk558e
2025-10-01 11:43:23,053 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,054 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:43073
2025-10-01 11:43:23,054 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:43073
2025-10-01 11:43:23,054 - distributed.worker - INFO -          dashboard at:          10.6.102.45:43235
2025-10-01 11:43:23,054 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:36179
2025-10-01 11:43:23,054 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,054 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,054 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:36179
2025-10-01 11:43:23,054 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:23,054 - distributed.worker - INFO -          dashboard at:          10.6.102.45:44233
2025-10-01 11:43:23,055 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:23,055 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,055 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-s8yqwau0
2025-10-01 11:43:23,055 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,055 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:23,055 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,055 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:23,055 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-30acu_r7
2025-10-01 11:43:23,055 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,059 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:36143
2025-10-01 11:43:23,059 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:36143
2025-10-01 11:43:23,059 - distributed.worker - INFO -          dashboard at:          10.6.102.45:33797
2025-10-01 11:43:23,059 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,059 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,059 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:23,060 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:23,060 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-qcqnk2gf
2025-10-01 11:43:23,060 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,061 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:42067
2025-10-01 11:43:23,061 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:42067
2025-10-01 11:43:23,061 - distributed.worker - INFO -          dashboard at:          10.6.102.45:42415
2025-10-01 11:43:23,061 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,061 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,061 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:23,061 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:23,061 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-8s5yj773
2025-10-01 11:43:23,061 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,064 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:23,065 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:45599
2025-10-01 11:43:23,065 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:45599
2025-10-01 11:43:23,065 - distributed.worker - INFO -          dashboard at:          10.6.102.45:46127
2025-10-01 11:43:23,065 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,065 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,065 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:39025
2025-10-01 11:43:23,065 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:23,065 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:23,065 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:39025
2025-10-01 11:43:23,065 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-s9v0yla1
2025-10-01 11:43:23,065 - distributed.worker - INFO -          dashboard at:          10.6.102.45:39213
2025-10-01 11:43:23,065 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,065 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,065 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,065 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:23,065 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:23,065 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-ep3it0yx
2025-10-01 11:43:23,065 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,065 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,066 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,066 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:37225
2025-10-01 11:43:23,067 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:37225
2025-10-01 11:43:23,067 - distributed.worker - INFO -          dashboard at:          10.6.102.45:41111
2025-10-01 11:43:23,067 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,067 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,067 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:23,067 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:23,067 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-8vvjwzje
2025-10-01 11:43:23,067 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,067 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:23,070 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:23,070 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:35767
2025-10-01 11:43:23,070 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:35767
2025-10-01 11:43:23,070 - distributed.worker - INFO -          dashboard at:          10.6.102.45:33575
2025-10-01 11:43:23,070 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,070 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,070 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:23,070 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:23,070 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,070 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-ca_3n1dl
2025-10-01 11:43:23,070 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,070 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,071 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:23,071 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:42257
2025-10-01 11:43:23,071 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:42257
2025-10-01 11:43:23,071 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:40285
2025-10-01 11:43:23,071 - distributed.worker - INFO -          dashboard at:          10.6.102.45:43759
2025-10-01 11:43:23,071 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:40285
2025-10-01 11:43:23,071 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,071 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,071 - distributed.worker - INFO -          dashboard at:          10.6.102.45:42585
2025-10-01 11:43:23,071 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:23,071 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,071 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:23,072 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,072 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:23,072 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-m1zl80p6
2025-10-01 11:43:23,072 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:23,072 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,072 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-0k7ggekz
2025-10-01 11:43:23,072 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,073 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:23,073 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,073 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,073 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:37991
2025-10-01 11:43:23,073 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:37991
2025-10-01 11:43:23,073 - distributed.worker - INFO -          dashboard at:          10.6.102.45:33095
2025-10-01 11:43:23,073 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,073 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,073 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:23,073 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:23,073 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-4sxvilwq
2025-10-01 11:43:23,074 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,074 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:23,075 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:41435
2025-10-01 11:43:23,075 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:41435
2025-10-01 11:43:23,075 - distributed.worker - INFO -          dashboard at:          10.6.102.45:41939
2025-10-01 11:43:23,075 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,075 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,076 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:23,076 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:23,076 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-ryj850i8
2025-10-01 11:43:23,076 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,077 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:45785
2025-10-01 11:43:23,077 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:45785
2025-10-01 11:43:23,077 - distributed.worker - INFO -          dashboard at:          10.6.102.45:46643
2025-10-01 11:43:23,077 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,077 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,077 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:23,077 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:23,078 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-1aatevg1
2025-10-01 11:43:23,078 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,081 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:23,082 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,082 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,084 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:40549
2025-10-01 11:43:23,084 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:40549
2025-10-01 11:43:23,084 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:23,084 - distributed.worker - INFO -          dashboard at:          10.6.102.45:39871
2025-10-01 11:43:23,084 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,084 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,084 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:23,084 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:23,084 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-r1ie42es
2025-10-01 11:43:23,084 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,084 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:39225
2025-10-01 11:43:23,085 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:39225
2025-10-01 11:43:23,085 - distributed.worker - INFO -          dashboard at:          10.6.102.45:33917
2025-10-01 11:43:23,085 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,085 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,085 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:23,085 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:23,085 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-qado4xvf
2025-10-01 11:43:23,085 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,086 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:23,087 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,087 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,088 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:23,088 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.45:42877
2025-10-01 11:43:23,088 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.45:42877
2025-10-01 11:43:23,088 - distributed.worker - INFO -          dashboard at:          10.6.102.45:46509
2025-10-01 11:43:23,088 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,089 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,089 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:23,089 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:23,089 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-7xinnlja
2025-10-01 11:43:23,089 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:23,089 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,090 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,090 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,091 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:23,092 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:23,092 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,092 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,093 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:23,095 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:23,096 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,096 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,097 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:23,098 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,098 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,098 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:23,099 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:23,099 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:23,100 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,100 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,100 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:23,101 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:23,101 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,102 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,102 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:23,103 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,103 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:23,103 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,103 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:23,104 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,104 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,105 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:23,105 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:23,106 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:23,106 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,107 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,107 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:23,107 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:23,108 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,108 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,108 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,108 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,108 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:23,109 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:23,109 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:23,109 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:23,110 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,110 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,110 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:23,112 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:23,113 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:23,113 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:23,115 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 17:43:18,722 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,722 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,723 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,723 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:40285. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,723 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,723 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:43073. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,723 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,724 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:36179. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,724 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:45785. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,724 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:35649. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,723 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,723 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,723 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,723 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,723 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,726 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,723 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,724 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,726 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,724 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,724 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,724 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,727 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:42877. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,724 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,724 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,724 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,724 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,725 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,725 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,727 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:35767. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,727 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:46593. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,727 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:35491. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,725 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,727 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:41435. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,727 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:34427. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,728 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:39225. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,728 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:37991. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,728 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:38359. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,728 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:38569. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,728 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:42257. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,729 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:39025. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,722 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.45:48754 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,729 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:40779. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,729 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:36659. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,729 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,729 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:42067. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,729 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:42901. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,727 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,729 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:36143. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,729 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:34809. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,727 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,727 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,730 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:45599. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,730 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:37225. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,728 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,731 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:35439. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,729 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,731 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:40549. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,722 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.45:48748 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,722 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.45:48836 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,722 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.45:48904 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,729 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,731 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:45697. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,732 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:41165. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,732 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,733 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,733 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,733 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:43971. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,733 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:36153. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,733 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:37609. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,733 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:38395. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,733 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:37749. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,724 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.45:48794 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,724 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.45:48930 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,724 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.45:48850 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,725 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.45:48888 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,724 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.45:48872 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,724 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.45:48908 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,724 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.45:48864 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,724 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.45:48946 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,739 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,725 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.45:48782 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,738 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,724 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.45:48776 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,724 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.45:48820 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,725 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.45:48918 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,724 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.45:48772 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,724 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.45:48824 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,724 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.45:48806 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,724 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.45:48760 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,725 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.45:48914 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,739 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,739 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,725 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.45:48818 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,739 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,739 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,739 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,740 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:39241. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,740 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:33235. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,740 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,740 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,740 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,740 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,740 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,740 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,740 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,740 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,741 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,741 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,741 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:43793. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,741 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:44325. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,741 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,741 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:41897. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,741 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:46227. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,741 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:38151. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,741 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:41877. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,741 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:46523. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,741 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:40733. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,741 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:34849. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,742 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:37059. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,742 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:39587. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,742 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:33845. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,742 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:32999. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,742 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:37929. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,742 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:44933. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,742 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.45:46801. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,761 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:41277'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,762 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:37695'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,762 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:46003'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,762 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,763 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:38003'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,763 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:37047'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,763 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,763 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:45303'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,763 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,763 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:34215'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,763 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,763 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:33025'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,764 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:33621'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,764 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,764 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,764 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:46843'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,764 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,764 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,764 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:40355'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,764 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:41069'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,764 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:35055'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,765 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:43441'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,765 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,765 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,765 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,765 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,765 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,765 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,767 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,768 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,768 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,769 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,769 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,769 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,769 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,770 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,770 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:34635'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,771 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:34189'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,771 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:43877'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,771 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:40785'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,771 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:41789'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,771 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:41371'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,771 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,771 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:40179'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,771 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,771 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:37989'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,771 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,771 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,772 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,772 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:35961'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,772 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:39553'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,772 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,772 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:40061'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,772 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,772 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:44603'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,772 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,773 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:40987'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,772 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,773 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:33197'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,773 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:46125'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,773 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,773 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:34219'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,773 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,773 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:36307'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,773 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,773 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:43289'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,773 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,774 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:41755'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,774 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,774 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:35797'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,774 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,774 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,774 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,774 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,774 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,774 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,774 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,774 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,775 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,775 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,775 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,775 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,777 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,777 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,778 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,778 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:44393'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,779 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:43103'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,779 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:36071'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,779 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:38585'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,779 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:42917'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,779 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:44157'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,779 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,780 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:37063'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,780 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,780 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:35077'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,780 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,780 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:42645'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,780 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,780 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,780 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:39003'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,780 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,780 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:46505'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,780 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,781 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:39271'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,781 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:36549'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,781 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,781 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:39779'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,781 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,781 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:40007'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,781 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,781 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,781 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:41971'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,781 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:42173'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,781 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,781 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.45:39877'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,782 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,782 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,782 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,782 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,782 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,782 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,782 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,782 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,783 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,783 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,784 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,784 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,784 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,785 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,785 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,785 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,785 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,785 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,786 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,786 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,786 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,778 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x146bdc6a6c90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-01 17:43:18,779 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15457dba8850>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-01 17:43:18,789 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,790 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,790 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,791 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,791 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,792 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,792 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,792 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,792 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,792 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,792 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,792 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,793 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,793 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,793 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,793 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,793 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,794 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,794 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,794 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:20,772 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,780 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,780 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,780 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,780 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,781 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,787 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,788 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,789 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,793 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,794 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,794 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,797 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,797 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,798 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,798 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,799 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,799 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,800 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,800 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,800 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,800 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,971 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:38003'. Reason: nanny-close-gracefully
2025-10-01 17:43:20,974 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:37695'. Reason: nanny-close-gracefully
2025-10-01 17:43:20,975 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:38003' closed.
2025-10-01 17:43:20,976 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:37695' closed.
2025-10-01 17:43:20,979 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:33025'. Reason: nanny-close-gracefully
2025-10-01 17:43:20,983 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:33025' closed.
2025-10-01 17:43:20,984 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:45303'. Reason: nanny-close-gracefully
2025-10-01 17:43:20,989 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:45303' closed.
2025-10-01 17:43:20,995 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:40785'. Reason: nanny-close-gracefully
2025-10-01 17:43:20,996 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:40785' closed.
2025-10-01 17:43:21,010 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:37047'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,010 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:34215'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,012 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:37047' closed.
2025-10-01 17:43:21,012 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:34215' closed.
2025-10-01 17:43:21,027 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:46003'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,027 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:41277'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,028 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:46003' closed.
2025-10-01 17:43:21,028 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:41277' closed.
2025-10-01 17:43:21,086 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:35961'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,087 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:41789'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,088 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:35961' closed.
2025-10-01 17:43:21,091 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:41789' closed.
2025-10-01 17:43:21,127 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:41971'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,128 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:41971' closed.
2025-10-01 17:43:21,142 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:42173'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,157 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:42173' closed.
2025-10-01 17:43:21,157 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:43289'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,158 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:46505'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,162 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:39779'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,163 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:38585'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,163 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:43289' closed.
2025-10-01 17:43:21,164 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:40179'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,164 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:46505' closed.
2025-10-01 17:43:21,166 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:39779' closed.
2025-10-01 17:43:21,166 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:43103'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,167 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:38585' closed.
2025-10-01 17:43:21,167 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:35055'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,167 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:40179' closed.
2025-10-01 17:43:21,168 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:43103' closed.
2025-10-01 17:43:21,168 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:35055' closed.
2025-10-01 17:43:21,179 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:43877'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,180 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:43877' closed.
2025-10-01 17:43:21,182 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:36549'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,182 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:36549' closed.
2025-10-01 17:43:21,185 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:40061'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,187 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:37989'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,188 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:40061' closed.
2025-10-01 17:43:21,188 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:37989' closed.
2025-10-01 17:43:21,192 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:41069'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,194 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:34189'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,195 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:41069' closed.
2025-10-01 17:43:21,197 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:44157'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,199 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:35797'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,200 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:34189' closed.
2025-10-01 17:43:21,203 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:39553'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,205 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:37063'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,206 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:44157' closed.
2025-10-01 17:43:21,206 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:44393'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,206 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:39271'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,206 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:35797' closed.
2025-10-01 17:43:21,208 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:39003'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,212 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:39553' closed.
2025-10-01 17:43:21,212 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:33621'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,214 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:40355'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,214 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:34635'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,214 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:36071'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,218 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:46843'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,219 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:42917'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,219 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:37063' closed.
2025-10-01 17:43:21,219 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:44393' closed.
2025-10-01 17:43:21,219 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:39271' closed.
2025-10-01 17:43:21,222 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:44603'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,223 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:39003' closed.
2025-10-01 17:43:21,223 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:42645'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,223 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:41371'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,223 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:41755'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,223 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:39877'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,224 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:40007'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,224 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:43441'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,224 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:35077'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,224 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:40987'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,225 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:36307'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,225 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:34219'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,225 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:46125'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,225 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:33621' closed.
2025-10-01 17:43:21,225 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:40355' closed.
2025-10-01 17:43:21,225 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:34635' closed.
2025-10-01 17:43:21,226 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:36071' closed.
2025-10-01 17:43:21,226 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:46843' closed.
2025-10-01 17:43:21,226 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:42917' closed.
2025-10-01 17:43:21,226 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:44603' closed.
2025-10-01 17:43:21,226 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:42645' closed.
2025-10-01 17:43:21,226 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:41371' closed.
2025-10-01 17:43:21,227 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:41755' closed.
2025-10-01 17:43:21,227 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:39877' closed.
2025-10-01 17:43:21,227 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:40007' closed.
2025-10-01 17:43:21,227 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:43441' closed.
2025-10-01 17:43:21,227 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:35077' closed.
2025-10-01 17:43:21,227 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:40987' closed.
2025-10-01 17:43:21,227 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:36307' closed.
2025-10-01 17:43:21,227 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:34219' closed.
2025-10-01 17:43:21,227 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:46125' closed.
2025-10-01 17:43:21,297 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.45:33197'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,298 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.45:33197' closed.
2025-10-01 17:43:21,300 - distributed.dask_worker - INFO - End worker
