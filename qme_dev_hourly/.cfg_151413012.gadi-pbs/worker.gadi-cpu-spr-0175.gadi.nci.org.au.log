Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-10-02 09:24:44,512 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:34949'
2025-10-02 09:24:44,520 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:34973'
2025-10-02 09:24:44,524 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:33969'
2025-10-02 09:24:44,528 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:33111'
2025-10-02 09:24:44,532 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:45557'
2025-10-02 09:24:44,536 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:42757'
2025-10-02 09:24:44,540 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:43559'
2025-10-02 09:24:44,545 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:42503'
2025-10-02 09:24:44,549 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:46301'
2025-10-02 09:24:44,555 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:34891'
2025-10-02 09:24:44,560 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:45217'
2025-10-02 09:24:44,563 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:32827'
2025-10-02 09:24:44,567 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:40543'
2025-10-02 09:24:44,572 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:37167'
2025-10-02 09:24:44,575 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:43227'
2025-10-02 09:24:44,580 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:43951'
2025-10-02 09:24:44,583 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:39663'
2025-10-02 09:24:44,588 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:41313'
2025-10-02 09:24:44,593 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:44779'
2025-10-02 09:24:44,597 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:38371'
2025-10-02 09:24:44,688 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:35659'
2025-10-02 09:24:44,693 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:36753'
2025-10-02 09:24:44,699 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:42515'
2025-10-02 09:24:44,703 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:33743'
2025-10-02 09:24:44,708 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:42053'
2025-10-02 09:24:44,712 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:43745'
2025-10-02 09:24:44,717 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:40787'
2025-10-02 09:24:44,721 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:34551'
2025-10-02 09:24:44,725 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:40071'
2025-10-02 09:24:44,729 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:46125'
2025-10-02 09:24:44,734 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:42763'
2025-10-02 09:24:44,738 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:43387'
2025-10-02 09:24:44,742 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:33697'
2025-10-02 09:24:44,746 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:38997'
2025-10-02 09:24:44,750 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:37557'
2025-10-02 09:24:44,754 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:35159'
2025-10-02 09:24:44,758 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:45863'
2025-10-02 09:24:44,763 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:44107'
2025-10-02 09:24:44,767 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:46245'
2025-10-02 09:24:44,772 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:40029'
2025-10-02 09:24:44,776 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:32929'
2025-10-02 09:24:44,783 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:42213'
2025-10-02 09:24:44,788 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:38943'
2025-10-02 09:24:44,792 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:43517'
2025-10-02 09:24:44,796 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:42187'
2025-10-02 09:24:44,799 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:33763'
2025-10-02 09:24:44,802 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:34989'
2025-10-02 09:24:44,805 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:35951'
2025-10-02 09:24:44,811 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:33151'
2025-10-02 09:24:44,816 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:43737'
2025-10-02 09:24:44,821 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:36177'
2025-10-02 09:24:44,825 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.31:41555'
2025-10-02 09:24:45,923 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:45673
2025-10-02 09:24:45,923 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:44211
2025-10-02 09:24:45,923 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:46551
2025-10-02 09:24:45,923 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:38517
2025-10-02 09:24:45,923 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:40355
2025-10-02 09:24:45,923 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:45673
2025-10-02 09:24:45,923 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:46381
2025-10-02 09:24:45,923 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:38051
2025-10-02 09:24:45,923 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:44211
2025-10-02 09:24:45,923 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:41135
2025-10-02 09:24:45,923 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:44033
2025-10-02 09:24:45,923 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:46551
2025-10-02 09:24:45,923 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:38181
2025-10-02 09:24:45,923 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:38517
2025-10-02 09:24:45,923 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:40355
2025-10-02 09:24:45,923 - distributed.worker - INFO -          dashboard at:           10.6.83.31:37743
2025-10-02 09:24:45,923 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:46381
2025-10-02 09:24:45,923 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:38051
2025-10-02 09:24:45,923 - distributed.worker - INFO -          dashboard at:           10.6.83.31:35429
2025-10-02 09:24:45,923 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:41135
2025-10-02 09:24:45,923 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:44033
2025-10-02 09:24:45,923 - distributed.worker - INFO -          dashboard at:           10.6.83.31:39677
2025-10-02 09:24:45,923 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:38181
2025-10-02 09:24:45,923 - distributed.worker - INFO -          dashboard at:           10.6.83.31:36347
2025-10-02 09:24:45,923 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,923 - distributed.worker - INFO -          dashboard at:           10.6.83.31:42691
2025-10-02 09:24:45,923 - distributed.worker - INFO -          dashboard at:           10.6.83.31:42631
2025-10-02 09:24:45,923 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,923 - distributed.worker - INFO -          dashboard at:           10.6.83.31:38795
2025-10-02 09:24:45,923 - distributed.worker - INFO -          dashboard at:           10.6.83.31:45345
2025-10-02 09:24:45,923 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,923 - distributed.worker - INFO -          dashboard at:           10.6.83.31:36449
2025-10-02 09:24:45,923 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,923 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,923 - distributed.worker - INFO -          dashboard at:           10.6.83.31:44027
2025-10-02 09:24:45,923 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,923 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,923 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,923 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,923 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,923 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,923 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,923 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:45,923 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,923 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,923 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,923 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,923 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:45,923 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,923 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:45,923 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,923 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,923 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:45,923 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:45,923 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:45,923 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,923 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:45,923 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:45,923 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:45,923 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:45,923 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:45,923 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:45,923 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:45,923 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:45,923 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:45,923 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-9ug033li
2025-10-02 09:24:45,923 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:45,923 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:45,923 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ywfeese3
2025-10-02 09:24:45,923 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:45,923 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-24b8bxkn
2025-10-02 09:24:45,923 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:45,923 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ja0lt21r
2025-10-02 09:24:45,923 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-giqe02st
2025-10-02 09:24:45,923 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:45,923 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,923 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-0qhujguu
2025-10-02 09:24:45,923 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-8bz5es4l
2025-10-02 09:24:45,923 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,923 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-51f0j0nc
2025-10-02 09:24:45,923 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,923 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-j4tuogby
2025-10-02 09:24:45,923 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,923 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,923 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-7u0frjs3
2025-10-02 09:24:45,924 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,924 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,924 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,924 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,924 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,931 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:39473
2025-10-02 09:24:45,931 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:39473
2025-10-02 09:24:45,931 - distributed.worker - INFO -          dashboard at:           10.6.83.31:36919
2025-10-02 09:24:45,931 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,931 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,931 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:45,931 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:45,931 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-d4cdhlih
2025-10-02 09:24:45,931 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,932 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:37017
2025-10-02 09:24:45,932 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:37017
2025-10-02 09:24:45,932 - distributed.worker - INFO -          dashboard at:           10.6.83.31:35337
2025-10-02 09:24:45,932 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,932 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,932 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:45,932 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:45,933 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-s26czmim
2025-10-02 09:24:45,933 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,934 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:35045
2025-10-02 09:24:45,934 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:35045
2025-10-02 09:24:45,934 - distributed.worker - INFO -          dashboard at:           10.6.83.31:35321
2025-10-02 09:24:45,934 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,934 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,934 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:45,934 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:39381
2025-10-02 09:24:45,934 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:45,934 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:39381
2025-10-02 09:24:45,934 - distributed.worker - INFO -          dashboard at:           10.6.83.31:44763
2025-10-02 09:24:45,934 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-sw2hrdx2
2025-10-02 09:24:45,934 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,935 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,935 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:45,935 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,935 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:45,935 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-vk3kdrhu
2025-10-02 09:24:45,935 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,935 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:42077
2025-10-02 09:24:45,935 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:42077
2025-10-02 09:24:45,935 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:36693
2025-10-02 09:24:45,935 - distributed.worker - INFO -          dashboard at:           10.6.83.31:43655
2025-10-02 09:24:45,935 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:36693
2025-10-02 09:24:45,935 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,935 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:38789
2025-10-02 09:24:45,935 - distributed.worker - INFO -          dashboard at:           10.6.83.31:37419
2025-10-02 09:24:45,935 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,935 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:38789
2025-10-02 09:24:45,935 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,935 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:45,935 - distributed.worker - INFO -          dashboard at:           10.6.83.31:37763
2025-10-02 09:24:45,935 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,935 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,935 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:45,935 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:45,935 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,935 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:45,935 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:45,936 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:45,935 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-9mkv_nb_
2025-10-02 09:24:45,936 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-d6sfqj1j
2025-10-02 09:24:45,936 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-yjqh1daj
2025-10-02 09:24:45,936 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,936 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,936 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,936 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:41725
2025-10-02 09:24:45,936 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:41725
2025-10-02 09:24:45,936 - distributed.worker - INFO -          dashboard at:           10.6.83.31:40221
2025-10-02 09:24:45,937 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,937 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,937 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:45,937 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:45,937 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-a1oge0_z
2025-10-02 09:24:45,937 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,937 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:33177
2025-10-02 09:24:45,937 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:33177
2025-10-02 09:24:45,937 - distributed.worker - INFO -          dashboard at:           10.6.83.31:43167
2025-10-02 09:24:45,937 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,937 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,937 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:45,937 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:45,937 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-e2eaw_zy
2025-10-02 09:24:45,937 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,952 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:45,952 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,952 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,953 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:45,959 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:45,960 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,960 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,960 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:45,967 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:45,968 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,968 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,970 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:45,971 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:45,973 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,973 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,974 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:45,976 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:45,977 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,978 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,979 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:45,980 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:45,981 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,981 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,982 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:45,983 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:45,984 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,985 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,986 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:45,987 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:45,988 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,988 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,990 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:45,990 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:45,991 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,991 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,993 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:45,993 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:45,994 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,994 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,995 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:45,995 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:45,996 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:45,997 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:45,998 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:45,998 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:45,999 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,000 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,000 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,001 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,002 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,002 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,003 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,004 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,005 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,005 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,006 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,006 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,007 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,008 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,009 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,009 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,011 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,011 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,012 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,012 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,013 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,013 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,015 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,015 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,016 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,016 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,017 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,017 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:39291
2025-10-02 09:24:46,018 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:39291
2025-10-02 09:24:46,018 - distributed.worker - INFO -          dashboard at:           10.6.83.31:45281
2025-10-02 09:24:46,018 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,018 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,018 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,018 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,018 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-as0_jczt
2025-10-02 09:24:46,018 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,018 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,018 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,019 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,020 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,040 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:39357
2025-10-02 09:24:46,040 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:39357
2025-10-02 09:24:46,040 - distributed.worker - INFO -          dashboard at:           10.6.83.31:43477
2025-10-02 09:24:46,040 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,040 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,040 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,040 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,040 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-vht5ulef
2025-10-02 09:24:46,040 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,042 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,043 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,043 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,045 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,058 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:34521
2025-10-02 09:24:46,064 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,121 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:34521
2025-10-02 09:24:46,097 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:39939
2025-10-02 09:24:46,121 - distributed.worker - INFO -          dashboard at:           10.6.83.31:34547
2025-10-02 09:24:46,120 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:37307
2025-10-02 09:24:46,121 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:37307
2025-10-02 09:24:46,121 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,121 - distributed.worker - INFO -          dashboard at:           10.6.83.31:46399
2025-10-02 09:24:46,121 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:39939
2025-10-02 09:24:46,121 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,121 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,121 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,121 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,121 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,121 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,121 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-8u7oq6y4
2025-10-02 09:24:46,121 - distributed.worker - INFO -          dashboard at:           10.6.83.31:33503
2025-10-02 09:24:46,121 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,122 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,122 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,122 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,122 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-uieh4d6z
2025-10-02 09:24:46,122 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,122 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,122 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,122 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,122 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-jgcf56g1
2025-10-02 09:24:46,122 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,122 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,124 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,127 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:39189
2025-10-02 09:24:46,127 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:39189
2025-10-02 09:24:46,127 - distributed.worker - INFO -          dashboard at:           10.6.83.31:36003
2025-10-02 09:24:46,127 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,127 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,127 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,128 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,128 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ii9s6c0f
2025-10-02 09:24:46,128 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,131 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:37943
2025-10-02 09:24:46,131 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:37943
2025-10-02 09:24:46,131 - distributed.worker - INFO -          dashboard at:           10.6.83.31:41487
2025-10-02 09:24:46,131 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,131 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,131 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,131 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,131 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-wydags44
2025-10-02 09:24:46,132 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,138 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:39087
2025-10-02 09:24:46,138 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:39087
2025-10-02 09:24:46,138 - distributed.worker - INFO -          dashboard at:           10.6.83.31:37633
2025-10-02 09:24:46,138 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,138 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,138 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,138 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,138 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-lea2xgo4
2025-10-02 09:24:46,138 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,145 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:39681
2025-10-02 09:24:46,145 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:39681
2025-10-02 09:24:46,145 - distributed.worker - INFO -          dashboard at:           10.6.83.31:40379
2025-10-02 09:24:46,145 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,145 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,145 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,145 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,145 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-2wl0zp24
2025-10-02 09:24:46,145 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,150 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,151 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,151 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,152 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,153 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:36297
2025-10-02 09:24:46,153 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:36297
2025-10-02 09:24:46,153 - distributed.worker - INFO -          dashboard at:           10.6.83.31:43101
2025-10-02 09:24:46,154 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,154 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,154 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,154 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,154 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-5f2ea3l1
2025-10-02 09:24:46,154 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,154 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:41807
2025-10-02 09:24:46,154 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:41807
2025-10-02 09:24:46,154 - distributed.worker - INFO -          dashboard at:           10.6.83.31:34233
2025-10-02 09:24:46,154 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,154 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,154 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,154 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,154 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-k1kpo0dy
2025-10-02 09:24:46,155 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,156 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:37485
2025-10-02 09:24:46,156 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:37485
2025-10-02 09:24:46,156 - distributed.worker - INFO -          dashboard at:           10.6.83.31:36289
2025-10-02 09:24:46,156 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,156 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,156 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,156 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,156 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-f7rvcl2r
2025-10-02 09:24:46,157 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,157 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,158 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,158 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,159 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,160 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:38331
2025-10-02 09:24:46,160 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:38331
2025-10-02 09:24:46,160 - distributed.worker - INFO -          dashboard at:           10.6.83.31:42645
2025-10-02 09:24:46,160 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,160 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,160 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,160 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,160 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-236txt7n
2025-10-02 09:24:46,160 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,166 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,167 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,167 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,169 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,169 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:43581
2025-10-02 09:24:46,169 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:43581
2025-10-02 09:24:46,169 - distributed.worker - INFO -          dashboard at:           10.6.83.31:44237
2025-10-02 09:24:46,169 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,169 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,169 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,170 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,170 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-w_l4pgug
2025-10-02 09:24:46,170 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,171 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:37749
2025-10-02 09:24:46,171 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:37749
2025-10-02 09:24:46,171 - distributed.worker - INFO -          dashboard at:           10.6.83.31:34049
2025-10-02 09:24:46,171 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,171 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,171 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,171 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,171 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-_5a47k66
2025-10-02 09:24:46,171 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,171 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,172 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,172 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,173 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:38413
2025-10-02 09:24:46,174 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:38413
2025-10-02 09:24:46,174 - distributed.worker - INFO -          dashboard at:           10.6.83.31:40941
2025-10-02 09:24:46,174 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,174 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,174 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,174 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,174 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-zcqq2h_l
2025-10-02 09:24:46,174 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,174 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,175 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,176 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,176 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,178 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,178 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:35959
2025-10-02 09:24:46,178 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:35959
2025-10-02 09:24:46,178 - distributed.worker - INFO -          dashboard at:           10.6.83.31:40569
2025-10-02 09:24:46,178 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,178 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,178 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,178 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,178 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-221_y432
2025-10-02 09:24:46,179 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,179 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,180 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,181 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,182 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,183 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,184 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,184 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,185 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:40121
2025-10-02 09:24:46,185 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:40121
2025-10-02 09:24:46,185 - distributed.worker - INFO -          dashboard at:           10.6.83.31:36695
2025-10-02 09:24:46,185 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,185 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,185 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,185 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,185 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-8q5wjgbv
2025-10-02 09:24:46,185 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,185 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,186 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,187 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,187 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,189 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,189 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,190 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,190 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,191 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:42339
2025-10-02 09:24:46,191 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:42339
2025-10-02 09:24:46,191 - distributed.worker - INFO -          dashboard at:           10.6.83.31:36915
2025-10-02 09:24:46,191 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:40717
2025-10-02 09:24:46,191 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,191 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,191 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:40717
2025-10-02 09:24:46,191 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,192 - distributed.worker - INFO -          dashboard at:           10.6.83.31:43829
2025-10-02 09:24:46,192 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,192 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,192 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-a0ru6a5a
2025-10-02 09:24:46,192 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,192 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,192 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,192 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,192 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ewe7mwb7
2025-10-02 09:24:46,192 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,192 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,198 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:36809
2025-10-02 09:24:46,198 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:36809
2025-10-02 09:24:46,198 - distributed.worker - INFO -          dashboard at:           10.6.83.31:46091
2025-10-02 09:24:46,198 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,198 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,198 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,198 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,198 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-qbrg_mhi
2025-10-02 09:24:46,198 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,202 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,202 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,202 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,203 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,205 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,206 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,206 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,207 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,209 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,210 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,210 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,212 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,212 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,213 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,213 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,214 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,215 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,216 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,216 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,217 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,217 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,218 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,218 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,220 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,220 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,221 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,222 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,223 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,223 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,224 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,224 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,225 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,226 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,226 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,226 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,227 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,228 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,229 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,229 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,231 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,316 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:33163
2025-10-02 09:24:46,316 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:33163
2025-10-02 09:24:46,316 - distributed.worker - INFO -          dashboard at:           10.6.83.31:34705
2025-10-02 09:24:46,316 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,316 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,316 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,316 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,316 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-684njecv
2025-10-02 09:24:46,316 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,317 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:34959
2025-10-02 09:24:46,317 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:34959
2025-10-02 09:24:46,317 - distributed.worker - INFO -          dashboard at:           10.6.83.31:37059
2025-10-02 09:24:46,317 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,317 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,317 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,317 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,317 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-3qe6rp_7
2025-10-02 09:24:46,317 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,323 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:37935
2025-10-02 09:24:46,323 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:37935
2025-10-02 09:24:46,323 - distributed.worker - INFO -          dashboard at:           10.6.83.31:43187
2025-10-02 09:24:46,323 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,323 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,323 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,323 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,323 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-gyueabbc
2025-10-02 09:24:46,323 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,326 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:43879
2025-10-02 09:24:46,326 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:43879
2025-10-02 09:24:46,326 - distributed.worker - INFO -          dashboard at:           10.6.83.31:38173
2025-10-02 09:24:46,326 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,326 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,327 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,327 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,327 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-rrm1518l
2025-10-02 09:24:46,327 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,333 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,333 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,333 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,334 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,341 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,341 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,341 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,342 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,342 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:37587
2025-10-02 09:24:46,342 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:37587
2025-10-02 09:24:46,342 - distributed.worker - INFO -          dashboard at:           10.6.83.31:35831
2025-10-02 09:24:46,342 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,342 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,342 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,343 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,343 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-se3nhuzu
2025-10-02 09:24:46,343 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,345 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:33477
2025-10-02 09:24:46,345 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:33477
2025-10-02 09:24:46,345 - distributed.worker - INFO -          dashboard at:           10.6.83.31:43557
2025-10-02 09:24:46,345 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,345 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,345 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,345 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,345 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-0tw9dd0n
2025-10-02 09:24:46,345 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,349 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,350 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,350 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,351 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,352 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:40899
2025-10-02 09:24:46,352 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:40899
2025-10-02 09:24:46,352 - distributed.worker - INFO -          dashboard at:           10.6.83.31:38851
2025-10-02 09:24:46,352 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,352 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,352 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,352 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,352 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-i4h6yy8p
2025-10-02 09:24:46,352 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,355 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:41739
2025-10-02 09:24:46,355 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:41739
2025-10-02 09:24:46,355 - distributed.worker - INFO -          dashboard at:           10.6.83.31:37385
2025-10-02 09:24:46,355 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,355 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,355 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,355 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,355 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-f8_84chi
2025-10-02 09:24:46,355 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,357 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:34537
2025-10-02 09:24:46,358 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:34537
2025-10-02 09:24:46,358 - distributed.worker - INFO -          dashboard at:           10.6.83.31:37037
2025-10-02 09:24:46,358 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,358 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,358 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,358 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,358 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-m_r8h996
2025-10-02 09:24:46,358 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,361 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:39849
2025-10-02 09:24:46,361 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:39849
2025-10-02 09:24:46,361 - distributed.worker - INFO -          dashboard at:           10.6.83.31:34247
2025-10-02 09:24:46,361 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,361 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,361 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,361 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,361 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-qzteon2n
2025-10-02 09:24:46,361 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,363 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:35523
2025-10-02 09:24:46,364 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:35523
2025-10-02 09:24:46,364 - distributed.worker - INFO -          dashboard at:           10.6.83.31:33479
2025-10-02 09:24:46,364 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,364 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,364 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,364 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,364 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ewq5a7gu
2025-10-02 09:24:46,364 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,365 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,366 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,366 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,367 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.31:43137
2025-10-02 09:24:46,367 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.31:43137
2025-10-02 09:24:46,367 - distributed.worker - INFO -          dashboard at:           10.6.83.31:34625
2025-10-02 09:24:46,367 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,367 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,367 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:46,367 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:46,367 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-9_s6hubi
2025-10-02 09:24:46,367 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,368 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,376 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,377 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,378 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,379 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,381 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,382 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,382 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,384 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,385 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,386 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,386 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,388 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,389 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,390 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,390 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,392 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,392 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,393 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,393 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,395 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,395 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,396 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,396 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,398 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,398 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,399 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,399 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,401 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:46,401 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:46,402 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:46,402 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:46,403 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:56,389 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,389 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,390 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,390 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,390 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,390 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,390 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,391 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,391 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,391 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,391 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,391 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,391 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,391 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,392 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,392 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,392 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,392 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,392 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,392 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,392 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,392 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,392 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,392 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,392 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,392 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,392 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,392 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,393 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,393 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,393 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,393 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,393 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,393 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,393 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,393 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,393 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,393 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,393 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,393 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,394 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,394 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,394 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,394 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,394 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,394 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,394 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,394 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,394 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,394 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,394 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,394 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,394 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,394 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,394 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,394 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,394 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,395 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,395 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,393 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,395 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,395 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,395 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,393 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,395 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,395 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,395 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,395 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,395 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,395 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,395 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,395 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,395 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,395 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,396 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,396 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,396 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,396 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,396 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,396 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,396 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,396 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,396 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,395 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,397 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,397 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,397 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,397 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,398 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,398 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,398 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,398 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,398 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,398 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,398 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,399 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,399 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,399 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,399 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,400 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,400 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,401 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,404 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,406 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:59,146 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,146 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,147 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,147 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,147 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,147 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,147 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,147 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,147 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,147 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,148 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,148 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,148 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,148 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,148 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,148 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,148 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,149 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,149 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,149 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,149 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,149 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,149 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,149 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,150 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,149 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,149 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,149 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,150 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,150 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,150 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,149 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,150 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,150 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,150 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,150 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,150 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,150 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,150 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,148 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,150 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,150 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,151 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,151 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,150 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,151 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,150 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,151 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,150 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,150 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,151 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,151 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,151 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,151 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,151 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,151 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,151 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,151 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,152 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,152 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,151 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,151 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,151 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,151 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,152 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,151 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,152 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,151 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,152 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,152 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,152 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,152 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,152 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,152 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,152 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,152 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,152 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,152 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,152 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,153 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,153 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,153 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,153 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,153 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,154 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,154 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,154 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,154 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,154 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,154 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,155 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,155 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,155 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,155 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,155 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,155 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,155 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,155 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,156 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,156 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,156 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,156 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,156 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,157 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,552 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,553 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,553 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,553 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,553 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,553 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,553 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,553 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,553 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,554 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,554 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,554 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,554 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,554 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,554 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,554 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,555 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,555 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,555 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,555 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,555 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,555 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,555 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,555 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,556 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,556 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,556 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,556 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,556 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,556 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,557 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,557 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,557 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,557 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,557 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,557 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,557 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,557 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,557 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,557 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,557 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,558 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,558 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,558 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,558 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,558 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,558 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,558 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,558 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,558 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,558 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,558 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,559 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,559 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,559 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,559 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,559 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,559 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,559 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,559 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,559 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,559 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,559 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,559 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,559 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,560 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,560 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,560 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,559 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,560 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,560 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,560 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,560 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,560 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,560 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,560 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,560 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,561 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,561 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,561 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,561 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,561 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,561 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,561 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,562 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,562 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,562 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,562 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,562 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,562 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,562 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,562 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,562 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,562 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,562 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,562 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,562 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,563 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,563 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,563 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,563 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,563 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,564 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,568 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,981 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,981 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,981 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,981 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,981 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,982 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,982 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,982 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,982 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,982 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,982 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,982 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,982 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,983 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,983 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,983 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,983 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,983 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,983 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,983 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,983 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,983 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,984 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,984 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,984 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,984 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,984 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,984 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,984 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,984 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,984 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,984 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,984 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,984 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,984 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,984 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,984 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,984 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,984 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,984 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,985 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,985 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,985 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,985 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,985 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,985 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,985 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,985 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,985 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,985 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,985 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,985 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,985 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,985 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,985 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,986 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,986 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,986 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,986 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,986 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,986 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,986 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,986 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,986 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,986 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,986 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,986 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,986 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,986 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,986 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,986 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,986 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,986 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,986 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,986 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,987 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,987 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,988 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,987 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,987 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,987 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,987 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,987 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,988 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,988 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,988 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,988 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,988 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,988 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,988 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,989 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,989 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,989 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,989 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,989 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,990 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,990 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,990 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,990 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,990 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,990 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,990 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,990 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,994 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:24,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,492 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,613 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,626 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 10:06:27,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 10:13:15,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 10:33:50,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 10:47:34,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 10:47:38,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 11:01:20,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 11:21:51,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 11:28:42,667 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 11:42:30,202 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 12:37:50,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 13:05:46,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 13:12:45,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 13:19:40,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 14:15:30,850 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 14:29:30,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 14:43:31,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:29:34,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:50:52,300 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 17:54:55,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 18:30:32,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 18:58:55,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 19:25:43,465 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,465 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,465 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,466 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:35959. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,466 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:39473. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,466 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:38517. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,465 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,465 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,464 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,467 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:43879. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,465 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,467 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:36809. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,465 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,465 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,465 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,465 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,468 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:34537. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,465 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,465 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,465 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,465 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,468 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:34959. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,465 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,465 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,466 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,468 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:40121. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,468 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:38181. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,468 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:41725. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,468 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:36693. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,468 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:44211. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,469 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:38051. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,469 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:35045. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,469 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:39681. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,469 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:46551. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,469 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:37943. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,469 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:38789. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,467 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,471 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:39939. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,478 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:42515'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,480 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,480 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,480 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,481 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,481 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,483 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:42757'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,484 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:43745'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,484 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,485 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:33111'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,485 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,485 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,485 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:38943'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,485 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,485 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,485 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,485 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,485 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:33151'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,485 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,485 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,486 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,486 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,486 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,486 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,486 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,486 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,486 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,486 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,486 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,486 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,487 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,487 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,487 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,487 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,487 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,487 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,487 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,491 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,492 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:46125'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,492 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:41555'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,493 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:43559'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,493 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,493 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,493 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:34973'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,493 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,493 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,493 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:40543'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,494 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,494 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:39663'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,494 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,494 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,494 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,494 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,494 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:45557'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,494 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,494 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,494 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:34891'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,494 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,494 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,494 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,494 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:38997'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,494 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,494 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,494 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,495 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,495 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,495 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,495 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:43517'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,495 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,495 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,495 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,495 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,495 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,495 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:46301'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,495 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,495 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,495 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,495 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,495 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,495 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,495 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,495 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:42503'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,495 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,495 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,495 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,495 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,496 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,496 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,496 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,496 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,496 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:33763'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,496 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,496 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,496 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,496 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,496 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,496 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,496 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,496 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,496 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,496 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,496 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,496 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,496 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,497 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,497 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,497 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,497 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,497 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,497 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,497 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,497 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,497 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,497 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,497 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,497 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,497 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,497 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,504 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,503 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,506 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:41135. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,508 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,505 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,505 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,509 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,498 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,511 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,508 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,511 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:33177. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,511 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,509 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,513 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,513 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,513 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,514 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:37017. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,514 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,514 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:42077. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,514 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,510 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,515 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,516 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,516 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,516 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:39291. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,516 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,517 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:39381. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,518 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:46381. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,515 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,520 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:40355. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,521 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,524 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:44033. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,522 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,524 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,530 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:39357. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,532 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:45673. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,537 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,546 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:43581. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,548 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,574 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,589 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:39087. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,599 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,599 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,607 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:36297. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,608 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:38331. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,607 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,616 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:41807. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,625 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,629 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,634 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:38413. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,626 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,638 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:37307. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,643 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:39189. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,694 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,702 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:40717. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,741 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,750 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:37749. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,751 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,759 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:34521. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,757 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,775 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:37485. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,778 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:42213'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,855 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:34949'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,868 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,876 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:35523. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,881 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 14, 7, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:43,873 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,890 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:33477. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,886 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,903 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:42339. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,918 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:40071'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,926 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 2, 14, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:43,928 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,932 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,934 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,937 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,939 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,943 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,944 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,948 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,949 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,948 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,953 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,956 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:37935. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,969 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,976 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:40899. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,019 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:44,032 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:41313'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,036 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:39849. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,038 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:33697'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,047 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:32827'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,055 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 14, 16, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,061 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,050 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:44,062 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:36753'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,066 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,067 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:37587. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,072 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,073 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:45217'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,077 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,067 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:44,082 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,084 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:33163. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,090 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:42053'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,101 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:38371'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,102 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:43951'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,108 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:33969'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,109 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:34989'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,116 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:40787'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,124 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:44,132 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:41739. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,136 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:44,144 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.31:43137. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,153 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 0, 14, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,155 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 12, 11, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,159 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,164 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,168 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 6, 2, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,170 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,174 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 1, 12, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,175 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,178 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:42187'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,180 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,182 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,188 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,193 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,198 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,198 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:35659'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,201 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,203 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,206 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,207 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 13, 7, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,212 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,217 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,220 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 6, 3, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,222 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,229 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,234 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,235 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:44779'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,238 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:37167'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,240 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,245 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,250 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,263 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:42763'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,276 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:43387'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,280 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:44107'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,284 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:33743'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,284 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 7, 0, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 7, 16, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 2, 19, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,306 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,306 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,307 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,307 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,307 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,344 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,350 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,355 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,360 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,366 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,391 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 10, 8, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,404 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 3, 9, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,430 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 10, 20, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,432 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:36177'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,436 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:45863'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,467 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,472 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,478 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,483 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,488 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,509 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 10, 15, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,520 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,526 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,531 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,536 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,541 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,549 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 10, 6, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,555 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 7, 21, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,558 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:34551'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,578 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,583 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,585 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,588 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,590 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,594 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,596 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,599 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,601 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,606 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,641 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 13, 21, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,753 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,758 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,764 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,769 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,774 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,802 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,807 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,813 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,818 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,823 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,879 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,884 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,890 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,894 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:43227'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,895 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,900 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,935 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 5, 18, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:45,102 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:46245'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:45,137 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:45,142 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:45,148 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:45,153 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:45,158 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:45,159 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 10, 12, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:45,175 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:45,181 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:45,187 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:45,191 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:45,192 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:45,197 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:45,197 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:45,202 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:45,207 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:45,213 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:45,396 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 15, 11, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:45,427 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 3, 3, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:45,448 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 2, 3, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:45,453 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:32929'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:45,455 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:40029'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:45,455 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 5, 3, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:45,455 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:45,461 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:45,466 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:45,472 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:45,477 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:45,490 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,495 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,498 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,507 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,512 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,513 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,514 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,517 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,517 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,518 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,518 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,519 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,519 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,525 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,551 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,587 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:45,588 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:45,588 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:45,588 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:45,588 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:45,607 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:35159'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:45,710 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:45,715 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:45,721 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:45,726 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:45,726 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:45,727 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:45,727 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:45,727 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:45,727 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:45,731 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:45,781 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.31:42515'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,783 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.31:42757'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,784 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.31:42515' closed.
2025-10-02 19:25:45,785 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.31:42757' closed.
2025-10-02 19:25:45,857 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:45,863 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:45,868 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:45,874 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:45,879 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:45,883 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.31:33111'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,885 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.31:33111' closed.
2025-10-02 19:25:45,914 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 8, 4, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:46,049 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.31:33763'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,052 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.31:38943'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,053 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.31:43745'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,053 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.31:33763' closed.
2025-10-02 19:25:46,054 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.31:38943' closed.
2025-10-02 19:25:46,054 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.31:43745' closed.
2025-10-02 19:25:46,086 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.31:40543'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,088 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.31:40543' closed.
2025-10-02 19:25:46,111 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.31:42503'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,112 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.31:42503' closed.
2025-10-02 19:25:46,120 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.31:34891'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,122 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.31:34891' closed.
2025-10-02 19:25:46,126 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.31:46301'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,126 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.31:38997'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,127 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.31:46301' closed.
2025-10-02 19:25:46,128 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.31:38997' closed.
2025-10-02 19:25:46,132 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.31:34973'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,133 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:43737'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:46,134 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.31:34973' closed.
2025-10-02 19:25:46,149 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.31:46125'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,149 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.31:46125' closed.
2025-10-02 19:25:46,157 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.31:41555'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,162 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.31:41555' closed.
2025-10-02 19:25:46,165 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.31:45557'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,169 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.31:45557' closed.
2025-10-02 19:25:46,170 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.31:39663'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,171 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.31:39663' closed.
2025-10-02 19:25:46,184 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.31:43517'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,185 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.31:43517' closed.
2025-10-02 19:25:46,187 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.31:43559'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,188 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.31:43559' closed.
2025-10-02 19:25:46,206 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.31:33151'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,211 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.31:33151' closed.
2025-10-02 19:25:46,238 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:46,244 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:46,249 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:46,254 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:46,260 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:46,301 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:35951'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:46,322 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:46,323 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:46,323 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:46,323 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:46,323 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:46,328 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 5, 6, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:46,410 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.31:37557'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:46,610 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 8, 0, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:46,692 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:46,698 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:46,703 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:46,708 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:46,713 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:46,758 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 7, 6, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:46,792 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 9, 14, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:46,891 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 9, 14, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:46,929 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 9, 15, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:47,090 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:47,096 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:47,101 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:47,106 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:47,111 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:46,761 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:47,186 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:47,192 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:47,197 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:47,201 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:47,202 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:47,206 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:47,208 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:47,211 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:47,216 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:47,221 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:47,500 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:47,506 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:47,512 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:47,515 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:47,517 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:47,520 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:47,522 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:47,526 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:47,531 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:47,536 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:47,218 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14d435fec490>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:47,582 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14b79e0e81d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:47,369 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1496f8775690>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:47,556 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x148b5f6777d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:48,019 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:47,709 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1550864e5050>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:47,796 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:48,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 9, 21, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:48,304 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:48,310 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:48,315 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:48,320 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:48,325 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:48,496 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:50,023 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:50,499 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.31:38371'. Reason: nanny-close-gracefully
2025-10-02 19:25:50,500 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.31:38371' closed.
